{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e4253003a6d611145265fdd6119f7ad3111428a"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Part 1\n",
    "\n",
    "What's the plan?\n",
    "\n",
    "1 Understand our data better in **Exploratory Data Analysis**, do necessary data wrangling\n",
    "\n",
    "2 Use sales from Oct 2015 as predictions for Nov 2015(**Previous Value Benchmark**)\n",
    "\n",
    "3 **Quick Baseline**. Apply some variant of decision tree(without any feature engineering, compare this with previous value benchmark)\n",
    "\n",
    "4 Set up **Cross Validation** to try out different feature engineering ideas\n",
    "\n",
    "5 Tune decision tree models, try to tune and get several diverse models with similar performance\n",
    "\n",
    "6 Use Ensemble methods to boost score\n",
    "  \n",
    "Btw, I'll omit the ploting part of EDA and all outputs of my code, because I am just compiling my notebooks and upload to kaggle as a kernel for future reference. But feel free to use my code here to get started and try my feature engineering ideas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第1部\n",
    "\n",
    "今後の方針は？\n",
    "\n",
    "1 Exploratory Data Analysisでデータを理解し、必要なデータ整理を行う。\n",
    "\n",
    "2 2015年10月の売上高を2015年11月の予測値として使用する(Previous Value Benchmark)\n",
    "\n",
    "3 クイックベースライン。デシジョンツリーを適用して、前回のベンチマークと比較します。\n",
    "\n",
    "4 クロスバリデーションを設定して、さまざまな特徴エンジニアリングのアイデアを試す\n",
    "\n",
    "5 決定木のモデルをチューニングし、似たような性能を持つ多様なモデルをいくつか作ってみる\n",
    "\n",
    "6 アンサンブル手法を使ってスコアを上げる\n",
    "\n",
    "私は自分のノートブックをコンパイルし、将来の参考のためにカーネルとしてkaggleにアップロードしているだけなので、EDAのプロット部分と私のコードのすべての出力は省略します。しかし、ここで私のコードを自由に使って、私の機能工学のアイデアを試してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "593876eb61eef4c5dc8f1ccc118496be7c427a0e"
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "acea7ea2efa32fd41d9133669421522935e2b213",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import loadtxt\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from xgboost import plot_tree\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "kernel_with_output = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d87fd338b141c70ce7341e93c95ad4ded7ffdbd"
   },
   "source": [
    "## Data loading\n",
    "Load all provided datasets and get a feel of the data provided to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "8903d7855924feb2d4c0b5373dc1a17651bc0d10",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    sales_train = pd.read_csv('data/sales_train.csv')\n",
    "    items = pd.read_csv('data/items.csv')\n",
    "    shops = pd.read_csv('data/shops.csv')\n",
    "    item_categories = pd.read_csv('data/item_categories.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea523489895e88e92ebbc54934f50f3ace0ae38a"
   },
   "source": [
    "## Insert missing rows and aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "fc37a4d4d3194efe0ab8e59a59e023bab7f05557",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    # For every month we create a grid from all shops/items combinations from that month\n",
    "    grid = []\n",
    "    for block_num in sales_train['date_block_num'].unique():\n",
    "        cur_shops = sales_train[sales_train['date_block_num']==block_num]['shop_id'].unique()\n",
    "        cur_items = sales_train[sales_train['date_block_num']==block_num]['item_id'].unique()\n",
    "        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "    index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "    # Aggregations\n",
    "    sales_train['item_cnt_day'] = sales_train['item_cnt_day'].clip(0,20)\n",
    "    groups = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'])\n",
    "    trainset = groups.agg({'item_cnt_day':'sum', 'item_price':'mean'}).reset_index()\n",
    "    trainset = trainset.rename(columns = {'item_cnt_day' : 'item_cnt_month'})\n",
    "    trainset['item_cnt_month'] = trainset['item_cnt_month'].clip(0,20)\n",
    "\n",
    "    trainset = pd.merge(grid,trainset,how='left',on=index_cols)\n",
    "    trainset.item_cnt_month = trainset.item_cnt_month.fillna(0)\n",
    "\n",
    "    # Get category id\n",
    "    trainset = pd.merge(trainset, items[['item_id', 'item_category_id']], on = 'item_id')\n",
    "    trainset.to_csv('trainset_with_grid.csv')\n",
    "\n",
    "    trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e034a42fdac02318f5a7de045f3f94746fcaa6cd"
   },
   "source": [
    "# Previous Value Benchmark\n",
    "**Copy from coursera**  \n",
    "\"\n",
    "A good exercise is to reproduce previous_value_benchmark. As the name suggest - in this benchmark for the each shop/item pair our predictions are just monthly sales from the previous month, i.e. October 2015.\n",
    "\n",
    "The most important step at reproducing this score is correctly aggregating daily data and constructing monthly sales data frame. You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range. If you do it correctly, you'll get precisely 1.16777 on the public leaderboard.\n",
    "\n",
    "Generating features like this is a necessary basis for more complex models. Also, if you decide to fit some model, don't forget to clip the target into [0,20] range, it makes a big difference.\"\n",
    "\n",
    "** Comments **\n",
    "\n",
    "Simply put: Use October 2015 sales(number of items sold) as our predictions for sales of November 2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前回のバリューベンチマーク\n",
    "\n",
    "courseraからのコピー\n",
    "\" previous_value_benchmarkを再現するのが良い練習になります。その名の通り、このベンチマークではショップとアイテムのペアごとに、前月（2015年10月）の月次売上だけを予測します。\n",
    "\n",
    "このスコアを再現するための最も重要なステップは、日次データを正しく集計し、月次売上データフレームを構築することです。遅延値を取得し、NaNをゼロで埋め、値を[0,20]の範囲にクリップする必要があります。正しく実行すれば、パブリックリーダーボードに正確に1.16777が表示されます。\n",
    "\n",
    "このような特徴を生成することは、より複雑なモデルに必要な基礎となります。また、何かのモデルをフィットさせようと思ったら、ターゲットを[0,20]の範囲にクリップすることを忘れないでください。\"これは大きな違いです。\n",
    "\n",
    "** コメント **\n",
    "\n",
    "簡単に言うと、2015年10月の売上(アイテム販売数)を2015年11月の売上の予測に使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f7bd2b50fc6a90dacdc694b36b14d2f1f431f1a8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    prev_month_selector = (trainset.month == 10) & (trainset.year == 2015)\n",
    "    train_subset = trainset[prev_month_selector]\n",
    "    groups = train_subset[['shop_id', 'item_id', 'item_cnt_month']].groupby(by = ['shop_id', 'item_id'])\n",
    "    train_subset = groups.agg({'item_cnt_month':'sum'}).reset_index()\n",
    "    train_subset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "93c704384588439502957d718f4215bc16735da8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    merged = test.merge(train_subset, on=[\"shop_id\", \"item_id\"], how=\"left\")[[\"ID\", \"item_cnt_month\"]]\n",
    "    merged.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28353cef5e25a434fe25d1b1fe5e5bbc2c1323e2"
   },
   "source": [
    "マージ後は、item_cnt_monthの欠損値がたくさん出てきます。これは、2015年10月からのshop_id/item_idのペアの数が少ないためです。欠損値を0で埋め、値を範囲(0,20)にクリップします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "48ec6947c381bde56835b77fd96f678ca88b6d04",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    merged['item_cnt_month'] = merged.item_cnt_month.fillna(0).clip(0,20)\n",
    "    submission = merged.set_index('ID')\n",
    "    submission.to_csv('benchmark.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "591b47e0c12c595a581e3f960d297be982e5d692"
   },
   "source": [
    "# Quick Baseline with XGBoost\n",
    "Here, I'll use only the following features to make a quick baseline solution for the problem  \n",
    "  \n",
    "  **'shop_id', 'item_id', 'item_category_id', 'date_block_num'**  \n",
    "  \n",
    "Note that target is **item_cnt_month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a085e0a332b42bac116270a2437d817f364dbb37",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    # Extract features and target we want\n",
    "    baseline_features = ['shop_id', 'item_id', 'item_category_id', 'date_block_num', 'item_cnt_month']\n",
    "    train = trainset[baseline_features]\n",
    "    # Remove pandas index column\n",
    "    train = train.set_index('shop_id')\n",
    "    train.item_cnt_month = train.item_cnt_month.astype(int)\n",
    "    train['item_cnt_month'] = train.item_cnt_month.fillna(0).clip(0,20)\n",
    "    # Save train set to file\n",
    "    train.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "60b6c336c90d533fb1f410a568a1008f050fd156",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    dataset = loadtxt('train.csv', delimiter=\",\" ,skiprows=1, dtype = int)\n",
    "    trainx = dataset[:, 0:4]\n",
    "    trainy = dataset[:, 4]\n",
    "\n",
    "    test_dataset = loadtxt('data/test.csv', delimiter=\",\" ,skiprows=1, usecols = (1,2), dtype=int)\n",
    "    test_df = pd.DataFrame(test_dataset, columns = ['shop_id', 'item_id'])\n",
    "\n",
    "    # Make test_dataset pandas data frame, add category id and date block num, then convert back to numpy array and predict\n",
    "    merged_test = pd.merge(test_df, items, on = ['item_id'])[['shop_id','item_id','item_category_id']]\n",
    "    merged_test['date_block_num'] = 33\n",
    "    merged_test.set_index('shop_id')\n",
    "    merged_test.head(3)\n",
    "\n",
    "    model = xgb.XGBRegressor(max_depth = 10, min_child_weight=0.5, subsample = 1, eta = 0.3, num_round = 1000, seed = 1)\n",
    "    model.fit(trainx, trainy, eval_metric='rmse')\n",
    "    preds = model.predict(merged_test.values)\n",
    "\n",
    "    df = pd.DataFrame(preds, columns = ['item_cnt_month'])\n",
    "    df['ID'] = df.index\n",
    "    df = df.set_index('ID')\n",
    "    df.to_csv('simple_xgb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64677c276f34d62b9278838a93f4caa22c427f88"
   },
   "source": [
    "Kaggleに初めて投稿した後、RMSEスコアが約15になりました。明らかに許容できません。ターゲットを0-20の範囲にクリップした後、RMSEは2.12となりました。これは、普通のGradient Boosted Treeが得られるであろうスコアに近いものです。このスコアを改善するために、以下のようなクロスバリデーションスキームを設定し、様々な特徴エンジニアリングのアイデアを試し、より良い結果が得られるかどうかを確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dfd2569538bdcf1c17b7500a530e3910e3fa755c"
   },
   "source": [
    "# Part2\n",
    "## Set up some global vars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "16a33a52e589a46038290776e285307352d6893b",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    # Set seeds and options\n",
    "    np.random.seed(10)\n",
    "    pd.set_option('display.max_rows', 231)\n",
    "    pd.set_option('display.max_columns', 100)\n",
    "\n",
    "    # Feature engineering list\n",
    "    new_features = []\n",
    "    enable_feature_idea = [True, True, True, True, True, True, True, True, True, True]\n",
    "\n",
    "    # Some parameters(maybe add more periods, score will be better) [1,2,3,12]\n",
    "    lookback_range = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "    tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e3ac3b16ed5614d255cc89376c9477a21d366a3"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d4b24b22de52b4b034dc58a7c682fe0199b38b73",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    current = time.time()\n",
    "\n",
    "    trainset = pd.read_csv('trainset_with_grid.csv')\n",
    "    items = pd.read_csv('data/items.csv')\n",
    "    shops = pd.read_csv('data/shops.csv')\n",
    "\n",
    "\n",
    "    # Only use more recent data\n",
    "    start_month = 0\n",
    "    end_month = 33\n",
    "    trainset = trainset[['shop_id', 'item_id', 'item_category_id', 'date_block_num', 'item_price', 'item_cnt_month']]\n",
    "    trainset = trainset[(trainset.date_block_num >= start_month) & (trainset.date_block_num <= end_month)]\n",
    "\n",
    "    print('Loading test set...')\n",
    "    test_dataset = loadtxt('data/test.csv', delimiter=\",\" ,skiprows=1, usecols = (1,2), dtype=int)\n",
    "    testset = pd.DataFrame(test_dataset, columns = ['shop_id', 'item_id'])\n",
    "\n",
    "    print('Merging with other datasets...')\n",
    "    # Get item category id into test_df\n",
    "    testset = testset.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')\n",
    "    testset['date_block_num'] = 34\n",
    "    # Make testset contains same column as trainset so we can concatenate them row-wise\n",
    "    testset['item_cnt_month'] = -1\n",
    "\n",
    "    train_test_set = pd.concat([trainset, testset], axis = 0) \n",
    "\n",
    "    end = time.time()\n",
    "    diff = end - current\n",
    "    print('Took ' + str(int(diff)) + ' seconds to train and predict val set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2737ba27e7868bfe7cdb8685b4502fa6f5811dde"
   },
   "source": [
    "## Fix category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "ebd0763a93b9e9b39f193160a499bf30d65bf4b7",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    item_cat = pd.read_csv('data/item_categories.csv')\n",
    "\n",
    "    # Fix category\n",
    "    l_cat = list(item_cat.item_category_name)\n",
    "    for ind in range(0,1):\n",
    "        l_cat[ind] = 'PC Headsets / Headphones'\n",
    "    for ind in range(1,8):\n",
    "        l_cat[ind] = 'Access'\n",
    "    l_cat[8] = 'Tickets (figure)'\n",
    "    l_cat[9] = 'Delivery of goods'\n",
    "    for ind in range(10,18):\n",
    "        l_cat[ind] = 'Consoles'\n",
    "    for ind in range(18,25):\n",
    "        l_cat[ind] = 'Consoles Games'\n",
    "    l_cat[25] = 'Accessories for games'\n",
    "    for ind in range(26,28):\n",
    "        l_cat[ind] = 'phone games'\n",
    "    for ind in range(28,32):\n",
    "        l_cat[ind] = 'CD games'\n",
    "    for ind in range(32,37):\n",
    "        l_cat[ind] = 'Card'\n",
    "    for ind in range(37,43):\n",
    "        l_cat[ind] = 'Movie'\n",
    "    for ind in range(43,55):\n",
    "        l_cat[ind] = 'Books'\n",
    "    for ind in range(55,61):\n",
    "        l_cat[ind] = 'Music'\n",
    "    for ind in range(61,73):\n",
    "        l_cat[ind] = 'Gifts'\n",
    "    for ind in range(73,79):\n",
    "        l_cat[ind] = 'Soft'\n",
    "    for ind in range(79,81):\n",
    "        l_cat[ind] = 'Office'\n",
    "    for ind in range(81,83):\n",
    "        l_cat[ind] = 'Clean'\n",
    "    l_cat[83] = 'Elements of a food'\n",
    "\n",
    "    lb = preprocessing.LabelEncoder()\n",
    "    item_cat['item_category_id_fix'] = lb.fit_transform(l_cat)\n",
    "    item_cat['item_category_name_fix'] = l_cat\n",
    "    train_test_set = train_test_set.merge(item_cat[['item_category_id', 'item_category_id_fix']], on = 'item_category_id', how = 'left')\n",
    "    _ = train_test_set.drop(['item_category_id'],axis=1, inplace=True)\n",
    "    train_test_set.rename(columns = {'item_category_id_fix':'item_category_id'}, inplace = True)\n",
    "\n",
    "    _ = item_cat.drop(['item_category_id'],axis=1, inplace=True)\n",
    "    _ = item_cat.drop(['item_category_name'],axis=1, inplace=True)\n",
    "\n",
    "    item_cat.rename(columns = {'item_category_id_fix':'item_category_id'}, inplace = True)\n",
    "    item_cat.rename(columns = {'item_category_name_fix':'item_category_name'}, inplace = True)\n",
    "    item_cat = item_cat.drop_duplicates()\n",
    "    item_cat.index = np.arange(0, len(item_cat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4142355194b030ae6e4d4a62e2ce1fea6935cf82"
   },
   "source": [
    "# Idea 0: 前回のショップ/アイテム販売を機能として追加（Lag機能)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "eb5eaa3887bac2c8e3934e40f62ede2fdbd1afb8",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    if enable_feature_idea[0]:\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = 'prev_shopitem_sales_' + str(diff)\n",
    "            trainset2 = train_test_set.copy()\n",
    "            trainset2.loc[:, 'date_block_num'] += diff\n",
    "            trainset2.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(trainset2[['shop_id', 'item_id', 'date_block_num', feature_name]], on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n",
    "            new_features.append(feature_name)\n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13e87f447a3d262ae8690918f827e59e9e262819"
   },
   "source": [
    "# Idea 1: 前アイテム販売を機能として追加（ラグ機能）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "0ee41b981d34911461476da515ff2379d85113f7",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    if enable_feature_idea[1]:\n",
    "        groups = train_test_set.groupby(by = ['item_id', 'date_block_num'])\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = 'prev_item_sales_' + str(diff)\n",
    "            result = groups.agg({'item_cnt_month':'mean'})\n",
    "            result = result.reset_index()\n",
    "            result.loc[:, 'date_block_num'] += diff\n",
    "            result.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n",
    "            new_features.append(feature_name)        \n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e9a874dad4a304013a4aec6d32be8f9a9e09dc4"
   },
   "source": [
    "# Idea 2: 前回のショップ/アイテム価格を機能として追加（Lag機能)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c05be3cb450b3f0a4bdd2a4f1570e369c783daf6",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    if enable_feature_idea[3]:\n",
    "        groups = train_test_set.groupby(by = ['shop_id', 'item_id', 'date_block_num'])\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = 'prev_shopitem_price_' + str(diff)\n",
    "            result = groups.agg({'item_price':'mean'})\n",
    "            result = result.reset_index()\n",
    "            result.loc[:, 'date_block_num'] += diff\n",
    "            result.rename(columns={'item_price': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(result, on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name]\n",
    "            new_features.append(feature_name)        \n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d630fd1b36c50da83d61d3dbe1f441bd6f9943e6"
   },
   "source": [
    "# Idea 3: 前のアイテムの価格を機能として追加（ラグ機能)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "d3a188b1a9228b1f07b689ade235e200fdbfa5e1",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    if enable_feature_idea[3]:\n",
    "        groups = train_test_set.groupby(by = ['item_id', 'date_block_num'])\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = 'prev_item_price_' + str(diff)\n",
    "            result = groups.agg({'item_price':'mean'})\n",
    "            result = result.reset_index()\n",
    "            result.loc[:, 'date_block_num'] += diff\n",
    "            result.rename(columns={'item_price': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name]\n",
    "            new_features.append(feature_name)        \n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1dd6fb36432c3a3a261d1738ad0da681cf95d65e"
   },
   "source": [
    "# Idea 4: ショップ/アイテムペアの平均エンコーディング(Mean encoding, doesnt work for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "a8a92fc73bfee328154f27053db08e213ea1e85e",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_mean_encodings(train_test_set, categorical_var_list, target):\n",
    "    feature_name = \"_\".join(categorical_var_list) + \"_\" + target + \"_mean\"\n",
    "\n",
    "    df = train_test_set.copy()\n",
    "    df1 = df[df.date_block_num <= 32]\n",
    "    df2 = df[df.date_block_num <= 33]\n",
    "    df3 = df[df.date_block_num == 34]\n",
    "\n",
    "    # Extract mean encodings using training data(here we don't use month 33 to avoid data leak on validation)\n",
    "    # If I try to extract mean encodings from all months, then val rmse decreases a tiny bit, but test rmse would increase by 4%\n",
    "    # So this is important\n",
    "    # トレーニングデータを使って平均エンコーディングを抽出する(ここでは、検証時のデータ漏洩を避けるため、33月は使用しない)\n",
    "    # すべての月から平均値を抽出しようとすると、valのrmsseはわずかに減少しますが、testのrmsseは4%増加します。\n",
    "    # だからこれは重要\n",
    "    mean_32 = df1[categorical_var_list + [target]].groupby(categorical_var_list, as_index=False)[[target]].mean()\n",
    "    mean_32 = mean_32.rename(columns={target:feature_name})\n",
    "\n",
    "    # 全データを使って平均値のエンコーディングを抽出し、これをテストデータに適用する。\n",
    "    mean_33 = df2[categorical_var_list + [target]].groupby(categorical_var_list, as_index=False)[[target]].mean()\n",
    "    mean_33 = mean_33.rename(columns={target:feature_name})\n",
    "\n",
    "    # Apply mean encodings\n",
    "    df2 = df2.merge(mean_32, on = categorical_var_list, how = 'left')\n",
    "    df3 = df3.merge(mean_33, on = categorical_var_list, how = 'left')\n",
    "\n",
    "    # Concatenate\n",
    "    train_test_set = pd.concat([df2, df3], axis = 0)\n",
    "    new_features.append(feature_name)\n",
    "    return train_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "45414a9761145946fbfb5a5d3f196272d933e2c6",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    create_mean_encodings(train_test_set, ['shop_id', 'item_id'], 'item_cnt_month')\n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0947f10980e5ff3b42685767ab45a68c2e519b0d"
   },
   "source": [
    "# Idea 5: Mean encodings for item (Mean encoding, doesnt work for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "161490603118eee6412dc4976b60a077066f2145",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    train_test_set = create_mean_encodings(train_test_set, ['item_id'], 'item_cnt_month')\n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3464c0c751405cd7f3ffa61410e5f7b7226507cf"
   },
   "source": [
    "# Idea 6: ショップ/アイテムの最終販売日からの月数（過去の情報を使用 (Use info from past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "ed61cfeb7168ac0f8f26e690122d1d38cabfa115",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_last_sale_shop_item(row):\n",
    "    for diff in range(1,33+1):\n",
    "        feature_name = '_prev_shopitem_sales_' + str(diff)\n",
    "        if row[feature_name] != 0.0:\n",
    "            return diff\n",
    "    return np.nan\n",
    "\n",
    "if kernel_with_output:\n",
    "    lookback_range = list(range(1, 33 + 1))\n",
    "    if enable_feature_idea[6]:\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = '_prev_shopitem_sales_' + str(diff)\n",
    "            trainset2 = train_test_set.copy()\n",
    "            trainset2.loc[:, 'date_block_num'] += diff\n",
    "            trainset2.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(trainset2[['shop_id', 'item_id', 'date_block_num', feature_name]], on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n",
    "            #new_features.append(feature_name)\n",
    "\n",
    "    train_test_set.loc[:, 'last_sale_shop_item'] = train_test_set.progress_apply (lambda row: create_last_sale_shop_item(row),axis=1)\n",
    "    new_features.append('last_sale_shop_item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6dcff93eb98178a767dfe08b3ed9f3612cbffc75"
   },
   "source": [
    "# Idea 7: 最後に商品を販売してからの月数(Use info from past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "bf86126642ac8f48d69c2a5125638ba6dd6c349f",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_last_sale_item(row):\n",
    "    for diff in range(1,33+1):\n",
    "        feature_name = '_prev_item_sales_' + str(diff)\n",
    "        if row[feature_name] != 0.0:\n",
    "            return diff\n",
    "    return np.nan\n",
    "if kernel_with_output:\n",
    "    lookback_range = list(range(1, 33 + 1))\n",
    "    if enable_feature_idea[1]:\n",
    "        groups = train_test_set.groupby(by = ['item_id', 'date_block_num'])\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = '_prev_item_sales_' + str(diff)\n",
    "            result = groups.agg({'item_cnt_month':'mean'})\n",
    "            result = result.reset_index()\n",
    "            result.loc[:, 'date_block_num'] += diff\n",
    "            result.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n",
    "            new_features.append(feature_name)        \n",
    "    train_test_set.loc[:, 'last_sale_item'] = train_test_set.progress_apply (lambda row: create_last_sale_item(row),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9b1198c05e4a86687c589559c966d6801af2a3e"
   },
   "source": [
    "# Idea 8: Item name (Tfidf text feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "b2508fdfb1a4a9b77465157480757e174b7f42f4",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    items_subset = items[['item_id', 'item_name']]\n",
    "    feature_count = 25\n",
    "    tfidf = TfidfVectorizer(max_features=feature_count)\n",
    "    items_df_item_name_text_features = pd.DataFrame(tfidf.fit_transform(items_subset['item_name']).toarray())\n",
    "\n",
    "    cols = items_df_item_name_text_features.columns\n",
    "    for i in range(feature_count):\n",
    "        feature_name = 'item_name_tfidf_' + str(i)\n",
    "        items_subset[feature_name] = items_df_item_name_text_features[cols[i]]\n",
    "        new_features.append(feature_name)\n",
    "\n",
    "    items_subset.drop('item_name', axis = 1, inplace = True)\n",
    "    train_test_set = train_test_set.merge(items_subset, on = 'item_id', how = 'left')\n",
    "    train_test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87511f885a323923e4aa92668b27bf34ec4bd29b"
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "75b8599039a9ef057f951f6d2ee1f4251d4355b3",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    current = time.time()\n",
    "\n",
    "    baseline_features = ['shop_id', 'item_id', 'item_category_id', 'date_block_num'] +  new_features + ['item_cnt_month']\n",
    "\n",
    "    # Clipping to range 0-20\n",
    "    train_test_set['item_cnt_month'] = train_test_set.item_cnt_month.fillna(0).clip(0,20)\n",
    "\n",
    "    # train: want rows with date_block_num from 0 to 31\n",
    "    train_time_range_lo = (train_test_set['date_block_num'] >= 0)\n",
    "    train_time_range_hi =  (train_test_set['date_block_num'] <= 32)\n",
    "\n",
    "    # val: want rows with date_block_num from 22\n",
    "    validation_time =  (train_test_set['date_block_num'] == 33)\n",
    "\n",
    "    # test: want rows with date_block_num from 34\n",
    "    test_time =  (train_test_set['date_block_num'] == 34)\n",
    "\n",
    "\n",
    "    # Retrieve rows for train set, val set, test set\n",
    "    cv_trainset = train_test_set[train_time_range_lo & train_time_range_hi]\n",
    "    cv_valset = train_test_set[validation_time]\n",
    "    cv_trainset = cv_trainset[baseline_features]\n",
    "    cv_valset = cv_valset[baseline_features]\n",
    "    testset = train_test_set[test_time]\n",
    "    testset = testset[baseline_features]\n",
    "\n",
    "    # Prepare numpy arrays for training/val/test\n",
    "    cv_trainset_vals = cv_trainset.values.astype(int)\n",
    "    trainx = cv_trainset_vals[:, 0:len(baseline_features) - 1]\n",
    "    trainy = cv_trainset_vals[:, len(baseline_features) - 1]\n",
    "\n",
    "    cv_valset_vals = cv_valset.values.astype(int)\n",
    "    valx = cv_valset_vals[:, 0:len(baseline_features) - 1]\n",
    "    valy = cv_valset_vals[:, len(baseline_features) - 1]\n",
    "\n",
    "    testset_vals = testset.values.astype(int)\n",
    "    testx = testset_vals[:, 0:len(baseline_features) - 1]\n",
    "\n",
    "    print('Fitting...')\n",
    "    model = xgb.XGBRegressor(max_depth = 11, min_child_weight=0.5, subsample = 1, eta = 0.3, num_round = 1000, seed = 1, nthread = 16)\n",
    "    model.fit(trainx, trainy, eval_metric='rmse')\n",
    "\n",
    "\n",
    "    preds = model.predict(valx)\n",
    "    # Clipping to range 0-20\n",
    "    preds = np.clip(preds, 0,20)\n",
    "    print('val set rmse: ', sqrt(mean_squared_error(valy, preds)))\n",
    "\n",
    "    preds = model.predict(testx)\n",
    "    # Clipping to range 0-20\n",
    "    preds = np.clip(preds, 0,20)\n",
    "    df = pd.DataFrame(preds, columns = ['item_cnt_month'])\n",
    "    df['ID'] = df.index\n",
    "    df = df.set_index('ID')\n",
    "    df.to_csv('test_preds.csv')\n",
    "    print('test predictions written to file')\n",
    "\n",
    "    end = time.time()\n",
    "    diff = end - current\n",
    "    print('Took ' + str(int(diff)) + ' seconds to train and predict val, test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4908eb443addb3140d8541c6ac863a37ebc6a44d"
   },
   "source": [
    "# Model Ensemble: Stacking\n",
    "\n",
    "I have tried to combine models from CatBoost, XGboost and LightGBM with stacking, but the results aren't as good as using XGboost alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1eeb8b7bd57b93ffb4319f2949556393bf51c525"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In the end, I got a rmse score of 0.89874 in the public leader board(top 11). For the private grader, I got rmse score of 0.88(yeah!! no overfitting)\n",
    "\n",
    "I learned one most important thing in this competition. Feature engineering is the single most important thing in machine learning! If you don't expose the interactions of data to your model explictily, then no matter how you tune your model, it can not learn those interactions between data!\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
