{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25337c8-37ca-4cf7-a90f-de807cebffbc",
   "metadata": {},
   "source": [
    "# Feature_generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1af73a-80d2-4f91-a7b4-669ee7d41438",
   "metadata": {},
   "source": [
    "PBL01_需要予測・在庫最適化　サンプルコード(PBL01_sample_code)   \n",
    "似ているkaggleのnotebook lag関数やgroupbyを踏まえて, データの整形やり直し"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665c5fb-0061-43ce-903d-b0f173ff460d",
   "metadata": {},
   "source": [
    "そこで作成したデータセットを使って, モデル作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc5b27-3662-4d71-85e2-df08ca72b832",
   "metadata": {},
   "source": [
    "# インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "234fb8a2-b4d5-45da-9c3d-d8bec3a9a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pandas_profiling as pdp\n",
    "import numpy as np\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "# 複数のリストの直積（デカルト積）を生成するためのライブラリ\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "#回帰の可視化\n",
    "#関数の処理で必要なライブラリ\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "\n",
    "import shap\n",
    "# import xgboost\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pillowをインポート\n",
    "from PIL import Image\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03835a8-c38f-4771-ae6d-5d21a5a5be73",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7882ee71-37fc-4f47-b19b-fd476ab93ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./inputs/dataset_addfeature_lag_trend.pickle')\n",
    "# test = pd.read_csv('./inputs/test.csv')\n",
    "sub = pd.read_csv('./inputs/sample_submission.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c37cc531-0d9e-4a9b-a598-aa84fb748111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['月ブロック', '店舗ID', '商品ID', '売上個数', 'year_cat', 'month_cat', 'year',\n",
       "       'month', '月ブロック_cat', 'holiday_cnt', 'month_cos', 'month_sin',\n",
       "       '商品カテゴリID', '商品カテゴリ名', '商品カテゴリ_type1', '商品カテゴリ_type2', '商品カテゴリ_type3',\n",
       "       '売上個数_lag_1', '売上個数_lag_2', '売上個数_lag_3', '売上個数_lag_6', '売上個数_lag_12',\n",
       "       'date_avg_売上個数_lag_1', 'date_avg_売上個数_lag_2', 'date_avg_売上個数_lag_3',\n",
       "       'date_avg_売上個数_lag_6', 'date_avg_売上個数_lag_12',\n",
       "       'date_item_avg_売上個数_lag_1', 'date_item_avg_売上個数_lag_2',\n",
       "       'date_item_avg_売上個数_lag_3', 'date_item_avg_売上個数_lag_6',\n",
       "       'date_item_avg_売上個数_lag_12', 'date_shop_avg_売上個数_lag_1',\n",
       "       'date_shop_avg_売上個数_lag_2', 'date_shop_avg_売上個数_lag_3',\n",
       "       'date_shop_avg_売上個数_lag_6', 'date_shop_avg_売上個数_lag_12',\n",
       "       'date_catname_avg_売上個数_lag_1', 'date_catname_avg_売上個数_lag_2',\n",
       "       'date_catname_avg_売上個数_lag_3', 'date_catname_avg_売上個数_lag_6',\n",
       "       'date_catname_avg_売上個数_lag_12', 'date_cattype1_avg_売上個数_lag_1',\n",
       "       'date_cattype1_avg_売上個数_lag_2', 'date_cattype1_avg_売上個数_lag_3',\n",
       "       'date_cattype1_avg_売上個数_lag_6', 'date_cattype1_avg_売上個数_lag_12',\n",
       "       'date_cattype2_avg_売上個数_lag_1', 'date_cattype2_avg_売上個数_lag_2',\n",
       "       'date_cattype2_avg_売上個数_lag_3', 'date_cattype2_avg_売上個数_lag_6',\n",
       "       'date_cattype2_avg_売上個数_lag_12', 'date_cattype3_avg_売上個数_lag_1',\n",
       "       'date_cattype3_avg_売上個数_lag_2', 'date_cattype3_avg_売上個数_lag_3',\n",
       "       'date_cattype3_avg_売上個数_lag_6', 'date_cattype3_avg_売上個数_lag_12',\n",
       "       'delta_価格_lag', 'delta_売上金額_lag_1', 'delta_売上金額_lag_2',\n",
       "       'delta_売上金額_lag_6', 'delta_売上金額_lag_12', 'days', 'item_shop_last_sale',\n",
       "       'item_shop_first_sale', 'item_first_sale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "363ccaad-620a-4a4b-85c8-e9e2c1f44658",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['月ブロック', '店舗ID', '商品ID', '売上個数', \n",
    "             'year_cat', 'month_cat', 'year','month', '月ブロック_cat', 'holiday_cnt', 'month_cos', 'month_sin',\n",
    "       #'商品カテゴリID', \n",
    "             '商品カテゴリ名', '商品カテゴリ_type1', '商品カテゴリ_type2', '商品カテゴリ_type3',\n",
    "       '売上個数_lag_1', '売上個数_lag_2', '売上個数_lag_3', '売上個数_lag_6', '売上個数_lag_12',\n",
    "       'date_avg_売上個数_lag_1', 'date_avg_売上個数_lag_2', 'date_avg_売上個数_lag_3',\n",
    "       'date_avg_売上個数_lag_6', 'date_avg_売上個数_lag_12',\n",
    "       'date_item_avg_売上個数_lag_1', 'date_item_avg_売上個数_lag_2',\n",
    "       'date_item_avg_売上個数_lag_3', 'date_item_avg_売上個数_lag_6',\n",
    "       'date_item_avg_売上個数_lag_12', 'date_shop_avg_売上個数_lag_1',\n",
    "       'date_shop_avg_売上個数_lag_2', 'date_shop_avg_売上個数_lag_3',\n",
    "       'date_shop_avg_売上個数_lag_6', 'date_shop_avg_売上個数_lag_12',\n",
    "       'date_catname_avg_売上個数_lag_1', 'date_catname_avg_売上個数_lag_2',\n",
    "       'date_catname_avg_売上個数_lag_3', 'date_catname_avg_売上個数_lag_6',\n",
    "       'date_catname_avg_売上個数_lag_12', 'date_cattype1_avg_売上個数_lag_1',\n",
    "       'date_cattype1_avg_売上個数_lag_2', 'date_cattype1_avg_売上個数_lag_3',\n",
    "       'date_cattype1_avg_売上個数_lag_6', 'date_cattype1_avg_売上個数_lag_12',\n",
    "       'date_cattype2_avg_売上個数_lag_1', 'date_cattype2_avg_売上個数_lag_2',\n",
    "       'date_cattype2_avg_売上個数_lag_3', 'date_cattype2_avg_売上個数_lag_6',\n",
    "       'date_cattype2_avg_売上個数_lag_12', 'date_cattype3_avg_売上個数_lag_1',\n",
    "       'date_cattype3_avg_売上個数_lag_2', 'date_cattype3_avg_売上個数_lag_3',\n",
    "       'date_cattype3_avg_売上個数_lag_6', 'date_cattype3_avg_売上個数_lag_12',\n",
    "       'delta_価格_lag', 'delta_売上金額_lag_1', 'delta_売上金額_lag_2',\n",
    "       'delta_売上金額_lag_6', 'delta_売上金額_lag_12', 'days', 'item_shop_last_sale',\n",
    "       'item_shop_first_sale', 'item_first_sale']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2741c-95a2-4f06-9a85-ae78ef888ee8",
   "metadata": {},
   "source": [
    "## 型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53fdf4c2-1c21-45ea-a5c2-eda19df8a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = [col for col in data.select_dtypes(include=object)]\n",
    "num_col = [col for col in data.select_dtypes(exclude=object)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984e5a7-32ef-4709-879d-ff5536ef7526",
   "metadata": {},
   "source": [
    "# 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35aaa178-8207-496b-94db-b6e887b44c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#説明変数を対数変換\n",
    "def logarithmic_transformation(df):\n",
    "    num_col = [col for col in df.select_dtypes(exclude=object)]\n",
    "    \n",
    "    print(num_col)\n",
    "    #各説明変数の歪度を計算\n",
    "    skewed_feats = df[num_col].apply(lambda x: x.skew()).sort_values(ascending = False)\n",
    "    \n",
    "    #歪度の絶対値が0.5より大きい変数だけに絞る\n",
    "    skewed_feats_over = skewed_feats[abs(skewed_feats) > 0.5]\n",
    "    \n",
    "    #欠損値のないものに絞る\n",
    "    num_col_feat_list = []\n",
    "    for i in skewed_feats_over.index:\n",
    "        flag = df[i].isnull().any()\n",
    "        if not flag:\n",
    "            num_col_feat_list.append(i)\n",
    "\n",
    "    print(num_col_feat_list)\n",
    "    \n",
    "    #グラフ化\n",
    "    skewed_feats_over_plot = skewed_feats_over[num_col_feat_list]\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.xticks(rotation='90')\n",
    "    sns.barplot(x=skewed_feats_over_plot.index, y=skewed_feats_over_plot)\n",
    "    \n",
    "    #Yeo-Johnson変換\n",
    "    pt = PowerTransformer()\n",
    "    pt.fit(df[num_col_feat_list])\n",
    "\n",
    "    #変換後のデータで各列を置換\n",
    "    tmp = pd.DataFrame()\n",
    "    tmp[num_col_feat_list] = pt.transform(df[num_col_feat_list])\n",
    "    tmp = tmp.add_prefix('Log_')\n",
    "    df[tmp.columns] = tmp\n",
    "    \n",
    "    #各説明変数の歪度を計算\n",
    "    skewed_feats_fixed = df[tmp.columns].apply(lambda x:x.skew()).sort_values(ascending = False)\n",
    "\n",
    "    #グラフ化\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.xticks(rotation='90')\n",
    "    sns.barplot(x=skewed_feats_fixed.index, y=skewed_feats_fixed)\n",
    "\n",
    "    return df, num_col_feat_list, pt\n",
    "\n",
    "\"\"\"==================================================\"\"\"\n",
    "\n",
    "#カテゴリカル変数化\n",
    "def process_categorical(df, target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "\n",
    "    #ターゲットカラム以外にカテゴリ変数があれば, ダミー変数にする\n",
    "    #df2 = pd.get_dummies(df2, drop_first=True)\n",
    "    df2 = pd.get_dummies(df2)\n",
    "\n",
    "    for column in tqdm(target_columns):\n",
    "        df2[column] = df2[column].astype('category')\n",
    "\n",
    "    return df2\n",
    "\n",
    "\"\"\"==================================================\"\"\"\n",
    "\n",
    "#予測値と正解値を描写する関数\n",
    "def True_Pred_map(pred_df):\n",
    "    RMSE = np.sqrt(mean_squared_error(pred_df['true'], pred_df['pred']))\n",
    "    R2 = r2_score(pred_df['true'], pred_df['pred'])\n",
    "    plt.figure(figsize=(8,8))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.scatter('true', 'pred', data=pred_df)\n",
    "    ax.set_xlabel('True Value', fontsize=15)\n",
    "    ax.set_ylabel('Pred Value', fontsize=15)\n",
    "    ax.set_xlim(pred_df.min().min()-0.05 , pred_df.max().max()+0.05)\n",
    "    ax.set_ylim(pred_df.min().min()-0.05 , pred_df.max().max()+0.05)\n",
    "    x = np.linspace(pred_df.min().min()-0.05, pred_df.max().max()+0.05, 2)\n",
    "    y = x\n",
    "    ax.plot(x,y,'r-')\n",
    "    plt.text(0.1, 0.9, 'RMSE = {}'.format(str(round(RMSE, 5))), transform=ax.transAxes, fontsize=15)\n",
    "    plt.text(0.1, 0.8, 'R^2 = {}'.format(str(round(R2, 5))), transform=ax.transAxes, fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84144e17-782d-4c47-9547-027e63024346",
   "metadata": {},
   "source": [
    "# データセットの整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "56757c60-bf1a-4354-b6b7-b333be9aa9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data[(data['月ブロック'] >= 12)].drop(['売上個数'], axis=1)\n",
    "y_data = data[(data['月ブロック'] >= 12)]['売上個数']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd3b2698-97f9-49bf-8712-dbcada5f8528",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>月ブロック</th>\n",
       "      <th>店舗ID</th>\n",
       "      <th>商品ID</th>\n",
       "      <th>year_cat</th>\n",
       "      <th>month_cat</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>月ブロック_cat</th>\n",
       "      <th>holiday_cnt</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>商品カテゴリ名</th>\n",
       "      <th>商品カテゴリ_type1</th>\n",
       "      <th>商品カテゴリ_type2</th>\n",
       "      <th>商品カテゴリ_type3</th>\n",
       "      <th>売上個数_lag_1</th>\n",
       "      <th>売上個数_lag_2</th>\n",
       "      <th>売上個数_lag_3</th>\n",
       "      <th>売上個数_lag_6</th>\n",
       "      <th>売上個数_lag_12</th>\n",
       "      <th>date_avg_売上個数_lag_1</th>\n",
       "      <th>date_avg_売上個数_lag_2</th>\n",
       "      <th>date_avg_売上個数_lag_3</th>\n",
       "      <th>date_avg_売上個数_lag_6</th>\n",
       "      <th>date_avg_売上個数_lag_12</th>\n",
       "      <th>date_item_avg_売上個数_lag_1</th>\n",
       "      <th>date_item_avg_売上個数_lag_2</th>\n",
       "      <th>date_item_avg_売上個数_lag_3</th>\n",
       "      <th>date_item_avg_売上個数_lag_6</th>\n",
       "      <th>date_item_avg_売上個数_lag_12</th>\n",
       "      <th>date_shop_avg_売上個数_lag_1</th>\n",
       "      <th>date_shop_avg_売上個数_lag_2</th>\n",
       "      <th>date_shop_avg_売上個数_lag_3</th>\n",
       "      <th>date_shop_avg_売上個数_lag_6</th>\n",
       "      <th>date_shop_avg_売上個数_lag_12</th>\n",
       "      <th>date_catname_avg_売上個数_lag_1</th>\n",
       "      <th>date_catname_avg_売上個数_lag_2</th>\n",
       "      <th>date_catname_avg_売上個数_lag_3</th>\n",
       "      <th>date_catname_avg_売上個数_lag_6</th>\n",
       "      <th>date_catname_avg_売上個数_lag_12</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_1</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_2</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_3</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_6</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_12</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_1</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_2</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_3</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_6</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_12</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_1</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_2</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_3</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_6</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_12</th>\n",
       "      <th>delta_価格_lag</th>\n",
       "      <th>delta_売上金額_lag_1</th>\n",
       "      <th>delta_売上金額_lag_2</th>\n",
       "      <th>delta_売上金額_lag_6</th>\n",
       "      <th>delta_売上金額_lag_12</th>\n",
       "      <th>days</th>\n",
       "      <th>item_shop_last_sale</th>\n",
       "      <th>item_shop_first_sale</th>\n",
       "      <th>item_first_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331704</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1000001</td>\n",
       "      <td>2019</td>\n",
       "      <td>01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>映画 - DVD</td>\n",
       "      <td>映画</td>\n",
       "      <td>DVD</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>1.388672</td>\n",
       "      <td>1.666992</td>\n",
       "      <td>1.888672</td>\n",
       "      <td>2.833984</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142090</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331705</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1000002</td>\n",
       "      <td>2019</td>\n",
       "      <td>01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>映画 - DVD</td>\n",
       "      <td>映画</td>\n",
       "      <td>DVD</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>1.388672</td>\n",
       "      <td>0.611328</td>\n",
       "      <td>0.555664</td>\n",
       "      <td>0.777832</td>\n",
       "      <td>1.555664</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054840</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331706</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1000003</td>\n",
       "      <td>2019</td>\n",
       "      <td>01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>映画 - DVD</td>\n",
       "      <td>映画</td>\n",
       "      <td>DVD</td>\n",
       "      <td>None</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>0.888672</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333252</td>\n",
       "      <td>0.666504</td>\n",
       "      <td>0.333252</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093506</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331707</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1000004</td>\n",
       "      <td>2019</td>\n",
       "      <td>01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>映画 - DVD</td>\n",
       "      <td>映画</td>\n",
       "      <td>DVD</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.611328</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.666504</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116089</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331708</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1000005</td>\n",
       "      <td>2019</td>\n",
       "      <td>01</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>映画 - DVD</td>\n",
       "      <td>映画</td>\n",
       "      <td>DVD</td>\n",
       "      <td>None</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>0.944336</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019363</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655023</th>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>3500001</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>本 - オーディオブック_タイプB</td>\n",
       "      <td>本</td>\n",
       "      <td>オーディオブック</td>\n",
       "      <td>タイプB</td>\n",
       "      <td>0.772949</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>1.370117</td>\n",
       "      <td>1.331055</td>\n",
       "      <td>1.257812</td>\n",
       "      <td>1.238281</td>\n",
       "      <td>1.966797</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.121765</td>\n",
       "      <td>-0.202026</td>\n",
       "      <td>0.755371</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655024</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>3500001</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>本 - オーディオブック_タイプB</td>\n",
       "      <td>本</td>\n",
       "      <td>オーディオブック</td>\n",
       "      <td>タイプB</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.433105</td>\n",
       "      <td>0.350342</td>\n",
       "      <td>0.335693</td>\n",
       "      <td>0.372559</td>\n",
       "      <td>0.711426</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.462402</td>\n",
       "      <td>-0.441895</td>\n",
       "      <td>0.650391</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655025</th>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>3500001</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>本 - オーディオブック_タイプB</td>\n",
       "      <td>本</td>\n",
       "      <td>オーディオブック</td>\n",
       "      <td>タイプB</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.317871</td>\n",
       "      <td>0.520996</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>0.423340</td>\n",
       "      <td>0.699219</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.066284</td>\n",
       "      <td>-0.271240</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655026</th>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>3500001</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>本 - オーディオブック_タイプB</td>\n",
       "      <td>本</td>\n",
       "      <td>オーディオブック</td>\n",
       "      <td>タイプB</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.339844</td>\n",
       "      <td>0.411377</td>\n",
       "      <td>0.405518</td>\n",
       "      <td>0.346680</td>\n",
       "      <td>0.569824</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>-0.263184</td>\n",
       "      <td>0.647461</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655027</th>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>3500001</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>本 - オーディオブック_タイプB</td>\n",
       "      <td>本</td>\n",
       "      <td>オーディオブック</td>\n",
       "      <td>タイプB</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.553711</td>\n",
       "      <td>0.636719</td>\n",
       "      <td>0.645996</td>\n",
       "      <td>0.616699</td>\n",
       "      <td>0.947754</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.083984</td>\n",
       "      <td>-0.143921</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>665946 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         月ブロック 店舗ID     商品ID year_cat month_cat  year  month 月ブロック_cat  \\\n",
       "331704      12    0  1000001     2019        01  2019      1        12   \n",
       "331705      12    0  1000002     2019        01  2019      1        12   \n",
       "331706      12    0  1000003     2019        01  2019      1        12   \n",
       "331707      12    0  1000004     2019        01  2019      1        12   \n",
       "331708      12    0  1000005     2019        01  2019      1        12   \n",
       "...        ...  ...      ...      ...       ...   ...    ...       ...   \n",
       "1655023     23   13  3500001     2019        12  2019     12        23   \n",
       "1655024     23   14  3500001     2019        12  2019     12        23   \n",
       "1655025     23   15  3500001     2019        12  2019     12        23   \n",
       "1655026     23   16  3500001     2019        12  2019     12        23   \n",
       "1655027     23   17  3500001     2019        12  2019     12        23   \n",
       "\n",
       "         holiday_cnt  month_cos     month_sin            商品カテゴリ名 商品カテゴリ_type1  \\\n",
       "331704             2   0.866025  5.000000e-01           映画 - DVD          映画    \n",
       "331705             2   0.866025  5.000000e-01           映画 - DVD          映画    \n",
       "331706             2   0.866025  5.000000e-01           映画 - DVD          映画    \n",
       "331707             2   0.866025  5.000000e-01           映画 - DVD          映画    \n",
       "331708             2   0.866025  5.000000e-01           映画 - DVD          映画    \n",
       "...              ...        ...           ...                ...          ...   \n",
       "1655023            0   1.000000 -2.449294e-16  本 - オーディオブック_タイプB           本    \n",
       "1655024            0   1.000000 -2.449294e-16  本 - オーディオブック_タイプB           本    \n",
       "1655025            0   1.000000 -2.449294e-16  本 - オーディオブック_タイプB           本    \n",
       "1655026            0   1.000000 -2.449294e-16  本 - オーディオブック_タイプB           本    \n",
       "1655027            0   1.000000 -2.449294e-16  本 - オーディオブック_タイプB           本    \n",
       "\n",
       "        商品カテゴリ_type2 商品カテゴリ_type3  売上個数_lag_1  売上個数_lag_2  売上個数_lag_3  \\\n",
       "331704           DVD         None    0.000000         2.0         0.0   \n",
       "331705           DVD         None    0.000000         0.0         1.0   \n",
       "331706           DVD         None    2.000000         0.0         0.0   \n",
       "331707           DVD         None    0.000000         0.0         0.0   \n",
       "331708           DVD         None    1.000000         1.0         2.0   \n",
       "...              ...          ...         ...         ...         ...   \n",
       "1655023     オーディオブック         タイプB    0.772949         2.0         3.0   \n",
       "1655024     オーディオブック         タイプB    0.000000         0.0         0.0   \n",
       "1655025     オーディオブック         タイプB    0.000000         0.0         0.0   \n",
       "1655026     オーディオブック         タイプB    0.000000         0.0         0.0   \n",
       "1655027     オーディオブック         タイプB    0.000000         0.0         0.0   \n",
       "\n",
       "         売上個数_lag_6  売上個数_lag_12  date_avg_売上個数_lag_1  date_avg_売上個数_lag_2  \\\n",
       "331704          3.0          6.0             1.066406             0.770508   \n",
       "331705          1.0          2.0             1.066406             0.770508   \n",
       "331706          1.0          0.0             1.066406             0.770508   \n",
       "331707          0.0          1.0             1.066406             0.770508   \n",
       "331708          3.0          8.0             1.066406             0.770508   \n",
       "...             ...          ...                  ...                  ...   \n",
       "1655023         1.0          3.0             0.619629             0.791016   \n",
       "1655024         0.0          0.0             0.619629             0.791016   \n",
       "1655025         0.0          0.0             0.619629             0.791016   \n",
       "1655026         0.0          0.0             0.619629             0.791016   \n",
       "1655027         0.0          1.0             0.619629             0.791016   \n",
       "\n",
       "         date_avg_売上個数_lag_3  date_avg_売上個数_lag_6  date_avg_売上個数_lag_12  \\\n",
       "331704              0.769043             0.730469              0.834473   \n",
       "331705              0.769043             0.730469              0.834473   \n",
       "331706              0.769043             0.730469              0.834473   \n",
       "331707              0.769043             0.730469              0.834473   \n",
       "331708              0.769043             0.730469              0.834473   \n",
       "...                      ...                  ...                   ...   \n",
       "1655023             0.661621             0.690918              1.066406   \n",
       "1655024             0.661621             0.690918              1.066406   \n",
       "1655025             0.661621             0.690918              1.066406   \n",
       "1655026             0.661621             0.690918              1.066406   \n",
       "1655027             0.661621             0.690918              1.066406   \n",
       "\n",
       "         date_item_avg_売上個数_lag_1  date_item_avg_売上個数_lag_2  \\\n",
       "331704                   2.111328                  1.388672   \n",
       "331705                   1.388672                  0.611328   \n",
       "331706                   0.888672                  0.500000   \n",
       "331707                   1.000000                  0.611328   \n",
       "331708                   1.333008                  1.000000   \n",
       "...                           ...                       ...   \n",
       "1655023                  0.128784                  0.277832   \n",
       "1655024                  0.128784                  0.277832   \n",
       "1655025                  0.128784                  0.277832   \n",
       "1655026                  0.128784                  0.277832   \n",
       "1655027                  0.128784                  0.277832   \n",
       "\n",
       "         date_item_avg_売上個数_lag_3  date_item_avg_売上個数_lag_6  \\\n",
       "331704                   1.666992                  1.888672   \n",
       "331705                   0.555664                  0.777832   \n",
       "331706                   0.333252                  0.666504   \n",
       "331707                   0.277832                  0.388916   \n",
       "331708                   1.222656                  0.944336   \n",
       "...                           ...                       ...   \n",
       "1655023                  0.277832                  0.388916   \n",
       "1655024                  0.277832                  0.388916   \n",
       "1655025                  0.277832                  0.388916   \n",
       "1655026                  0.277832                  0.388916   \n",
       "1655027                  0.277832                  0.388916   \n",
       "\n",
       "         date_item_avg_売上個数_lag_12  date_shop_avg_売上個数_lag_1  \\\n",
       "331704                    2.833984                  1.422852   \n",
       "331705                    1.555664                  1.422852   \n",
       "331706                    0.333252                  1.422852   \n",
       "331707                    0.666504                  1.422852   \n",
       "331708                    1.222656                  1.422852   \n",
       "...                            ...                       ...   \n",
       "1655023                   0.444336                  1.370117   \n",
       "1655024                   0.444336                  0.433105   \n",
       "1655025                   0.444336                  0.317871   \n",
       "1655026                   0.444336                  0.339844   \n",
       "1655027                   0.444336                  0.553711   \n",
       "\n",
       "         date_shop_avg_売上個数_lag_2  date_shop_avg_売上個数_lag_3  \\\n",
       "331704                   1.133789                  0.999023   \n",
       "331705                   1.133789                  0.999023   \n",
       "331706                   1.133789                  0.999023   \n",
       "331707                   1.133789                  0.999023   \n",
       "331708                   1.133789                  0.999023   \n",
       "...                           ...                       ...   \n",
       "1655023                  1.331055                  1.257812   \n",
       "1655024                  0.350342                  0.335693   \n",
       "1655025                  0.520996                  0.505859   \n",
       "1655026                  0.411377                  0.405518   \n",
       "1655027                  0.636719                  0.645996   \n",
       "\n",
       "         date_shop_avg_売上個数_lag_6  date_shop_avg_売上個数_lag_12  \\\n",
       "331704                   1.103516                   2.062500   \n",
       "331705                   1.103516                   2.062500   \n",
       "331706                   1.103516                   2.062500   \n",
       "331707                   1.103516                   2.062500   \n",
       "331708                   1.103516                   2.062500   \n",
       "...                           ...                        ...   \n",
       "1655023                  1.238281                   1.966797   \n",
       "1655024                  0.372559                   0.711426   \n",
       "1655025                  0.423340                   0.699219   \n",
       "1655026                  0.346680                   0.569824   \n",
       "1655027                  0.616699                   0.947754   \n",
       "\n",
       "         date_catname_avg_売上個数_lag_1  date_catname_avg_売上個数_lag_2  \\\n",
       "331704                      0.738281                     0.625488   \n",
       "331705                      0.738281                     0.625488   \n",
       "331706                      0.738281                     0.625488   \n",
       "331707                      0.738281                     0.625488   \n",
       "331708                      0.738281                     0.625488   \n",
       "...                              ...                          ...   \n",
       "1655023                     0.128784                     0.140991   \n",
       "1655024                     0.128784                     0.140991   \n",
       "1655025                     0.128784                     0.140991   \n",
       "1655026                     0.128784                     0.140991   \n",
       "1655027                     0.128784                     0.140991   \n",
       "\n",
       "         date_catname_avg_売上個数_lag_3  date_catname_avg_売上個数_lag_6  \\\n",
       "331704                      0.618652                     0.574219   \n",
       "331705                      0.618652                     0.574219   \n",
       "331706                      0.618652                     0.574219   \n",
       "331707                      0.618652                     0.574219   \n",
       "331708                      0.618652                     0.574219   \n",
       "...                              ...                          ...   \n",
       "1655023                     0.192139                     0.206055   \n",
       "1655024                     0.192139                     0.206055   \n",
       "1655025                     0.192139                     0.206055   \n",
       "1655026                     0.192139                     0.206055   \n",
       "1655027                     0.192139                     0.206055   \n",
       "\n",
       "         date_catname_avg_売上個数_lag_12  date_cattype1_avg_売上個数_lag_1  \\\n",
       "331704                       0.740723                      0.707520   \n",
       "331705                       0.740723                      0.707520   \n",
       "331706                       0.740723                      0.707520   \n",
       "331707                       0.740723                      0.707520   \n",
       "331708                       0.740723                      0.707520   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.352539                      0.257568   \n",
       "1655024                      0.352539                      0.257568   \n",
       "1655025                      0.352539                      0.257568   \n",
       "1655026                      0.352539                      0.257568   \n",
       "1655027                      0.352539                      0.257568   \n",
       "\n",
       "         date_cattype1_avg_売上個数_lag_2  date_cattype1_avg_売上個数_lag_3  \\\n",
       "331704                       0.572266                      0.555176   \n",
       "331705                       0.572266                      0.555176   \n",
       "331706                       0.572266                      0.555176   \n",
       "331707                       0.572266                      0.555176   \n",
       "331708                       0.572266                      0.555176   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.303711                      0.249023   \n",
       "1655024                      0.303711                      0.249023   \n",
       "1655025                      0.303711                      0.249023   \n",
       "1655026                      0.303711                      0.249023   \n",
       "1655027                      0.303711                      0.249023   \n",
       "\n",
       "         date_cattype1_avg_売上個数_lag_6  date_cattype1_avg_売上個数_lag_12  \\\n",
       "331704                       0.572754                        0.69043   \n",
       "331705                       0.572754                        0.69043   \n",
       "331706                       0.572754                        0.69043   \n",
       "331707                       0.572754                        0.69043   \n",
       "331708                       0.572754                        0.69043   \n",
       "...                               ...                            ...   \n",
       "1655023                      0.275635                        0.36377   \n",
       "1655024                      0.275635                        0.36377   \n",
       "1655025                      0.275635                        0.36377   \n",
       "1655026                      0.275635                        0.36377   \n",
       "1655027                      0.275635                        0.36377   \n",
       "\n",
       "         date_cattype2_avg_売上個数_lag_1  date_cattype2_avg_売上個数_lag_2  \\\n",
       "331704                       0.738281                      0.625488   \n",
       "331705                       0.738281                      0.625488   \n",
       "331706                       0.738281                      0.625488   \n",
       "331707                       0.738281                      0.625488   \n",
       "331708                       0.738281                      0.625488   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.193237                      0.216064   \n",
       "1655024                      0.193237                      0.216064   \n",
       "1655025                      0.193237                      0.216064   \n",
       "1655026                      0.193237                      0.216064   \n",
       "1655027                      0.193237                      0.216064   \n",
       "\n",
       "         date_cattype2_avg_売上個数_lag_3  date_cattype2_avg_売上個数_lag_6  \\\n",
       "331704                       0.618652                      0.574219   \n",
       "331705                       0.618652                      0.574219   \n",
       "331706                       0.618652                      0.574219   \n",
       "331707                       0.618652                      0.574219   \n",
       "331708                       0.618652                      0.574219   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.197266                      0.239624   \n",
       "1655024                      0.197266                      0.239624   \n",
       "1655025                      0.197266                      0.239624   \n",
       "1655026                      0.197266                      0.239624   \n",
       "1655027                      0.197266                      0.239624   \n",
       "\n",
       "         date_cattype2_avg_売上個数_lag_12  date_cattype3_avg_売上個数_lag_1  \\\n",
       "331704                        0.740723                           NaN   \n",
       "331705                        0.740723                           NaN   \n",
       "331706                        0.740723                           NaN   \n",
       "331707                        0.740723                           NaN   \n",
       "331708                        0.740723                           NaN   \n",
       "...                                ...                           ...   \n",
       "1655023                       0.312256                      0.300537   \n",
       "1655024                       0.312256                      0.300537   \n",
       "1655025                       0.312256                      0.300537   \n",
       "1655026                       0.312256                      0.300537   \n",
       "1655027                       0.312256                      0.300537   \n",
       "\n",
       "         date_cattype3_avg_売上個数_lag_2  date_cattype3_avg_売上個数_lag_3  \\\n",
       "331704                            NaN                           NaN   \n",
       "331705                            NaN                           NaN   \n",
       "331706                            NaN                           NaN   \n",
       "331707                            NaN                           NaN   \n",
       "331708                            NaN                           NaN   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.938965                      0.366943   \n",
       "1655024                      0.938965                      0.366943   \n",
       "1655025                      0.938965                      0.366943   \n",
       "1655026                      0.938965                      0.366943   \n",
       "1655027                      0.938965                      0.366943   \n",
       "\n",
       "         date_cattype3_avg_売上個数_lag_6  date_cattype3_avg_売上個数_lag_12  \\\n",
       "331704                            NaN                            NaN   \n",
       "331705                            NaN                            NaN   \n",
       "331706                            NaN                            NaN   \n",
       "331707                            NaN                            NaN   \n",
       "331708                            NaN                            NaN   \n",
       "...                               ...                            ...   \n",
       "1655023                      0.464844                       1.077148   \n",
       "1655024                      0.464844                       1.077148   \n",
       "1655025                      0.464844                       1.077148   \n",
       "1655026                      0.464844                       1.077148   \n",
       "1655027                      0.464844                       1.077148   \n",
       "\n",
       "         delta_価格_lag  delta_売上金額_lag_1  delta_売上金額_lag_2  delta_売上金額_lag_6  \\\n",
       "331704       0.142090          0.652832          0.230103          0.013771   \n",
       "331705       0.054840          0.652832          0.230103          0.013771   \n",
       "331706       0.093506          0.652832          0.230103          0.013771   \n",
       "331707       0.116089          0.652832          0.230103          0.013771   \n",
       "331708       0.019363          0.652832          0.230103          0.013771   \n",
       "...               ...               ...               ...               ...   \n",
       "1655023      0.000000               NaN         -0.121765         -0.202026   \n",
       "1655024      0.000000               NaN         -0.462402         -0.441895   \n",
       "1655025      0.000000               NaN         -0.066284         -0.271240   \n",
       "1655026      0.000000               NaN         -0.096680         -0.263184   \n",
       "1655027      0.000000               NaN         -0.083984         -0.143921   \n",
       "\n",
       "         delta_売上金額_lag_12  days  item_shop_last_sale  item_shop_first_sale  \\\n",
       "331704            0.696777    31                    1                    12   \n",
       "331705            0.696777    31                    1                    12   \n",
       "331706            0.696777    31                    1                    12   \n",
       "331707            0.696777    31                    1                    12   \n",
       "331708            0.696777    31                    1                    12   \n",
       "...                    ...   ...                  ...                   ...   \n",
       "1655023           0.755371    31                    1                    23   \n",
       "1655024           0.650391    31                    1                    23   \n",
       "1655025           0.726562    31                    1                    23   \n",
       "1655026           0.647461    31                    1                    23   \n",
       "1655027           0.937500    31                    1                    23   \n",
       "\n",
       "         item_first_sale  \n",
       "331704                12  \n",
       "331705                12  \n",
       "331706                12  \n",
       "331707                12  \n",
       "331708                12  \n",
       "...                  ...  \n",
       "1655023               23  \n",
       "1655024               23  \n",
       "1655025               23  \n",
       "1655026               23  \n",
       "1655027               23  \n",
       "\n",
       "[665946 rows x 64 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "749f76bb-2f02-49ba-9e39-2ee6690d9975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331704     2.0\n",
       "331705     0.0\n",
       "331706     0.0\n",
       "331707     0.0\n",
       "331708     1.0\n",
       "          ... \n",
       "1655023    NaN\n",
       "1655024    NaN\n",
       "1655025    NaN\n",
       "1655026    NaN\n",
       "1655027    NaN\n",
       "Name: 売上個数, Length: 665946, dtype: float16"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e1114-8247-4aba-978f-ddb88471c9fe",
   "metadata": {},
   "source": [
    "## カテゴリ変数化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce344005-9b0a-46fb-aa91-8a08996f5285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "店舗ID\n",
      "18\n",
      "****************************************************************************************************\n",
      "商品ID\n",
      "6629\n",
      "****************************************************************************************************\n",
      "year_cat\n",
      "1\n",
      "****************************************************************************************************\n",
      "month_cat\n",
      "12\n",
      "****************************************************************************************************\n",
      "月ブロック_cat\n",
      "12\n",
      "****************************************************************************************************\n",
      "商品カテゴリ名\n",
      "26\n",
      "****************************************************************************************************\n",
      "商品カテゴリ_type1\n",
      "8\n",
      "****************************************************************************************************\n",
      "商品カテゴリ_type2\n",
      "22\n",
      "****************************************************************************************************\n",
      "商品カテゴリ_type3\n",
      "3\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for cat in cat_col:\n",
    "    print(cat)\n",
    "    print(X_data[cat].nunique(dropna=False))\n",
    "    print('*' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57aebc4d-061b-41f9-bcac-87ef4ddbc22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c08e71f36604bf1b0a8a9962ef85b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#labelencodingするカラム → コメントアウト one-hot変換\n",
    "labelencoding_columns = [\n",
    "    #'店舗ID',\n",
    " '商品ID',\n",
    " #'year_cat',\n",
    " 'month_cat',\n",
    " '月ブロック_cat',\n",
    " '商品カテゴリ名',\n",
    " '商品カテゴリ_type1',\n",
    " '商品カテゴリ_type2',\n",
    " '商品カテゴリ_type3']\n",
    "\n",
    "X_data_dummy = process_categorical(X_data, labelencoding_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9fd9bf92-36f0-47e6-8a1d-f98d1a90509d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>月ブロック</th>\n",
       "      <th>商品ID</th>\n",
       "      <th>month_cat</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>月ブロック_cat</th>\n",
       "      <th>holiday_cnt</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>商品カテゴリ名</th>\n",
       "      <th>商品カテゴリ_type1</th>\n",
       "      <th>商品カテゴリ_type2</th>\n",
       "      <th>商品カテゴリ_type3</th>\n",
       "      <th>売上個数_lag_1</th>\n",
       "      <th>売上個数_lag_2</th>\n",
       "      <th>売上個数_lag_3</th>\n",
       "      <th>売上個数_lag_6</th>\n",
       "      <th>売上個数_lag_12</th>\n",
       "      <th>date_avg_売上個数_lag_1</th>\n",
       "      <th>date_avg_売上個数_lag_2</th>\n",
       "      <th>date_avg_売上個数_lag_3</th>\n",
       "      <th>date_avg_売上個数_lag_6</th>\n",
       "      <th>date_avg_売上個数_lag_12</th>\n",
       "      <th>date_item_avg_売上個数_lag_1</th>\n",
       "      <th>date_item_avg_売上個数_lag_2</th>\n",
       "      <th>date_item_avg_売上個数_lag_3</th>\n",
       "      <th>date_item_avg_売上個数_lag_6</th>\n",
       "      <th>date_item_avg_売上個数_lag_12</th>\n",
       "      <th>date_shop_avg_売上個数_lag_1</th>\n",
       "      <th>date_shop_avg_売上個数_lag_2</th>\n",
       "      <th>date_shop_avg_売上個数_lag_3</th>\n",
       "      <th>date_shop_avg_売上個数_lag_6</th>\n",
       "      <th>date_shop_avg_売上個数_lag_12</th>\n",
       "      <th>date_catname_avg_売上個数_lag_1</th>\n",
       "      <th>date_catname_avg_売上個数_lag_2</th>\n",
       "      <th>date_catname_avg_売上個数_lag_3</th>\n",
       "      <th>date_catname_avg_売上個数_lag_6</th>\n",
       "      <th>date_catname_avg_売上個数_lag_12</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_1</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_2</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_3</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_6</th>\n",
       "      <th>date_cattype1_avg_売上個数_lag_12</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_1</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_2</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_3</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_6</th>\n",
       "      <th>date_cattype2_avg_売上個数_lag_12</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_1</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_2</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_3</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_6</th>\n",
       "      <th>date_cattype3_avg_売上個数_lag_12</th>\n",
       "      <th>delta_価格_lag</th>\n",
       "      <th>delta_売上金額_lag_1</th>\n",
       "      <th>delta_売上金額_lag_2</th>\n",
       "      <th>delta_売上金額_lag_6</th>\n",
       "      <th>delta_売上金額_lag_12</th>\n",
       "      <th>days</th>\n",
       "      <th>item_shop_last_sale</th>\n",
       "      <th>item_shop_first_sale</th>\n",
       "      <th>item_first_sale</th>\n",
       "      <th>店舗ID_0</th>\n",
       "      <th>店舗ID_1</th>\n",
       "      <th>店舗ID_10</th>\n",
       "      <th>店舗ID_11</th>\n",
       "      <th>店舗ID_12</th>\n",
       "      <th>店舗ID_13</th>\n",
       "      <th>店舗ID_14</th>\n",
       "      <th>店舗ID_15</th>\n",
       "      <th>店舗ID_16</th>\n",
       "      <th>店舗ID_17</th>\n",
       "      <th>店舗ID_2</th>\n",
       "      <th>店舗ID_3</th>\n",
       "      <th>店舗ID_4</th>\n",
       "      <th>店舗ID_5</th>\n",
       "      <th>店舗ID_6</th>\n",
       "      <th>店舗ID_7</th>\n",
       "      <th>店舗ID_8</th>\n",
       "      <th>店舗ID_9</th>\n",
       "      <th>year_cat_2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331704</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>2.111328</td>\n",
       "      <td>1.388672</td>\n",
       "      <td>1.666992</td>\n",
       "      <td>1.888672</td>\n",
       "      <td>2.833984</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142090</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331705</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>1.388672</td>\n",
       "      <td>0.611328</td>\n",
       "      <td>0.555664</td>\n",
       "      <td>0.777832</td>\n",
       "      <td>1.555664</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054840</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331706</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>0.888672</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333252</td>\n",
       "      <td>0.666504</td>\n",
       "      <td>0.333252</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093506</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331707</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.611328</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.666504</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116089</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331708</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.769043</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>1.333008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>0.944336</td>\n",
       "      <td>1.222656</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.999023</td>\n",
       "      <td>1.103516</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.69043</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>0.740723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019363</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.696777</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655023</th>\n",
       "      <td>23</td>\n",
       "      <td>6585</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.772949</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>1.370117</td>\n",
       "      <td>1.331055</td>\n",
       "      <td>1.257812</td>\n",
       "      <td>1.238281</td>\n",
       "      <td>1.966797</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.121765</td>\n",
       "      <td>-0.202026</td>\n",
       "      <td>0.755371</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655024</th>\n",
       "      <td>23</td>\n",
       "      <td>6585</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.433105</td>\n",
       "      <td>0.350342</td>\n",
       "      <td>0.335693</td>\n",
       "      <td>0.372559</td>\n",
       "      <td>0.711426</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.462402</td>\n",
       "      <td>-0.441895</td>\n",
       "      <td>0.650391</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655025</th>\n",
       "      <td>23</td>\n",
       "      <td>6585</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.317871</td>\n",
       "      <td>0.520996</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>0.423340</td>\n",
       "      <td>0.699219</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.066284</td>\n",
       "      <td>-0.271240</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655026</th>\n",
       "      <td>23</td>\n",
       "      <td>6585</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.339844</td>\n",
       "      <td>0.411377</td>\n",
       "      <td>0.405518</td>\n",
       "      <td>0.346680</td>\n",
       "      <td>0.569824</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>-0.263184</td>\n",
       "      <td>0.647461</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655027</th>\n",
       "      <td>23</td>\n",
       "      <td>6585</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.619629</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.661621</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>1.066406</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.277832</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.553711</td>\n",
       "      <td>0.636719</td>\n",
       "      <td>0.645996</td>\n",
       "      <td>0.616699</td>\n",
       "      <td>0.947754</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.352539</td>\n",
       "      <td>0.257568</td>\n",
       "      <td>0.303711</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.275635</td>\n",
       "      <td>0.36377</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.216064</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.366943</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.083984</td>\n",
       "      <td>-0.143921</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>665946 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         月ブロック  商品ID month_cat  year  month 月ブロック_cat  holiday_cnt  month_cos  \\\n",
       "331704      12     0         0  2019      1         0            2   0.866025   \n",
       "331705      12     1         0  2019      1         0            2   0.866025   \n",
       "331706      12     2         0  2019      1         0            2   0.866025   \n",
       "331707      12     3         0  2019      1         0            2   0.866025   \n",
       "331708      12     4         0  2019      1         0            2   0.866025   \n",
       "...        ...   ...       ...   ...    ...       ...          ...        ...   \n",
       "1655023     23  6585        11  2019     12        11            0   1.000000   \n",
       "1655024     23  6585        11  2019     12        11            0   1.000000   \n",
       "1655025     23  6585        11  2019     12        11            0   1.000000   \n",
       "1655026     23  6585        11  2019     12        11            0   1.000000   \n",
       "1655027     23  6585        11  2019     12        11            0   1.000000   \n",
       "\n",
       "            month_sin 商品カテゴリ名 商品カテゴリ_type1 商品カテゴリ_type2 商品カテゴリ_type3  \\\n",
       "331704   5.000000e-01      16            5            1            0   \n",
       "331705   5.000000e-01      16            5            1            0   \n",
       "331706   5.000000e-01      16            5            1            0   \n",
       "331707   5.000000e-01      16            5            1            0   \n",
       "331708   5.000000e-01      16            5            1            0   \n",
       "...               ...     ...          ...          ...          ...   \n",
       "1655023 -2.449294e-16      21            6            5            2   \n",
       "1655024 -2.449294e-16      21            6            5            2   \n",
       "1655025 -2.449294e-16      21            6            5            2   \n",
       "1655026 -2.449294e-16      21            6            5            2   \n",
       "1655027 -2.449294e-16      21            6            5            2   \n",
       "\n",
       "         売上個数_lag_1  売上個数_lag_2  売上個数_lag_3  売上個数_lag_6  売上個数_lag_12  \\\n",
       "331704     0.000000         2.0         0.0         3.0          6.0   \n",
       "331705     0.000000         0.0         1.0         1.0          2.0   \n",
       "331706     2.000000         0.0         0.0         1.0          0.0   \n",
       "331707     0.000000         0.0         0.0         0.0          1.0   \n",
       "331708     1.000000         1.0         2.0         3.0          8.0   \n",
       "...             ...         ...         ...         ...          ...   \n",
       "1655023    0.772949         2.0         3.0         1.0          3.0   \n",
       "1655024    0.000000         0.0         0.0         0.0          0.0   \n",
       "1655025    0.000000         0.0         0.0         0.0          0.0   \n",
       "1655026    0.000000         0.0         0.0         0.0          0.0   \n",
       "1655027    0.000000         0.0         0.0         0.0          1.0   \n",
       "\n",
       "         date_avg_売上個数_lag_1  date_avg_売上個数_lag_2  date_avg_売上個数_lag_3  \\\n",
       "331704              1.066406             0.770508             0.769043   \n",
       "331705              1.066406             0.770508             0.769043   \n",
       "331706              1.066406             0.770508             0.769043   \n",
       "331707              1.066406             0.770508             0.769043   \n",
       "331708              1.066406             0.770508             0.769043   \n",
       "...                      ...                  ...                  ...   \n",
       "1655023             0.619629             0.791016             0.661621   \n",
       "1655024             0.619629             0.791016             0.661621   \n",
       "1655025             0.619629             0.791016             0.661621   \n",
       "1655026             0.619629             0.791016             0.661621   \n",
       "1655027             0.619629             0.791016             0.661621   \n",
       "\n",
       "         date_avg_売上個数_lag_6  date_avg_売上個数_lag_12  date_item_avg_売上個数_lag_1  \\\n",
       "331704              0.730469              0.834473                  2.111328   \n",
       "331705              0.730469              0.834473                  1.388672   \n",
       "331706              0.730469              0.834473                  0.888672   \n",
       "331707              0.730469              0.834473                  1.000000   \n",
       "331708              0.730469              0.834473                  1.333008   \n",
       "...                      ...                   ...                       ...   \n",
       "1655023             0.690918              1.066406                  0.128784   \n",
       "1655024             0.690918              1.066406                  0.128784   \n",
       "1655025             0.690918              1.066406                  0.128784   \n",
       "1655026             0.690918              1.066406                  0.128784   \n",
       "1655027             0.690918              1.066406                  0.128784   \n",
       "\n",
       "         date_item_avg_売上個数_lag_2  date_item_avg_売上個数_lag_3  \\\n",
       "331704                   1.388672                  1.666992   \n",
       "331705                   0.611328                  0.555664   \n",
       "331706                   0.500000                  0.333252   \n",
       "331707                   0.611328                  0.277832   \n",
       "331708                   1.000000                  1.222656   \n",
       "...                           ...                       ...   \n",
       "1655023                  0.277832                  0.277832   \n",
       "1655024                  0.277832                  0.277832   \n",
       "1655025                  0.277832                  0.277832   \n",
       "1655026                  0.277832                  0.277832   \n",
       "1655027                  0.277832                  0.277832   \n",
       "\n",
       "         date_item_avg_売上個数_lag_6  date_item_avg_売上個数_lag_12  \\\n",
       "331704                   1.888672                   2.833984   \n",
       "331705                   0.777832                   1.555664   \n",
       "331706                   0.666504                   0.333252   \n",
       "331707                   0.388916                   0.666504   \n",
       "331708                   0.944336                   1.222656   \n",
       "...                           ...                        ...   \n",
       "1655023                  0.388916                   0.444336   \n",
       "1655024                  0.388916                   0.444336   \n",
       "1655025                  0.388916                   0.444336   \n",
       "1655026                  0.388916                   0.444336   \n",
       "1655027                  0.388916                   0.444336   \n",
       "\n",
       "         date_shop_avg_売上個数_lag_1  date_shop_avg_売上個数_lag_2  \\\n",
       "331704                   1.422852                  1.133789   \n",
       "331705                   1.422852                  1.133789   \n",
       "331706                   1.422852                  1.133789   \n",
       "331707                   1.422852                  1.133789   \n",
       "331708                   1.422852                  1.133789   \n",
       "...                           ...                       ...   \n",
       "1655023                  1.370117                  1.331055   \n",
       "1655024                  0.433105                  0.350342   \n",
       "1655025                  0.317871                  0.520996   \n",
       "1655026                  0.339844                  0.411377   \n",
       "1655027                  0.553711                  0.636719   \n",
       "\n",
       "         date_shop_avg_売上個数_lag_3  date_shop_avg_売上個数_lag_6  \\\n",
       "331704                   0.999023                  1.103516   \n",
       "331705                   0.999023                  1.103516   \n",
       "331706                   0.999023                  1.103516   \n",
       "331707                   0.999023                  1.103516   \n",
       "331708                   0.999023                  1.103516   \n",
       "...                           ...                       ...   \n",
       "1655023                  1.257812                  1.238281   \n",
       "1655024                  0.335693                  0.372559   \n",
       "1655025                  0.505859                  0.423340   \n",
       "1655026                  0.405518                  0.346680   \n",
       "1655027                  0.645996                  0.616699   \n",
       "\n",
       "         date_shop_avg_売上個数_lag_12  date_catname_avg_売上個数_lag_1  \\\n",
       "331704                    2.062500                     0.738281   \n",
       "331705                    2.062500                     0.738281   \n",
       "331706                    2.062500                     0.738281   \n",
       "331707                    2.062500                     0.738281   \n",
       "331708                    2.062500                     0.738281   \n",
       "...                            ...                          ...   \n",
       "1655023                   1.966797                     0.128784   \n",
       "1655024                   0.711426                     0.128784   \n",
       "1655025                   0.699219                     0.128784   \n",
       "1655026                   0.569824                     0.128784   \n",
       "1655027                   0.947754                     0.128784   \n",
       "\n",
       "         date_catname_avg_売上個数_lag_2  date_catname_avg_売上個数_lag_3  \\\n",
       "331704                      0.625488                     0.618652   \n",
       "331705                      0.625488                     0.618652   \n",
       "331706                      0.625488                     0.618652   \n",
       "331707                      0.625488                     0.618652   \n",
       "331708                      0.625488                     0.618652   \n",
       "...                              ...                          ...   \n",
       "1655023                     0.140991                     0.192139   \n",
       "1655024                     0.140991                     0.192139   \n",
       "1655025                     0.140991                     0.192139   \n",
       "1655026                     0.140991                     0.192139   \n",
       "1655027                     0.140991                     0.192139   \n",
       "\n",
       "         date_catname_avg_売上個数_lag_6  date_catname_avg_売上個数_lag_12  \\\n",
       "331704                      0.574219                      0.740723   \n",
       "331705                      0.574219                      0.740723   \n",
       "331706                      0.574219                      0.740723   \n",
       "331707                      0.574219                      0.740723   \n",
       "331708                      0.574219                      0.740723   \n",
       "...                              ...                           ...   \n",
       "1655023                     0.206055                      0.352539   \n",
       "1655024                     0.206055                      0.352539   \n",
       "1655025                     0.206055                      0.352539   \n",
       "1655026                     0.206055                      0.352539   \n",
       "1655027                     0.206055                      0.352539   \n",
       "\n",
       "         date_cattype1_avg_売上個数_lag_1  date_cattype1_avg_売上個数_lag_2  \\\n",
       "331704                       0.707520                      0.572266   \n",
       "331705                       0.707520                      0.572266   \n",
       "331706                       0.707520                      0.572266   \n",
       "331707                       0.707520                      0.572266   \n",
       "331708                       0.707520                      0.572266   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.257568                      0.303711   \n",
       "1655024                      0.257568                      0.303711   \n",
       "1655025                      0.257568                      0.303711   \n",
       "1655026                      0.257568                      0.303711   \n",
       "1655027                      0.257568                      0.303711   \n",
       "\n",
       "         date_cattype1_avg_売上個数_lag_3  date_cattype1_avg_売上個数_lag_6  \\\n",
       "331704                       0.555176                      0.572754   \n",
       "331705                       0.555176                      0.572754   \n",
       "331706                       0.555176                      0.572754   \n",
       "331707                       0.555176                      0.572754   \n",
       "331708                       0.555176                      0.572754   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.249023                      0.275635   \n",
       "1655024                      0.249023                      0.275635   \n",
       "1655025                      0.249023                      0.275635   \n",
       "1655026                      0.249023                      0.275635   \n",
       "1655027                      0.249023                      0.275635   \n",
       "\n",
       "         date_cattype1_avg_売上個数_lag_12  date_cattype2_avg_売上個数_lag_1  \\\n",
       "331704                         0.69043                      0.738281   \n",
       "331705                         0.69043                      0.738281   \n",
       "331706                         0.69043                      0.738281   \n",
       "331707                         0.69043                      0.738281   \n",
       "331708                         0.69043                      0.738281   \n",
       "...                                ...                           ...   \n",
       "1655023                        0.36377                      0.193237   \n",
       "1655024                        0.36377                      0.193237   \n",
       "1655025                        0.36377                      0.193237   \n",
       "1655026                        0.36377                      0.193237   \n",
       "1655027                        0.36377                      0.193237   \n",
       "\n",
       "         date_cattype2_avg_売上個数_lag_2  date_cattype2_avg_売上個数_lag_3  \\\n",
       "331704                       0.625488                      0.618652   \n",
       "331705                       0.625488                      0.618652   \n",
       "331706                       0.625488                      0.618652   \n",
       "331707                       0.625488                      0.618652   \n",
       "331708                       0.625488                      0.618652   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.216064                      0.197266   \n",
       "1655024                      0.216064                      0.197266   \n",
       "1655025                      0.216064                      0.197266   \n",
       "1655026                      0.216064                      0.197266   \n",
       "1655027                      0.216064                      0.197266   \n",
       "\n",
       "         date_cattype2_avg_売上個数_lag_6  date_cattype2_avg_売上個数_lag_12  \\\n",
       "331704                       0.574219                       0.740723   \n",
       "331705                       0.574219                       0.740723   \n",
       "331706                       0.574219                       0.740723   \n",
       "331707                       0.574219                       0.740723   \n",
       "331708                       0.574219                       0.740723   \n",
       "...                               ...                            ...   \n",
       "1655023                      0.239624                       0.312256   \n",
       "1655024                      0.239624                       0.312256   \n",
       "1655025                      0.239624                       0.312256   \n",
       "1655026                      0.239624                       0.312256   \n",
       "1655027                      0.239624                       0.312256   \n",
       "\n",
       "         date_cattype3_avg_売上個数_lag_1  date_cattype3_avg_売上個数_lag_2  \\\n",
       "331704                            NaN                           NaN   \n",
       "331705                            NaN                           NaN   \n",
       "331706                            NaN                           NaN   \n",
       "331707                            NaN                           NaN   \n",
       "331708                            NaN                           NaN   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.300537                      0.938965   \n",
       "1655024                      0.300537                      0.938965   \n",
       "1655025                      0.300537                      0.938965   \n",
       "1655026                      0.300537                      0.938965   \n",
       "1655027                      0.300537                      0.938965   \n",
       "\n",
       "         date_cattype3_avg_売上個数_lag_3  date_cattype3_avg_売上個数_lag_6  \\\n",
       "331704                            NaN                           NaN   \n",
       "331705                            NaN                           NaN   \n",
       "331706                            NaN                           NaN   \n",
       "331707                            NaN                           NaN   \n",
       "331708                            NaN                           NaN   \n",
       "...                               ...                           ...   \n",
       "1655023                      0.366943                      0.464844   \n",
       "1655024                      0.366943                      0.464844   \n",
       "1655025                      0.366943                      0.464844   \n",
       "1655026                      0.366943                      0.464844   \n",
       "1655027                      0.366943                      0.464844   \n",
       "\n",
       "         date_cattype3_avg_売上個数_lag_12  delta_価格_lag  delta_売上金額_lag_1  \\\n",
       "331704                             NaN      0.142090          0.652832   \n",
       "331705                             NaN      0.054840          0.652832   \n",
       "331706                             NaN      0.093506          0.652832   \n",
       "331707                             NaN      0.116089          0.652832   \n",
       "331708                             NaN      0.019363          0.652832   \n",
       "...                                ...           ...               ...   \n",
       "1655023                       1.077148      0.000000               NaN   \n",
       "1655024                       1.077148      0.000000               NaN   \n",
       "1655025                       1.077148      0.000000               NaN   \n",
       "1655026                       1.077148      0.000000               NaN   \n",
       "1655027                       1.077148      0.000000               NaN   \n",
       "\n",
       "         delta_売上金額_lag_2  delta_売上金額_lag_6  delta_売上金額_lag_12  days  \\\n",
       "331704           0.230103          0.013771           0.696777    31   \n",
       "331705           0.230103          0.013771           0.696777    31   \n",
       "331706           0.230103          0.013771           0.696777    31   \n",
       "331707           0.230103          0.013771           0.696777    31   \n",
       "331708           0.230103          0.013771           0.696777    31   \n",
       "...                   ...               ...                ...   ...   \n",
       "1655023         -0.121765         -0.202026           0.755371    31   \n",
       "1655024         -0.462402         -0.441895           0.650391    31   \n",
       "1655025         -0.066284         -0.271240           0.726562    31   \n",
       "1655026         -0.096680         -0.263184           0.647461    31   \n",
       "1655027         -0.083984         -0.143921           0.937500    31   \n",
       "\n",
       "         item_shop_last_sale  item_shop_first_sale  item_first_sale  店舗ID_0  \\\n",
       "331704                     1                    12               12       1   \n",
       "331705                     1                    12               12       1   \n",
       "331706                     1                    12               12       1   \n",
       "331707                     1                    12               12       1   \n",
       "331708                     1                    12               12       1   \n",
       "...                      ...                   ...              ...     ...   \n",
       "1655023                    1                    23               23       0   \n",
       "1655024                    1                    23               23       0   \n",
       "1655025                    1                    23               23       0   \n",
       "1655026                    1                    23               23       0   \n",
       "1655027                    1                    23               23       0   \n",
       "\n",
       "         店舗ID_1  店舗ID_10  店舗ID_11  店舗ID_12  店舗ID_13  店舗ID_14  店舗ID_15  \\\n",
       "331704        0        0        0        0        0        0        0   \n",
       "331705        0        0        0        0        0        0        0   \n",
       "331706        0        0        0        0        0        0        0   \n",
       "331707        0        0        0        0        0        0        0   \n",
       "331708        0        0        0        0        0        0        0   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "1655023       0        0        0        0        1        0        0   \n",
       "1655024       0        0        0        0        0        1        0   \n",
       "1655025       0        0        0        0        0        0        1   \n",
       "1655026       0        0        0        0        0        0        0   \n",
       "1655027       0        0        0        0        0        0        0   \n",
       "\n",
       "         店舗ID_16  店舗ID_17  店舗ID_2  店舗ID_3  店舗ID_4  店舗ID_5  店舗ID_6  店舗ID_7  \\\n",
       "331704         0        0       0       0       0       0       0       0   \n",
       "331705         0        0       0       0       0       0       0       0   \n",
       "331706         0        0       0       0       0       0       0       0   \n",
       "331707         0        0       0       0       0       0       0       0   \n",
       "331708         0        0       0       0       0       0       0       0   \n",
       "...          ...      ...     ...     ...     ...     ...     ...     ...   \n",
       "1655023        0        0       0       0       0       0       0       0   \n",
       "1655024        0        0       0       0       0       0       0       0   \n",
       "1655025        0        0       0       0       0       0       0       0   \n",
       "1655026        1        0       0       0       0       0       0       0   \n",
       "1655027        0        1       0       0       0       0       0       0   \n",
       "\n",
       "         店舗ID_8  店舗ID_9  year_cat_2019  \n",
       "331704        0       0              1  \n",
       "331705        0       0              1  \n",
       "331706        0       0              1  \n",
       "331707        0       0              1  \n",
       "331708        0       0              1  \n",
       "...         ...     ...            ...  \n",
       "1655023       0       0              1  \n",
       "1655024       0       0              1  \n",
       "1655025       0       0              1  \n",
       "1655026       0       0              1  \n",
       "1655027       0       0              1  \n",
       "\n",
       "[665946 rows x 81 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88886ac9-2cbf-493c-99af-20a3e101ae8a",
   "metadata": {},
   "source": [
    "# データセットを入力値、目標値に分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8a322b0c-4237-4047-9237-338c5d6f0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データとしては、「12ヶ月前の売上個数」カラムが存在するレコードのみを抽出する\n",
    "X_train = X_data_dummy[(X_data_dummy.月ブロック >= 12) & (X_data_dummy.月ブロック < 21)]\n",
    "y_train = y_data[(X_data_dummy.月ブロック>= 12) & (X_data_dummy.月ブロック< 21)]\n",
    "\n",
    "X_valid = X_data_dummy[X_data_dummy.月ブロック == 21] #10月\n",
    "y_valid = y_data[X_data_dummy.月ブロック == 21]\n",
    "\n",
    "X_test = X_data_dummy[X_data_dummy.月ブロック == 23] #12月"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40961b-789f-405a-9bfc-065b00b44f70",
   "metadata": {},
   "source": [
    "# 目的変数の前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "84ac42bb-abc9-4bda-8d4e-a3ee53b4dc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train\n",
      "歪度: nan\n",
      "尖度: nan\n",
      "=============\n",
      "np.exp(train_y_log)\n",
      "歪度: nan\n",
      "尖度: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[30, 10])\n",
    "\n",
    "sns.distplot(y_data[y_data.notnull()], ax=ax1)\n",
    "#歪度と尖度を計算\n",
    "print('y_train')\n",
    "print(\"歪度: %f\" % y_data.skew())\n",
    "print(\"尖度: %f\" % y_data.kurt())\n",
    "\n",
    "print('=============')\n",
    "\n",
    "sns.distplot(np.log1p(y_data[y_data.notnull()]), ax=ax2)\n",
    "print('np.exp(train_y_log)')\n",
    "#歪度と尖度を計算\n",
    "print(\"歪度: %f\" % np.log1p(y_data[y_data.notnull()]).skew())\n",
    "print(\"尖度: %f\" % np.log1p(y_data[y_data.notnull()]).kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd19685-79aa-4ab7-9391-9acd247f273d",
   "metadata": {},
   "source": [
    "# モデリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9862e-ead1-46c6-9225-71583f68cb05",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87bd6e7-0073-4706-a74c-3017339e487d",
   "metadata": {},
   "source": [
    "### パラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f1d1ed50-72ed-4305-9b3f-1e49cc61e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as op_lgb\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "dtrain_tuning = op_lgb.Dataset(X_train.values, label=y_train.values)\n",
    "dval_tuning = op_lgb.Dataset(X_valid.values, label=y_valid.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94d1af14-2e31-4b07-a50d-b04031a229e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-23 03:32:44,276]\u001b[0m A new study created in memory with name: no-name-5f850d64-565c-472f-9838-f9ad611ba9ad\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                   | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 2.814233:  14%|8     | 1/7 [00:06<00:41,  6.84s/it]\u001b[32m[I 2021-09-23 03:32:51,121]\u001b[0m Trial 0 finished with value: 2.8142326944520097 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 2.8142326944520097.\u001b[0m\n",
      "feature_fraction, val_score: 2.814233:  14%|8     | 1/7 [00:06<00:41,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 2.800475:  29%|#7    | 2/7 [00:14<00:36,  7.30s/it]\u001b[32m[I 2021-09-23 03:32:58,753]\u001b[0m Trial 1 finished with value: 2.8004754230060267 and parameters: {'feature_fraction': 1.0}. Best is trial 1 with value: 2.8004754230060267.\u001b[0m\n",
      "feature_fraction, val_score: 2.800475:  29%|#7    | 2/7 [00:14<00:36,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 2.795194:  43%|##5   | 3/7 [00:23<00:32,  8.15s/it]\u001b[32m[I 2021-09-23 03:33:07,900]\u001b[0m Trial 2 finished with value: 2.795194168502188 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 2 with value: 2.795194168502188.\u001b[0m\n",
      "feature_fraction, val_score: 2.795194:  43%|##5   | 3/7 [00:23<00:32,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 2.795194:  57%|###4  | 4/7 [00:31<00:24,  8.05s/it]\u001b[32m[I 2021-09-23 03:33:15,802]\u001b[0m Trial 3 finished with value: 2.8084020318186 and parameters: {'feature_fraction': 0.8}. Best is trial 2 with value: 2.795194168502188.\u001b[0m\n",
      "feature_fraction, val_score: 2.795194:  57%|###4  | 4/7 [00:31<00:24,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 2.795194:  71%|####2 | 5/7 [00:39<00:15,  7.97s/it]\u001b[32m[I 2021-09-23 03:33:23,637]\u001b[0m Trial 4 finished with value: 2.805619296014664 and parameters: {'feature_fraction': 0.6}. Best is trial 2 with value: 2.795194168502188.\u001b[0m\n",
      "feature_fraction, val_score: 2.795194:  71%|####2 | 5/7 [00:39<00:15,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 2.795194:  86%|#####1| 6/7 [00:47<00:07,  7.96s/it]\u001b[32m[I 2021-09-23 03:33:31,575]\u001b[0m Trial 5 finished with value: 2.7998704729709134 and parameters: {'feature_fraction': 0.5}. Best is trial 2 with value: 2.795194168502188.\u001b[0m\n",
      "feature_fraction, val_score: 2.795194:  86%|#####1| 6/7 [00:47<00:07,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 2.795194: 100%|######| 7/7 [00:54<00:00,  7.69s/it]\u001b[32m[I 2021-09-23 03:33:38,715]\u001b[0m Trial 6 finished with value: 2.80164672994002 and parameters: {'feature_fraction': 0.4}. Best is trial 2 with value: 2.795194168502188.\u001b[0m\n",
      "feature_fraction, val_score: 2.795194: 100%|######| 7/7 [00:54<00:00,  7.78s/it]\n",
      "num_leaves, val_score: 2.795194:   0%|                   | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:   5%|5          | 1/20 [00:09<02:56,  9.29s/it]\u001b[32m[I 2021-09-23 03:33:48,010]\u001b[0m Trial 7 finished with value: 2.844411810500403 and parameters: {'num_leaves': 111}. Best is trial 7 with value: 2.844411810500403.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:   5%|5          | 1/20 [00:09<02:56,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:  10%|#1         | 2/20 [00:17<02:39,  8.86s/it]\u001b[32m[I 2021-09-23 03:33:56,577]\u001b[0m Trial 8 finished with value: 2.8378256281607 and parameters: {'num_leaves': 97}. Best is trial 8 with value: 2.8378256281607.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:  10%|#1         | 2/20 [00:17<02:39,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:  15%|#6         | 3/20 [00:28<02:46,  9.82s/it]\u001b[32m[I 2021-09-23 03:34:07,531]\u001b[0m Trial 9 finished with value: 2.8484626074756303 and parameters: {'num_leaves': 154}. Best is trial 8 with value: 2.8378256281607.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:  15%|#6         | 3/20 [00:28<02:46,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:  20%|##2        | 4/20 [00:38<02:35,  9.74s/it]\u001b[32m[I 2021-09-23 03:34:17,142]\u001b[0m Trial 10 finished with value: 2.874303180624664 and parameters: {'num_leaves': 222}. Best is trial 8 with value: 2.8378256281607.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:  20%|##2        | 4/20 [00:38<02:35,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:  25%|##7        | 5/20 [00:48<02:29,  9.98s/it]\u001b[32m[I 2021-09-23 03:34:27,543]\u001b[0m Trial 11 finished with value: 2.8440410793919613 and parameters: {'num_leaves': 147}. Best is trial 8 with value: 2.8378256281607.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:  25%|##7        | 5/20 [00:48<02:29,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:  30%|###3       | 6/20 [00:58<02:16,  9.73s/it]\u001b[32m[I 2021-09-23 03:34:36,799]\u001b[0m Trial 12 finished with value: 2.795194168502188 and parameters: {'num_leaves': 31}. Best is trial 12 with value: 2.795194168502188.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:  30%|###3       | 6/20 [00:58<02:16,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:  35%|###8       | 7/20 [01:08<02:09,  9.99s/it]\u001b[32m[I 2021-09-23 03:34:47,336]\u001b[0m Trial 13 finished with value: 2.8378763061408363 and parameters: {'num_leaves': 136}. Best is trial 12 with value: 2.795194168502188.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:  35%|###8       | 7/20 [01:08<02:09,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:  40%|####4      | 8/20 [01:20<02:06, 10.52s/it]\u001b[32m[I 2021-09-23 03:34:58,997]\u001b[0m Trial 14 finished with value: 2.8508523272172615 and parameters: {'num_leaves': 216}. Best is trial 12 with value: 2.795194168502188.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:  40%|####4      | 8/20 [01:20<02:06, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:  45%|####9      | 9/20 [01:31<01:57, 10.68s/it]\u001b[32m[I 2021-09-23 03:35:10,013]\u001b[0m Trial 15 finished with value: 2.8388087304878926 and parameters: {'num_leaves': 126}. Best is trial 12 with value: 2.795194168502188.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:  45%|####9      | 9/20 [01:31<01:57, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.795194:  50%|#####     | 10/20 [01:44<01:55, 11.51s/it]\u001b[32m[I 2021-09-23 03:35:23,399]\u001b[0m Trial 16 finished with value: 2.7966103280672248 and parameters: {'num_leaves': 4}. Best is trial 12 with value: 2.795194168502188.\u001b[0m\n",
      "num_leaves, val_score: 2.795194:  50%|#####     | 10/20 [01:44<01:55, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.790485:  55%|#####5    | 11/20 [01:53<01:36, 10.69s/it]\u001b[32m[I 2021-09-23 03:35:32,219]\u001b[0m Trial 17 finished with value: 2.790484827535004 and parameters: {'num_leaves': 6}. Best is trial 17 with value: 2.790484827535004.\u001b[0m\n",
      "num_leaves, val_score: 2.790485:  55%|#####5    | 11/20 [01:53<01:36, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.788206:  60%|######    | 12/20 [02:02<01:22, 10.29s/it]\u001b[32m[I 2021-09-23 03:35:41,598]\u001b[0m Trial 18 finished with value: 2.788205779634444 and parameters: {'num_leaves': 10}. Best is trial 18 with value: 2.788205779634444.\u001b[0m\n",
      "num_leaves, val_score: 2.788206:  60%|######    | 12/20 [02:02<01:22, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.788206:  65%|######5   | 13/20 [02:10<01:07,  9.62s/it]\u001b[32m[I 2021-09-23 03:35:49,686]\u001b[0m Trial 19 finished with value: 2.81873597845998 and parameters: {'num_leaves': 52}. Best is trial 18 with value: 2.788205779634444.\u001b[0m\n",
      "num_leaves, val_score: 2.788206:  65%|######5   | 13/20 [02:10<01:07,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.788206:  70%|#######   | 14/20 [02:19<00:56,  9.40s/it]\u001b[32m[I 2021-09-23 03:35:58,565]\u001b[0m Trial 20 finished with value: 2.790484827535004 and parameters: {'num_leaves': 6}. Best is trial 18 with value: 2.788205779634444.\u001b[0m\n",
      "num_leaves, val_score: 2.788206:  70%|#######   | 14/20 [02:19<00:56,  9.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.788206:  75%|#######5  | 15/20 [02:27<00:44,  8.93s/it]\u001b[32m[I 2021-09-23 03:36:06,421]\u001b[0m Trial 21 finished with value: 2.8293337632440405 and parameters: {'num_leaves': 64}. Best is trial 18 with value: 2.788205779634444.\u001b[0m\n",
      "num_leaves, val_score: 2.788206:  75%|#######5  | 15/20 [02:27<00:44,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.780211:  80%|########  | 16/20 [02:38<00:37,  9.48s/it]\u001b[32m[I 2021-09-23 03:36:17,154]\u001b[0m Trial 22 finished with value: 2.7802114790115544 and parameters: {'num_leaves': 7}. Best is trial 22 with value: 2.7802114790115544.\u001b[0m\n",
      "num_leaves, val_score: 2.780211:  80%|########  | 16/20 [02:38<00:37,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.780211:  85%|########5 | 17/20 [02:47<00:28,  9.39s/it]\u001b[32m[I 2021-09-23 03:36:26,337]\u001b[0m Trial 23 finished with value: 2.8281829957339633 and parameters: {'num_leaves': 82}. Best is trial 22 with value: 2.7802114790115544.\u001b[0m\n",
      "num_leaves, val_score: 2.780211:  85%|########5 | 17/20 [02:47<00:28,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.780211:  90%|######### | 18/20 [02:57<00:19,  9.68s/it]\u001b[32m[I 2021-09-23 03:36:36,693]\u001b[0m Trial 24 finished with value: 2.795451793735384 and parameters: {'num_leaves': 35}. Best is trial 22 with value: 2.7802114790115544.\u001b[0m\n",
      "num_leaves, val_score: 2.780211:  90%|######### | 18/20 [02:57<00:19,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.780211:  95%|#########5| 19/20 [03:05<00:09,  9.06s/it]\u001b[32m[I 2021-09-23 03:36:44,310]\u001b[0m Trial 25 finished with value: 2.8078536837969463 and parameters: {'num_leaves': 26}. Best is trial 22 with value: 2.7802114790115544.\u001b[0m\n",
      "num_leaves, val_score: 2.780211:  95%|#########5| 19/20 [03:05<00:09,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 2.780211: 100%|##########| 20/20 [03:17<00:00,  9.77s/it]\u001b[32m[I 2021-09-23 03:36:55,724]\u001b[0m Trial 26 finished with value: 2.845752349150819 and parameters: {'num_leaves': 173}. Best is trial 22 with value: 2.7802114790115544.\u001b[0m\n",
      "num_leaves, val_score: 2.780211: 100%|##########| 20/20 [03:17<00:00,  9.85s/it]\n",
      "bagging, val_score: 2.780211:   0%|                      | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211:  10%|#4            | 1/10 [00:06<00:59,  6.59s/it]\u001b[32m[I 2021-09-23 03:37:02,316]\u001b[0m Trial 27 finished with value: 2.795910129925342 and parameters: {'bagging_fraction': 0.42606718884124506, 'bagging_freq': 6}. Best is trial 27 with value: 2.795910129925342.\u001b[0m\n",
      "bagging, val_score: 2.780211:  10%|#4            | 1/10 [00:06<00:59,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211:  20%|##8           | 2/10 [00:13<00:56,  7.07s/it]\u001b[32m[I 2021-09-23 03:37:09,717]\u001b[0m Trial 28 finished with value: 2.7965893597803695 and parameters: {'bagging_fraction': 0.48188328189941043, 'bagging_freq': 4}. Best is trial 27 with value: 2.795910129925342.\u001b[0m\n",
      "bagging, val_score: 2.780211:  20%|##8           | 2/10 [00:13<00:56,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211:  30%|####2         | 3/10 [00:23<00:57,  8.22s/it]\u001b[32m[I 2021-09-23 03:37:19,307]\u001b[0m Trial 29 finished with value: 2.7912431906138084 and parameters: {'bagging_fraction': 0.7833293212683121, 'bagging_freq': 6}. Best is trial 29 with value: 2.7912431906138084.\u001b[0m\n",
      "bagging, val_score: 2.780211:  30%|####2         | 3/10 [00:23<00:57,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211:  40%|#####6        | 4/10 [00:33<00:54,  9.01s/it]\u001b[32m[I 2021-09-23 03:37:29,540]\u001b[0m Trial 30 finished with value: 2.787325367235679 and parameters: {'bagging_fraction': 0.7032469992621581, 'bagging_freq': 5}. Best is trial 30 with value: 2.787325367235679.\u001b[0m\n",
      "bagging, val_score: 2.780211:  40%|#####6        | 4/10 [00:33<00:54,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211:  50%|#######       | 5/10 [00:49<00:57, 11.58s/it]\u001b[32m[I 2021-09-23 03:37:45,676]\u001b[0m Trial 31 finished with value: 2.781429777103769 and parameters: {'bagging_fraction': 0.9987616846151319, 'bagging_freq': 4}. Best is trial 31 with value: 2.781429777103769.\u001b[0m\n",
      "bagging, val_score: 2.780211:  50%|#######       | 5/10 [00:49<00:57, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211:  60%|########4     | 6/10 [01:01<00:46, 11.58s/it]\u001b[32m[I 2021-09-23 03:37:57,240]\u001b[0m Trial 32 finished with value: 2.791719226006407 and parameters: {'bagging_fraction': 0.8948685043220146, 'bagging_freq': 6}. Best is trial 31 with value: 2.781429777103769.\u001b[0m\n",
      "bagging, val_score: 2.780211:  60%|########4     | 6/10 [01:01<00:46, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211:  70%|#########7    | 7/10 [01:13<00:35, 11.70s/it]\u001b[32m[I 2021-09-23 03:38:09,193]\u001b[0m Trial 33 finished with value: 2.7893984327052457 and parameters: {'bagging_fraction': 0.8419250122396043, 'bagging_freq': 4}. Best is trial 31 with value: 2.781429777103769.\u001b[0m\n",
      "bagging, val_score: 2.780211:  70%|#########7    | 7/10 [01:13<00:35, 11.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211:  80%|###########2  | 8/10 [01:27<00:25, 12.57s/it]\u001b[32m[I 2021-09-23 03:38:23,622]\u001b[0m Trial 34 finished with value: 2.781279625423858 and parameters: {'bagging_fraction': 0.8198340452339228, 'bagging_freq': 2}. Best is trial 34 with value: 2.781279625423858.\u001b[0m\n",
      "bagging, val_score: 2.780211:  80%|###########2  | 8/10 [01:27<00:25, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211:  90%|############6 | 9/10 [01:33<00:10, 10.41s/it]\u001b[32m[I 2021-09-23 03:38:29,285]\u001b[0m Trial 35 finished with value: 2.8177646315555864 and parameters: {'bagging_fraction': 0.520163222857288, 'bagging_freq': 7}. Best is trial 34 with value: 2.781279625423858.\u001b[0m\n",
      "bagging, val_score: 2.780211:  90%|############6 | 9/10 [01:33<00:10, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 2.780211: 100%|#############| 10/10 [01:44<00:00, 10.44s/it]\u001b[32m[I 2021-09-23 03:38:39,800]\u001b[0m Trial 36 finished with value: 2.7847989758934313 and parameters: {'bagging_fraction': 0.7517787361891334, 'bagging_freq': 5}. Best is trial 34 with value: 2.781279625423858.\u001b[0m\n",
      "bagging, val_score: 2.780211: 100%|#############| 10/10 [01:44<00:00, 10.41s/it]\n",
      "feature_fraction_stage2, val_score: 2.780211:   0%|       | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 2.778472:  17%|1| 1/6 [00:11<00:56, 11.25s/i\u001b[32m[I 2021-09-23 03:38:51,056]\u001b[0m Trial 37 finished with value: 2.778472295109509 and parameters: {'feature_fraction': 0.82}. Best is trial 37 with value: 2.778472295109509.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2.778472:  17%|1| 1/6 [00:11<00:56, 11.25s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 2.778472:  33%|3| 2/6 [00:21<00:42, 10.52s/i\u001b[32m[I 2021-09-23 03:39:01,062]\u001b[0m Trial 38 finished with value: 2.7857067992465065 and parameters: {'feature_fraction': 0.9159999999999999}. Best is trial 37 with value: 2.778472295109509.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2.778472:  33%|3| 2/6 [00:21<00:42, 10.52s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 2.778472:  50%|5| 3/6 [00:32<00:32, 10.68s/i\u001b[32m[I 2021-09-23 03:39:11,926]\u001b[0m Trial 39 finished with value: 2.7791692279385867 and parameters: {'feature_fraction': 0.8839999999999999}. Best is trial 37 with value: 2.778472295109509.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2.778472:  50%|5| 3/6 [00:32<00:32, 10.68s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 2.778472:  67%|6| 4/6 [00:43<00:21, 10.85s/i\u001b[32m[I 2021-09-23 03:39:23,041]\u001b[0m Trial 40 finished with value: 2.778737729194321 and parameters: {'feature_fraction': 0.948}. Best is trial 37 with value: 2.778472295109509.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2.778472:  67%|6| 4/6 [00:43<00:21, 10.85s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 2.778472:  83%|8| 5/6 [00:52<00:10, 10.13s/i\u001b[32m[I 2021-09-23 03:39:31,904]\u001b[0m Trial 41 finished with value: 2.78913632994709 and parameters: {'feature_fraction': 0.9799999999999999}. Best is trial 37 with value: 2.778472295109509.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2.778472:  83%|8| 5/6 [00:52<00:10, 10.13s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 2.776243: 100%|#| 6/6 [01:03<00:00, 10.64s/i\u001b[32m[I 2021-09-23 03:39:43,524]\u001b[0m Trial 42 finished with value: 2.7762431817828133 and parameters: {'feature_fraction': 0.852}. Best is trial 42 with value: 2.7762431817828133.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2.776243: 100%|#| 6/6 [01:03<00:00, 10.62s/i\n",
      "regularization_factors, val_score: 2.776243:   0%|       | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776243:   5%| | 1/20 [00:11<03:33, 11.23s/i\u001b[32m[I 2021-09-23 03:39:54,758]\u001b[0m Trial 43 finished with value: 2.779236803338084 and parameters: {'lambda_l1': 1.6659525529362795, 'lambda_l2': 3.007542602163906e-06}. Best is trial 43 with value: 2.779236803338084.\u001b[0m\n",
      "regularization_factors, val_score: 2.776243:   5%| | 1/20 [00:11<03:33, 11.23s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  10%|1| 2/20 [00:22<03:27, 11.54s/i\u001b[32m[I 2021-09-23 03:40:06,513]\u001b[0m Trial 44 finished with value: 2.7760606198788347 and parameters: {'lambda_l1': 1.4423118469183488e-05, 'lambda_l2': 2.0327826235842657e-07}. Best is trial 44 with value: 2.7760606198788347.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  10%|1| 2/20 [00:22<03:27, 11.54s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  15%|1| 3/20 [00:35<03:23, 11.96s/i\u001b[32m[I 2021-09-23 03:40:18,973]\u001b[0m Trial 45 finished with value: 2.7760606195202286 and parameters: {'lambda_l1': 5.241802078943606e-07, 'lambda_l2': 3.173272812833984e-07}. Best is trial 45 with value: 2.7760606195202286.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  15%|1| 3/20 [00:35<03:23, 11.96s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  20%|2| 4/20 [00:47<03:14, 12.18s/i\u001b[32m[I 2021-09-23 03:40:31,504]\u001b[0m Trial 46 finished with value: 2.779063788203541 and parameters: {'lambda_l1': 0.006691831176134015, 'lambda_l2': 0.0032256287699356794}. Best is trial 45 with value: 2.7760606195202286.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  20%|2| 4/20 [00:47<03:14, 12.18s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  25%|2| 5/20 [00:59<02:59, 12.00s/i\u001b[32m[I 2021-09-23 03:40:43,167]\u001b[0m Trial 47 finished with value: 2.777860974481219 and parameters: {'lambda_l1': 0.00010040353629845025, 'lambda_l2': 0.0011558161112652367}. Best is trial 45 with value: 2.7760606195202286.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  25%|2| 5/20 [00:59<02:59, 12.00s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  30%|3| 6/20 [01:10<02:41, 11.50s/i\u001b[32m[I 2021-09-23 03:40:53,703]\u001b[0m Trial 48 finished with value: 2.7848124099274885 and parameters: {'lambda_l1': 4.337468920658879, 'lambda_l2': 1.5067063712432652e-06}. Best is trial 45 with value: 2.7760606195202286.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  30%|3| 6/20 [01:10<02:41, 11.50s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  35%|3| 7/20 [01:22<02:31, 11.64s/i\u001b[32m[I 2021-09-23 03:41:05,633]\u001b[0m Trial 49 finished with value: 2.776845422365284 and parameters: {'lambda_l1': 0.008132999344498666, 'lambda_l2': 0.28155762944209806}. Best is trial 45 with value: 2.7760606195202286.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  35%|3| 7/20 [01:22<02:31, 11.64s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  40%|4| 8/20 [01:33<02:18, 11.58s/i\u001b[32m[I 2021-09-23 03:41:17,070]\u001b[0m Trial 50 finished with value: 2.777861125155483 and parameters: {'lambda_l1': 1.8044092485903225e-08, 'lambda_l2': 0.002251222122874101}. Best is trial 45 with value: 2.7760606195202286.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  40%|4| 8/20 [01:33<02:18, 11.58s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  45%|4| 9/20 [01:45<02:06, 11.54s/i\u001b[32m[I 2021-09-23 03:41:28,533]\u001b[0m Trial 51 finished with value: 2.7788941903830944 and parameters: {'lambda_l1': 1.216722025939574, 'lambda_l2': 1.551185031572325e-07}. Best is trial 45 with value: 2.7760606195202286.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  45%|4| 9/20 [01:45<02:06, 11.54s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  50%|5| 10/20 [01:56<01:55, 11.53s/\u001b[32m[I 2021-09-23 03:41:40,036]\u001b[0m Trial 52 finished with value: 2.7811108976450836 and parameters: {'lambda_l1': 0.0034366810283998706, 'lambda_l2': 2.8763064628812964}. Best is trial 45 with value: 2.7760606195202286.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  50%|5| 10/20 [01:56<01:55, 11.53s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  55%|5| 11/20 [02:08<01:45, 11.73s/\u001b[32m[I 2021-09-23 03:41:52,235]\u001b[0m Trial 53 finished with value: 2.7760606194585487 and parameters: {'lambda_l1': 1.1614036692566033e-08, 'lambda_l2': 1.1752199911128163e-08}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  55%|5| 11/20 [02:08<01:45, 11.73s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  60%|6| 12/20 [02:20<01:33, 11.74s/\u001b[32m[I 2021-09-23 03:42:03,991]\u001b[0m Trial 54 finished with value: 2.7760606194607234 and parameters: {'lambda_l1': 1.656130076544553e-08, 'lambda_l2': 2.4611394396952482e-08}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  60%|6| 12/20 [02:20<01:33, 11.74s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  65%|6| 13/20 [02:32<01:21, 11.71s/\u001b[32m[I 2021-09-23 03:42:15,618]\u001b[0m Trial 55 finished with value: 2.7760606194586694 and parameters: {'lambda_l1': 1.4514479770923962e-08, 'lambda_l2': 1.0941180200290299e-08}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  65%|6| 13/20 [02:32<01:21, 11.71s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  70%|7| 14/20 [02:44<01:10, 11.78s/\u001b[32m[I 2021-09-23 03:42:27,577]\u001b[0m Trial 56 finished with value: 2.776243188405737 and parameters: {'lambda_l1': 4.0674307435169754e-07, 'lambda_l2': 4.5482454616272845e-05}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  70%|7| 14/20 [02:44<01:10, 11.78s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  75%|7| 15/20 [02:55<00:59, 11.83s/\u001b[32m[I 2021-09-23 03:42:39,525]\u001b[0m Trial 57 finished with value: 2.7760606194717585 and parameters: {'lambda_l1': 3.452099645099457e-07, 'lambda_l2': 3.4114466191922604e-08}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  75%|7| 15/20 [02:55<00:59, 11.83s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  80%|8| 16/20 [03:07<00:47, 11.83s/\u001b[32m[I 2021-09-23 03:42:51,355]\u001b[0m Trial 58 finished with value: 2.776432931319516 and parameters: {'lambda_l1': 1.3347922959109903e-08, 'lambda_l2': 4.395948701825604e-05}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  80%|8| 16/20 [03:07<00:47, 11.83s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  85%|8| 17/20 [03:19<00:35, 11.89s/\u001b[32m[I 2021-09-23 03:43:03,369]\u001b[0m Trial 59 finished with value: 2.7760606196661106 and parameters: {'lambda_l1': 7.499578197693269e-06, 'lambda_l2': 1.0907009489442933e-08}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  85%|8| 17/20 [03:19<00:35, 11.89s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  90%|9| 18/20 [03:32<00:24, 12.11s/\u001b[32m[I 2021-09-23 03:43:16,014]\u001b[0m Trial 60 finished with value: 2.77606062063052 and parameters: {'lambda_l1': 8.480352718151953e-08, 'lambda_l2': 8.054890670287336e-06}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  90%|9| 18/20 [03:32<00:24, 12.11s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061:  95%|9| 19/20 [03:43<00:11, 11.86s/\u001b[32m[I 2021-09-23 03:43:27,296]\u001b[0m Trial 61 finished with value: 2.7787299878281657 and parameters: {'lambda_l1': 4.537400029753006e-06, 'lambda_l2': 0.026633261065638174}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061:  95%|9| 19/20 [03:43<00:11, 11.86s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 2.776061: 100%|#| 20/20 [03:56<00:00, 12.16s/\u001b[32m[I 2021-09-23 03:43:40,141]\u001b[0m Trial 62 finished with value: 2.7764910033206167 and parameters: {'lambda_l1': 0.13295915601315594, 'lambda_l2': 1.0032285606634422e-08}. Best is trial 53 with value: 2.7760606194585487.\u001b[0m\n",
      "regularization_factors, val_score: 2.776061: 100%|#| 20/20 [03:56<00:00, 11.83s/\n",
      "min_data_in_leaf, val_score: 2.776061:   0%|              | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 2.776061:  20%|#2    | 1/5 [00:11<00:46, 11.51s/it]\u001b[32m[I 2021-09-23 03:43:51,660]\u001b[0m Trial 63 finished with value: 2.780476616563173 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 2.780476616563173.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2.776061:  20%|#2    | 1/5 [00:11<00:46, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 2.776061:  40%|##4   | 2/5 [00:23<00:35, 11.83s/it]\u001b[32m[I 2021-09-23 03:44:03,717]\u001b[0m Trial 64 finished with value: 2.777354511081523 and parameters: {'min_child_samples': 10}. Best is trial 64 with value: 2.777354511081523.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2.776061:  40%|##4   | 2/5 [00:23<00:35, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034586 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 2.776061:  60%|###6  | 3/5 [00:33<00:22, 11.03s/it]\u001b[32m[I 2021-09-23 03:44:13,798]\u001b[0m Trial 65 finished with value: 2.7807243076054102 and parameters: {'min_child_samples': 100}. Best is trial 64 with value: 2.777354511081523.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2.776061:  60%|###6  | 3/5 [00:33<00:22, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 2.776061:  80%|####8 | 4/5 [00:45<00:11, 11.41s/it]\u001b[32m[I 2021-09-23 03:44:25,796]\u001b[0m Trial 66 finished with value: 2.7783732202910962 and parameters: {'min_child_samples': 25}. Best is trial 64 with value: 2.777354511081523.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2.776061:  80%|####8 | 4/5 [00:45<00:11, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6436\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 2.776061: 100%|######| 5/5 [00:58<00:00, 11.95s/it]\u001b[32m[I 2021-09-23 03:44:38,695]\u001b[0m Trial 67 finished with value: 2.777093492849462 and parameters: {'min_child_samples': 5}. Best is trial 67 with value: 2.777093492849462.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2.776061: 100%|######| 5/5 [00:58<00:00, 11.71s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "714.4210948944092"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "best_params, tuning_history = dict(), list()\n",
    "# ハイパーパラメータサーチ&モデル構築(回帰モデル用)\n",
    "tuning_params = {'objective': 'regression',\n",
    "          'metric': 'rmse',\n",
    "          'num_boost_round': 5000, # 最大試行数\n",
    "          'random_seed':100,\n",
    "         'learning_rate': 0.01, # 学習率\n",
    "         }\n",
    "\n",
    "booster = op_lgb.train(tuning_params, \n",
    "                       dtrain_tuning, \n",
    "                       valid_sets=[dtrain_tuning, dval_tuning],\n",
    "                       verbose_eval=0,\n",
    "                       early_stopping_rounds=5,)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7b478-03c8-416b-b617-1ecd3f89fecf",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0578e2a5-d7e8-421b-8402-3dd650747c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train) #(DataFrame, Series)\n",
    "lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train) #(DataFrame, Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3bc7c4e-18e1-49ab-9758-506896b17dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'regression', 'metric': 'rmse', 'verbose': 1000, 'random_seed': 100, 'learning_rate': 0.01, 'feature_pre_filter': False, 'lambda_l1': 1.1614036692566033e-08, 'lambda_l2': 1.1752199911128163e-08, 'num_leaves': 7, 'feature_fraction': 0.852, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'num_iterations': 5000, 'early_stopping_round': 5}\n"
     ]
    }
   ],
   "source": [
    "lgbm_params = {\n",
    "    \"objective\":\"regression\",\n",
    "    \"metric\":\"rmse\",\n",
    "   \"verbose\":1000,\n",
    "    \"random_seed\":100,\n",
    "    'learning_rate': 0.01,\n",
    "        }\n",
    "lgbm_params.update(booster.params)\n",
    "lgbm_params['num_leaves'] = int(lgbm_params['num_leaves'])\n",
    "lgbm_params['min_child_samples'] = int(lgbm_params['min_child_samples'])\n",
    "lgbm_params['bagging_freq'] = int(lgbm_params['bagging_freq'])\n",
    "\n",
    "print(lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "790f27e5-062f-45ca-a954-540c2d92abc5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.951397\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.168378\n",
      "[LightGBM] [Debug] init for col-wise cost 0.003934 seconds, init for row-wise cost 0.040780 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 12138\n",
      "[LightGBM] [Info] Number of data points in the train set: 601974, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.723507\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[1]\ttrain's rmse: 2.35025\tvalid's rmse: 3.30739\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[2]\ttrain's rmse: 2.34172\tvalid's rmse: 3.30144\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[3]\ttrain's rmse: 2.33237\tvalid's rmse: 3.29642\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[4]\ttrain's rmse: 2.32282\tvalid's rmse: 3.29214\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[5]\ttrain's rmse: 2.31345\tvalid's rmse: 3.28798\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[6]\ttrain's rmse: 2.30451\tvalid's rmse: 3.28324\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[7]\ttrain's rmse: 2.29538\tvalid's rmse: 3.2793\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[8]\ttrain's rmse: 2.28817\tvalid's rmse: 3.27406\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[9]\ttrain's rmse: 2.28065\tvalid's rmse: 3.26921\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[10]\ttrain's rmse: 2.2729\tvalid's rmse: 3.26386\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[11]\ttrain's rmse: 2.26431\tvalid's rmse: 3.25935\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[12]\ttrain's rmse: 2.25557\tvalid's rmse: 3.25585\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[13]\ttrain's rmse: 2.24699\tvalid's rmse: 3.25231\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[14]\ttrain's rmse: 2.23872\tvalid's rmse: 3.24769\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[15]\ttrain's rmse: 2.23032\tvalid's rmse: 3.24346\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[16]\ttrain's rmse: 2.22225\tvalid's rmse: 3.23929\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[17]\ttrain's rmse: 2.21397\tvalid's rmse: 3.23514\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[18]\ttrain's rmse: 2.20591\tvalid's rmse: 3.232\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[19]\ttrain's rmse: 2.198\tvalid's rmse: 3.2278\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[20]\ttrain's rmse: 2.19012\tvalid's rmse: 3.22472\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[21]\ttrain's rmse: 2.18272\tvalid's rmse: 3.22072\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[22]\ttrain's rmse: 2.17502\tvalid's rmse: 3.21667\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[23]\ttrain's rmse: 2.16767\tvalid's rmse: 3.21292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[24]\ttrain's rmse: 2.16025\tvalid's rmse: 3.20899\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[25]\ttrain's rmse: 2.15324\tvalid's rmse: 3.20495\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[26]\ttrain's rmse: 2.14604\tvalid's rmse: 3.2012\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[27]\ttrain's rmse: 2.13974\tvalid's rmse: 3.19679\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[28]\ttrain's rmse: 2.13221\tvalid's rmse: 3.19394\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[29]\ttrain's rmse: 2.1253\tvalid's rmse: 3.19084\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[30]\ttrain's rmse: 2.11851\tvalid's rmse: 3.18805\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[31]\ttrain's rmse: 2.1118\tvalid's rmse: 3.18449\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[32]\ttrain's rmse: 2.10491\tvalid's rmse: 3.18213\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[33]\ttrain's rmse: 2.09838\tvalid's rmse: 3.17863\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[34]\ttrain's rmse: 2.09189\tvalid's rmse: 3.17578\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[35]\ttrain's rmse: 2.0853\tvalid's rmse: 3.17254\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[36]\ttrain's rmse: 2.07893\tvalid's rmse: 3.16978\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[37]\ttrain's rmse: 2.07339\tvalid's rmse: 3.16571\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[38]\ttrain's rmse: 2.06748\tvalid's rmse: 3.1628\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[39]\ttrain's rmse: 2.06117\tvalid's rmse: 3.15974\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[40]\ttrain's rmse: 2.05572\tvalid's rmse: 3.1559\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[41]\ttrain's rmse: 2.04959\tvalid's rmse: 3.15295\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[42]\ttrain's rmse: 2.04434\tvalid's rmse: 3.14916\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[43]\ttrain's rmse: 2.03818\tvalid's rmse: 3.14723\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[44]\ttrain's rmse: 2.03257\tvalid's rmse: 3.14406\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[45]\ttrain's rmse: 2.02716\tvalid's rmse: 3.14155\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[46]\ttrain's rmse: 2.02154\tvalid's rmse: 3.13931\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[47]\ttrain's rmse: 2.01698\tvalid's rmse: 3.13647\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[48]\ttrain's rmse: 2.01165\tvalid's rmse: 3.13387\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[49]\ttrain's rmse: 2.00606\tvalid's rmse: 3.13172\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[50]\ttrain's rmse: 2.0007\tvalid's rmse: 3.12971\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[51]\ttrain's rmse: 1.99527\tvalid's rmse: 3.12719\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[52]\ttrain's rmse: 1.98964\tvalid's rmse: 3.12549\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[53]\ttrain's rmse: 1.98461\tvalid's rmse: 3.12307\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[54]\ttrain's rmse: 1.97966\tvalid's rmse: 3.12033\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[55]\ttrain's rmse: 1.97525\tvalid's rmse: 3.11721\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[56]\ttrain's rmse: 1.96988\tvalid's rmse: 3.11568\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[57]\ttrain's rmse: 1.96448\tvalid's rmse: 3.11422\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[58]\ttrain's rmse: 1.95943\tvalid's rmse: 3.112\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[59]\ttrain's rmse: 1.95473\tvalid's rmse: 3.10952\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[60]\ttrain's rmse: 1.95002\tvalid's rmse: 3.10755\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[61]\ttrain's rmse: 1.94533\tvalid's rmse: 3.10535\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[62]\ttrain's rmse: 1.9406\tvalid's rmse: 3.10353\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[63]\ttrain's rmse: 1.93623\tvalid's rmse: 3.10141\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[64]\ttrain's rmse: 1.93194\tvalid's rmse: 3.09917\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[65]\ttrain's rmse: 1.92738\tvalid's rmse: 3.09803\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[66]\ttrain's rmse: 1.92351\tvalid's rmse: 3.0954\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[67]\ttrain's rmse: 1.91922\tvalid's rmse: 3.09337\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[68]\ttrain's rmse: 1.91451\tvalid's rmse: 3.09217\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[69]\ttrain's rmse: 1.91037\tvalid's rmse: 3.09006\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[70]\ttrain's rmse: 1.9067\tvalid's rmse: 3.08756\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[71]\ttrain's rmse: 1.90244\tvalid's rmse: 3.08642\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[72]\ttrain's rmse: 1.89811\tvalid's rmse: 3.08559\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[73]\ttrain's rmse: 1.89417\tvalid's rmse: 3.08407\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[74]\ttrain's rmse: 1.89112\tvalid's rmse: 3.08152\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[75]\ttrain's rmse: 1.88696\tvalid's rmse: 3.08001\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[76]\ttrain's rmse: 1.88355\tvalid's rmse: 3.07752\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[77]\ttrain's rmse: 1.87954\tvalid's rmse: 3.07557\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[78]\ttrain's rmse: 1.87532\tvalid's rmse: 3.07451\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[79]\ttrain's rmse: 1.87147\tvalid's rmse: 3.07298\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[80]\ttrain's rmse: 1.86832\tvalid's rmse: 3.07103\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[81]\ttrain's rmse: 1.86412\tvalid's rmse: 3.07\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[82]\ttrain's rmse: 1.86056\tvalid's rmse: 3.06828\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[83]\ttrain's rmse: 1.85666\tvalid's rmse: 3.0669\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[84]\ttrain's rmse: 1.85276\tvalid's rmse: 3.06591\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[85]\ttrain's rmse: 1.8491\tvalid's rmse: 3.0648\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[86]\ttrain's rmse: 1.84604\tvalid's rmse: 3.06262\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[87]\ttrain's rmse: 1.84233\tvalid's rmse: 3.06132\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[88]\ttrain's rmse: 1.83865\tvalid's rmse: 3.06043\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[89]\ttrain's rmse: 1.83505\tvalid's rmse: 3.05915\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[90]\ttrain's rmse: 1.83175\tvalid's rmse: 3.058\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[91]\ttrain's rmse: 1.82815\tvalid's rmse: 3.05737\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[92]\ttrain's rmse: 1.82482\tvalid's rmse: 3.05605\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[93]\ttrain's rmse: 1.82138\tvalid's rmse: 3.05526\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[94]\ttrain's rmse: 1.81804\tvalid's rmse: 3.05412\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[95]\ttrain's rmse: 1.81494\tvalid's rmse: 3.05307\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[96]\ttrain's rmse: 1.81214\tvalid's rmse: 3.05102\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[97]\ttrain's rmse: 1.80902\tvalid's rmse: 3.05015\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[98]\ttrain's rmse: 1.80575\tvalid's rmse: 3.04901\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[99]\ttrain's rmse: 1.80315\tvalid's rmse: 3.04712\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[100]\ttrain's rmse: 1.80003\tvalid's rmse: 3.0464\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[101]\ttrain's rmse: 1.7969\tvalid's rmse: 3.04557\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[102]\ttrain's rmse: 1.79372\tvalid's rmse: 3.04478\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[103]\ttrain's rmse: 1.79073\tvalid's rmse: 3.04384\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[104]\ttrain's rmse: 1.78767\tvalid's rmse: 3.04315\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[105]\ttrain's rmse: 1.78467\tvalid's rmse: 3.04212\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[106]\ttrain's rmse: 1.78184\tvalid's rmse: 3.04141\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[107]\ttrain's rmse: 1.7788\tvalid's rmse: 3.0403\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[108]\ttrain's rmse: 1.77598\tvalid's rmse: 3.03969\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[109]\ttrain's rmse: 1.77357\tvalid's rmse: 3.03793\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[110]\ttrain's rmse: 1.77069\tvalid's rmse: 3.03705\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[111]\ttrain's rmse: 1.76792\tvalid's rmse: 3.03648\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[112]\ttrain's rmse: 1.76563\tvalid's rmse: 3.0347\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[113]\ttrain's rmse: 1.76271\tvalid's rmse: 3.03366\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[114]\ttrain's rmse: 1.76008\tvalid's rmse: 3.03316\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[115]\ttrain's rmse: 1.75754\tvalid's rmse: 3.03263\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[116]\ttrain's rmse: 1.75494\tvalid's rmse: 3.03213\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[117]\ttrain's rmse: 1.75246\tvalid's rmse: 3.03167\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[118]\ttrain's rmse: 1.74998\tvalid's rmse: 3.03116\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[119]\ttrain's rmse: 1.74757\tvalid's rmse: 3.03061\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[120]\ttrain's rmse: 1.74478\tvalid's rmse: 3.02969\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[121]\ttrain's rmse: 1.74243\tvalid's rmse: 3.0292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[122]\ttrain's rmse: 1.73966\tvalid's rmse: 3.02807\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[123]\ttrain's rmse: 1.73742\tvalid's rmse: 3.02721\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[124]\ttrain's rmse: 1.73467\tvalid's rmse: 3.02637\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[125]\ttrain's rmse: 1.73205\tvalid's rmse: 3.02545\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[126]\ttrain's rmse: 1.72972\tvalid's rmse: 3.02453\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[127]\ttrain's rmse: 1.727\tvalid's rmse: 3.02364\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[128]\ttrain's rmse: 1.72459\tvalid's rmse: 3.02274\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[129]\ttrain's rmse: 1.7225\tvalid's rmse: 3.02216\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[130]\ttrain's rmse: 1.71993\tvalid's rmse: 3.02139\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[131]\ttrain's rmse: 1.71807\tvalid's rmse: 3.01998\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[132]\ttrain's rmse: 1.71552\tvalid's rmse: 3.01935\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[133]\ttrain's rmse: 1.71348\tvalid's rmse: 3.01852\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[134]\ttrain's rmse: 1.71146\tvalid's rmse: 3.01804\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[135]\ttrain's rmse: 1.70887\tvalid's rmse: 3.01738\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[136]\ttrain's rmse: 1.70687\tvalid's rmse: 3.01682\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[137]\ttrain's rmse: 1.7044\tvalid's rmse: 3.01612\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[138]\ttrain's rmse: 1.70239\tvalid's rmse: 3.01569\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[139]\ttrain's rmse: 1.69993\tvalid's rmse: 3.015\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[140]\ttrain's rmse: 1.69822\tvalid's rmse: 3.01358\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[141]\ttrain's rmse: 1.69586\tvalid's rmse: 3.01291\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[142]\ttrain's rmse: 1.69358\tvalid's rmse: 3.01213\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[143]\ttrain's rmse: 1.69193\tvalid's rmse: 3.01082\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[144]\ttrain's rmse: 1.69006\tvalid's rmse: 3.01033\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[145]\ttrain's rmse: 1.68826\tvalid's rmse: 3.00997\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[146]\ttrain's rmse: 1.68664\tvalid's rmse: 3.00863\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[147]\ttrain's rmse: 1.68485\tvalid's rmse: 3.00816\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[148]\ttrain's rmse: 1.68262\tvalid's rmse: 3.00776\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[149]\ttrain's rmse: 1.6809\tvalid's rmse: 3.00758\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[150]\ttrain's rmse: 1.67939\tvalid's rmse: 3.00629\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[151]\ttrain's rmse: 1.67748\tvalid's rmse: 3.00581\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[152]\ttrain's rmse: 1.67564\tvalid's rmse: 3.00547\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[153]\ttrain's rmse: 1.67375\tvalid's rmse: 3.00496\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[154]\ttrain's rmse: 1.67224\tvalid's rmse: 3.00351\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[155]\ttrain's rmse: 1.67077\tvalid's rmse: 3.00228\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[156]\ttrain's rmse: 1.66894\tvalid's rmse: 3.00184\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[157]\ttrain's rmse: 1.66708\tvalid's rmse: 3.00144\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[158]\ttrain's rmse: 1.66505\tvalid's rmse: 3.00087\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[159]\ttrain's rmse: 1.66363\tvalid's rmse: 2.99972\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[160]\ttrain's rmse: 1.66211\tvalid's rmse: 2.99948\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[161]\ttrain's rmse: 1.66052\tvalid's rmse: 2.99918\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[162]\ttrain's rmse: 1.65914\tvalid's rmse: 2.99805\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[163]\ttrain's rmse: 1.65738\tvalid's rmse: 2.99763\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[164]\ttrain's rmse: 1.65549\tvalid's rmse: 2.99755\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[165]\ttrain's rmse: 1.65391\tvalid's rmse: 2.99743\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[166]\ttrain's rmse: 1.65207\tvalid's rmse: 2.99736\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[167]\ttrain's rmse: 1.65038\tvalid's rmse: 2.99701\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[168]\ttrain's rmse: 1.64851\tvalid's rmse: 2.99666\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[169]\ttrain's rmse: 1.64664\tvalid's rmse: 2.99618\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[170]\ttrain's rmse: 1.64484\tvalid's rmse: 2.9957\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[171]\ttrain's rmse: 1.64345\tvalid's rmse: 2.99524\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[172]\ttrain's rmse: 1.64193\tvalid's rmse: 2.99491\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[173]\ttrain's rmse: 1.64016\tvalid's rmse: 2.99445\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[174]\ttrain's rmse: 1.63849\tvalid's rmse: 2.99415\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[175]\ttrain's rmse: 1.63679\tvalid's rmse: 2.99368\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[176]\ttrain's rmse: 1.63513\tvalid's rmse: 2.99345\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[177]\ttrain's rmse: 1.6335\tvalid's rmse: 2.99303\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[178]\ttrain's rmse: 1.63236\tvalid's rmse: 2.99204\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[179]\ttrain's rmse: 1.63074\tvalid's rmse: 2.99185\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[180]\ttrain's rmse: 1.62904\tvalid's rmse: 2.99173\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[181]\ttrain's rmse: 1.62789\tvalid's rmse: 2.99102\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[182]\ttrain's rmse: 1.62676\tvalid's rmse: 2.99006\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[183]\ttrain's rmse: 1.62538\tvalid's rmse: 2.98987\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[184]\ttrain's rmse: 1.62369\tvalid's rmse: 2.98978\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[185]\ttrain's rmse: 1.62192\tvalid's rmse: 2.98966\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[186]\ttrain's rmse: 1.62019\tvalid's rmse: 2.98948\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[187]\ttrain's rmse: 1.61895\tvalid's rmse: 2.98922\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[188]\ttrain's rmse: 1.61744\tvalid's rmse: 2.98907\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[189]\ttrain's rmse: 1.61594\tvalid's rmse: 2.9891\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[190]\ttrain's rmse: 1.61444\tvalid's rmse: 2.98873\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[191]\ttrain's rmse: 1.61294\tvalid's rmse: 2.98836\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[192]\ttrain's rmse: 1.61153\tvalid's rmse: 2.98822\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[193]\ttrain's rmse: 1.60982\tvalid's rmse: 2.98813\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[194]\ttrain's rmse: 1.60836\tvalid's rmse: 2.98779\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[195]\ttrain's rmse: 1.60695\tvalid's rmse: 2.98734\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[196]\ttrain's rmse: 1.60572\tvalid's rmse: 2.987\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[197]\ttrain's rmse: 1.60436\tvalid's rmse: 2.98657\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[198]\ttrain's rmse: 1.60316\tvalid's rmse: 2.98617\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[199]\ttrain's rmse: 1.60187\tvalid's rmse: 2.98583\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[200]\ttrain's rmse: 1.60038\tvalid's rmse: 2.98575\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[201]\ttrain's rmse: 1.59911\tvalid's rmse: 2.98531\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[202]\ttrain's rmse: 1.59782\tvalid's rmse: 2.98517\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[203]\ttrain's rmse: 1.59653\tvalid's rmse: 2.98504\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[204]\ttrain's rmse: 1.59529\tvalid's rmse: 2.9848\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[205]\ttrain's rmse: 1.59362\tvalid's rmse: 2.98425\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[206]\ttrain's rmse: 1.59205\tvalid's rmse: 2.98374\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[207]\ttrain's rmse: 1.59099\tvalid's rmse: 2.98362\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[208]\ttrain's rmse: 1.58982\tvalid's rmse: 2.98334\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[209]\ttrain's rmse: 1.58871\tvalid's rmse: 2.98338\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[210]\ttrain's rmse: 1.58752\tvalid's rmse: 2.9829\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[211]\ttrain's rmse: 1.58613\tvalid's rmse: 2.98267\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[212]\ttrain's rmse: 1.58512\tvalid's rmse: 2.98247\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[213]\ttrain's rmse: 1.5838\tvalid's rmse: 2.98228\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[214]\ttrain's rmse: 1.58227\tvalid's rmse: 2.98176\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[215]\ttrain's rmse: 1.58139\tvalid's rmse: 2.98094\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[216]\ttrain's rmse: 1.58042\tvalid's rmse: 2.98075\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[217]\ttrain's rmse: 1.57927\tvalid's rmse: 2.98036\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[218]\ttrain's rmse: 1.57827\tvalid's rmse: 2.98042\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[219]\ttrain's rmse: 1.57681\tvalid's rmse: 2.97993\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[220]\ttrain's rmse: 1.57563\tvalid's rmse: 2.97983\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[221]\ttrain's rmse: 1.57416\tvalid's rmse: 2.97972\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[222]\ttrain's rmse: 1.57275\tvalid's rmse: 2.97923\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[223]\ttrain's rmse: 1.5717\tvalid's rmse: 2.97906\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[224]\ttrain's rmse: 1.57027\tvalid's rmse: 2.97857\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[225]\ttrain's rmse: 1.56922\tvalid's rmse: 2.9785\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[226]\ttrain's rmse: 1.56781\tvalid's rmse: 2.97846\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[227]\ttrain's rmse: 1.56677\tvalid's rmse: 2.97831\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[228]\ttrain's rmse: 1.56577\tvalid's rmse: 2.97808\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[229]\ttrain's rmse: 1.56463\tvalid's rmse: 2.97794\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[230]\ttrain's rmse: 1.56384\tvalid's rmse: 2.97721\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[231]\ttrain's rmse: 1.56276\tvalid's rmse: 2.97707\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[232]\ttrain's rmse: 1.56145\tvalid's rmse: 2.97703\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[233]\ttrain's rmse: 1.56042\tvalid's rmse: 2.97689\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[234]\ttrain's rmse: 1.55917\tvalid's rmse: 2.9769\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[235]\ttrain's rmse: 1.55826\tvalid's rmse: 2.97675\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[236]\ttrain's rmse: 1.5573\tvalid's rmse: 2.97666\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[237]\ttrain's rmse: 1.55597\tvalid's rmse: 2.97623\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[238]\ttrain's rmse: 1.55467\tvalid's rmse: 2.97578\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[239]\ttrain's rmse: 1.55365\tvalid's rmse: 2.97565\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[240]\ttrain's rmse: 1.55268\tvalid's rmse: 2.97544\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[241]\ttrain's rmse: 1.55161\tvalid's rmse: 2.97516\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[242]\ttrain's rmse: 1.55089\tvalid's rmse: 2.97448\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[243]\ttrain's rmse: 1.54992\tvalid's rmse: 2.97426\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[244]\ttrain's rmse: 1.54906\tvalid's rmse: 2.97407\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[245]\ttrain's rmse: 1.54829\tvalid's rmse: 2.97317\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[246]\ttrain's rmse: 1.54762\tvalid's rmse: 2.97256\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[247]\ttrain's rmse: 1.54665\tvalid's rmse: 2.97245\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[248]\ttrain's rmse: 1.54571\tvalid's rmse: 2.97235\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[249]\ttrain's rmse: 1.54484\tvalid's rmse: 2.9722\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[250]\ttrain's rmse: 1.544\tvalid's rmse: 2.97209\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[251]\ttrain's rmse: 1.54317\tvalid's rmse: 2.97197\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[252]\ttrain's rmse: 1.54226\tvalid's rmse: 2.97188\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[253]\ttrain's rmse: 1.54114\tvalid's rmse: 2.9718\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[254]\ttrain's rmse: 1.5399\tvalid's rmse: 2.97161\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[255]\ttrain's rmse: 1.53897\tvalid's rmse: 2.97152\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[256]\ttrain's rmse: 1.53799\tvalid's rmse: 2.97148\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[257]\ttrain's rmse: 1.53702\tvalid's rmse: 2.97139\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[258]\ttrain's rmse: 1.53606\tvalid's rmse: 2.97135\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[259]\ttrain's rmse: 1.53498\tvalid's rmse: 2.97132\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[260]\ttrain's rmse: 1.53408\tvalid's rmse: 2.97123\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[261]\ttrain's rmse: 1.53296\tvalid's rmse: 2.97085\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[262]\ttrain's rmse: 1.53229\tvalid's rmse: 2.97026\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[263]\ttrain's rmse: 1.53156\tvalid's rmse: 2.97019\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[264]\ttrain's rmse: 1.53091\tvalid's rmse: 2.96939\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[265]\ttrain's rmse: 1.52985\tvalid's rmse: 2.96931\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[266]\ttrain's rmse: 1.52924\tvalid's rmse: 2.96866\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[267]\ttrain's rmse: 1.52852\tvalid's rmse: 2.96857\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[268]\ttrain's rmse: 1.52782\tvalid's rmse: 2.96849\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[269]\ttrain's rmse: 1.52714\tvalid's rmse: 2.96841\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[270]\ttrain's rmse: 1.52619\tvalid's rmse: 2.9683\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[271]\ttrain's rmse: 1.52547\tvalid's rmse: 2.96823\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[272]\ttrain's rmse: 1.52479\tvalid's rmse: 2.96818\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[273]\ttrain's rmse: 1.52418\tvalid's rmse: 2.96743\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[274]\ttrain's rmse: 1.52315\tvalid's rmse: 2.96738\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[275]\ttrain's rmse: 1.52213\tvalid's rmse: 2.96735\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[276]\ttrain's rmse: 1.52103\tvalid's rmse: 2.96722\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[277]\ttrain's rmse: 1.52022\tvalid's rmse: 2.96684\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[278]\ttrain's rmse: 1.51901\tvalid's rmse: 2.96676\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 3\n",
      "[279]\ttrain's rmse: 1.51829\tvalid's rmse: 2.9665\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[280]\ttrain's rmse: 1.51712\tvalid's rmse: 2.96641\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[281]\ttrain's rmse: 1.51655\tvalid's rmse: 2.96577\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[282]\ttrain's rmse: 1.51569\tvalid's rmse: 2.96569\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[283]\ttrain's rmse: 1.51455\tvalid's rmse: 2.96562\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[284]\ttrain's rmse: 1.51365\tvalid's rmse: 2.96555\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[285]\ttrain's rmse: 1.51293\tvalid's rmse: 2.96553\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[286]\ttrain's rmse: 1.51225\tvalid's rmse: 2.96534\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[287]\ttrain's rmse: 1.51158\tvalid's rmse: 2.96528\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[288]\ttrain's rmse: 1.51049\tvalid's rmse: 2.9652\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[289]\ttrain's rmse: 1.50964\tvalid's rmse: 2.96513\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[290]\ttrain's rmse: 1.50886\tvalid's rmse: 2.96506\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[291]\ttrain's rmse: 1.508\tvalid's rmse: 2.96502\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[292]\ttrain's rmse: 1.50743\tvalid's rmse: 2.965\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[293]\ttrain's rmse: 1.50686\tvalid's rmse: 2.96497\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[294]\ttrain's rmse: 1.50628\tvalid's rmse: 2.96495\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[295]\ttrain's rmse: 1.50569\tvalid's rmse: 2.96486\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[296]\ttrain's rmse: 1.50512\tvalid's rmse: 2.96483\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[297]\ttrain's rmse: 1.50459\tvalid's rmse: 2.96417\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[298]\ttrain's rmse: 1.50376\tvalid's rmse: 2.96414\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[299]\ttrain's rmse: 1.50315\tvalid's rmse: 2.96408\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[300]\ttrain's rmse: 1.50234\tvalid's rmse: 2.96413\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[301]\ttrain's rmse: 1.50178\tvalid's rmse: 2.964\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[302]\ttrain's rmse: 1.50122\tvalid's rmse: 2.96395\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[303]\ttrain's rmse: 1.50072\tvalid's rmse: 2.96361\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[304]\ttrain's rmse: 1.50015\tvalid's rmse: 2.96355\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[305]\ttrain's rmse: 1.49966\tvalid's rmse: 2.96322\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[306]\ttrain's rmse: 1.4989\tvalid's rmse: 2.96322\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[307]\ttrain's rmse: 1.49836\tvalid's rmse: 2.9632\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[308]\ttrain's rmse: 1.49749\tvalid's rmse: 2.96309\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[309]\ttrain's rmse: 1.49674\tvalid's rmse: 2.96317\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[310]\ttrain's rmse: 1.496\tvalid's rmse: 2.9632\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[311]\ttrain's rmse: 1.49516\tvalid's rmse: 2.96313\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[312]\ttrain's rmse: 1.49468\tvalid's rmse: 2.96255\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[313]\ttrain's rmse: 1.49414\tvalid's rmse: 2.9625\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[314]\ttrain's rmse: 1.4932\tvalid's rmse: 2.96244\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[315]\ttrain's rmse: 1.49245\tvalid's rmse: 2.96233\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[316]\ttrain's rmse: 1.49192\tvalid's rmse: 2.96221\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[317]\ttrain's rmse: 1.49142\tvalid's rmse: 2.96205\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[318]\ttrain's rmse: 1.49055\tvalid's rmse: 2.96199\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[319]\ttrain's rmse: 1.4901\tvalid's rmse: 2.96168\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[320]\ttrain's rmse: 1.4896\tvalid's rmse: 2.96167\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[321]\ttrain's rmse: 1.4891\tvalid's rmse: 2.96161\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[322]\ttrain's rmse: 1.4886\tvalid's rmse: 2.96161\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[323]\ttrain's rmse: 1.48815\tvalid's rmse: 2.96151\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[324]\ttrain's rmse: 1.48771\tvalid's rmse: 2.96108\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[325]\ttrain's rmse: 1.48729\tvalid's rmse: 2.96077\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[326]\ttrain's rmse: 1.48659\tvalid's rmse: 2.96069\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[327]\ttrain's rmse: 1.48574\tvalid's rmse: 2.96057\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[328]\ttrain's rmse: 1.4851\tvalid's rmse: 2.9606\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[329]\ttrain's rmse: 1.48446\tvalid's rmse: 2.96059\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[330]\ttrain's rmse: 1.48385\tvalid's rmse: 2.96047\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[331]\ttrain's rmse: 1.48343\tvalid's rmse: 2.96018\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[332]\ttrain's rmse: 1.48277\tvalid's rmse: 2.96013\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[333]\ttrain's rmse: 1.48209\tvalid's rmse: 2.96015\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[334]\ttrain's rmse: 1.48163\tvalid's rmse: 2.96011\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[335]\ttrain's rmse: 1.48117\tvalid's rmse: 2.95994\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[336]\ttrain's rmse: 1.48072\tvalid's rmse: 2.95992\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[337]\ttrain's rmse: 1.48028\tvalid's rmse: 2.95974\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[338]\ttrain's rmse: 1.47983\tvalid's rmse: 2.95975\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[339]\ttrain's rmse: 1.47934\tvalid's rmse: 2.95968\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[340]\ttrain's rmse: 1.47891\tvalid's rmse: 2.9597\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[341]\ttrain's rmse: 1.47848\tvalid's rmse: 2.95967\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[342]\ttrain's rmse: 1.47784\tvalid's rmse: 2.95959\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[343]\ttrain's rmse: 1.47746\tvalid's rmse: 2.9593\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[344]\ttrain's rmse: 1.47706\tvalid's rmse: 2.959\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[345]\ttrain's rmse: 1.4763\tvalid's rmse: 2.95909\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[346]\ttrain's rmse: 1.4756\tvalid's rmse: 2.95902\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[347]\ttrain's rmse: 1.47504\tvalid's rmse: 2.95899\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[348]\ttrain's rmse: 1.47463\tvalid's rmse: 2.95878\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[349]\ttrain's rmse: 1.47396\tvalid's rmse: 2.95869\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[350]\ttrain's rmse: 1.47342\tvalid's rmse: 2.95866\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[351]\ttrain's rmse: 1.47274\tvalid's rmse: 2.95875\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[352]\ttrain's rmse: 1.47237\tvalid's rmse: 2.95836\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[353]\ttrain's rmse: 1.47196\tvalid's rmse: 2.95833\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[354]\ttrain's rmse: 1.47157\tvalid's rmse: 2.95833\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[355]\ttrain's rmse: 1.47114\tvalid's rmse: 2.95834\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[356]\ttrain's rmse: 1.47077\tvalid's rmse: 2.9583\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[357]\ttrain's rmse: 1.47016\tvalid's rmse: 2.95836\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[358]\ttrain's rmse: 1.46981\tvalid's rmse: 2.95809\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[359]\ttrain's rmse: 1.46945\tvalid's rmse: 2.95806\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[360]\ttrain's rmse: 1.4691\tvalid's rmse: 2.95809\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[361]\ttrain's rmse: 1.46877\tvalid's rmse: 2.95807\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[362]\ttrain's rmse: 1.4681\tvalid's rmse: 2.95793\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[363]\ttrain's rmse: 1.46753\tvalid's rmse: 2.95803\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[364]\ttrain's rmse: 1.46689\tvalid's rmse: 2.9581\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[365]\ttrain's rmse: 1.46654\tvalid's rmse: 2.95783\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[366]\ttrain's rmse: 1.46623\tvalid's rmse: 2.95771\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[367]\ttrain's rmse: 1.46585\tvalid's rmse: 2.95769\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[368]\ttrain's rmse: 1.46552\tvalid's rmse: 2.95741\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[369]\ttrain's rmse: 1.46497\tvalid's rmse: 2.95751\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[370]\ttrain's rmse: 1.46462\tvalid's rmse: 2.95749\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[371]\ttrain's rmse: 1.46428\tvalid's rmse: 2.95708\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[372]\ttrain's rmse: 1.46375\tvalid's rmse: 2.95717\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[373]\ttrain's rmse: 1.46342\tvalid's rmse: 2.95693\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[374]\ttrain's rmse: 1.46311\tvalid's rmse: 2.95666\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[375]\ttrain's rmse: 1.46279\tvalid's rmse: 2.95641\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[376]\ttrain's rmse: 1.46239\tvalid's rmse: 2.95644\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[377]\ttrain's rmse: 1.46207\tvalid's rmse: 2.95626\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[378]\ttrain's rmse: 1.46172\tvalid's rmse: 2.95627\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[379]\ttrain's rmse: 1.46112\tvalid's rmse: 2.95637\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[380]\ttrain's rmse: 1.46073\tvalid's rmse: 2.95637\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[381]\ttrain's rmse: 1.46043\tvalid's rmse: 2.95577\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[382]\ttrain's rmse: 1.46002\tvalid's rmse: 2.9558\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[383]\ttrain's rmse: 1.4595\tvalid's rmse: 2.95586\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 4\n",
      "[384]\ttrain's rmse: 1.45922\tvalid's rmse: 2.95588\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 6\n",
      "[385]\ttrain's rmse: 1.45872\tvalid's rmse: 2.9559\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 7 and depth = 5\n",
      "[386]\ttrain's rmse: 1.45814\tvalid's rmse: 2.95601\n",
      "Early stopping, best iteration is:\n",
      "[381]\ttrain's rmse: 1.46043\tvalid's rmse: 2.95577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.05105185508728"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "FIT_PARAMS_LGB = {\"num_boost_round\": 10000, \"early_stopping_rounds\": 5,}\n",
    "\n",
    "evaluation_results  = {}  # to record evaluation results for plotting\n",
    "model = lgb.train(lgbm_params, \n",
    "                  lgb_train,\n",
    "                   **FIT_PARAMS_LGB,\n",
    "                  \n",
    "                  valid_names=['train', 'valid'], \n",
    "                  valid_sets=[lgb_train, lgb_valid],\n",
    "                 evals_result = evaluation_results,)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72529e6a-81d3-4c7e-b3b5-a79c2a159980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABssAAAK1CAYAAACO+dDoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAADwOklEQVR4nOzdeZxedXn//9c1+2SSTHYSErJA2A0CBlyACu4r1uJSrBRrlSK4fH9Iq6hotWptBbdqbREr1qLgWlpRQQURqywB2dcAIQQSsu/JZJnr98c5k9wZZjJJmJl7ltfz8TiP+yyfc+7rPhwmc+Z9fz4nMhNJkiRJkiRJkiRpOKqpdgGSJEmSJEmSJElStRiWSZIkSZIkSZIkadgyLJMkSZIkSZIkSdKwZVgmSZIkSZIkSZKkYcuwTJIkSZIkSZIkScOWYZkkSZIkSZIkSZKGLcMySRqkIuIDEbGunNoiYmvF8j/uw/H2j4iFEfG8Hto1RsRDEXHqvlffPyLiwohYHRE3R0R9teuRJEmSJEmSNPDUVbsASdK+ycwvA18GiIhPANMy893P4pCbgAeAtT2021a2W/4s3qvPRcQI4O+BozPz7iqXI0mSJEmSJGmAMiyTJAGQmauAV+xBu+3AgO9VBkwE0qBMkiRJkiRJ0u44DKMkDWERcUtEnBcRd0XE/eUQii+OiDsiYn1ELImIj1W0fzIixkbEhIh4PCJOjYg/RsSqiPhCp+POqdjnNRHxh3LIw8sjoq7cNjoivhMRy8rtr42I67up9dSI+F5E/F1EPFq+/99WbJ8UET8sj/VARJy2m8/5VuA+oLYclvLSst0xEXFjWeedEfHqHs7VzRFxdrntiYh4R0ScFRFLI+LpiHhzuW9dRPx7Wdu6iPh9RBxSbntfRHw+Ir4SEU9FxCMR8dKK931V+d9jbUT8LiJml+vrI+KfyqExF0XEJyPCf7clSZIkSZKkXuYf3SRpaJsIvAN4VWYenpltwKuATwFjgOOAD0XErLL9/sAIoAWYCvwl8HLgxcC5EfH8iuOOrtjnfOCtwLHAyUBHkPWVst3sct3/A2Z2U+tYip5to4AjgdcCH4yI15TbLwdWAAcAbwK+HhFHdvM5rwSOALZl5qjMfFdEjAauAb5Ztv8A8P2IOGg352oS8OaylguAfynPxyHAl4COZ8MdXr4eCYwD/g/4aLluHHAWcHd5Hr4LfBUgIg4Efliev0llfd8u9/s74BSK/0bPLc/NWd2cO0mSJEmSJEn7yLBMkoa+f8vMpyqWL6YIf+ZTBDgtFIFXZ7XAxzNzeWbeBdxGERJ15aLMXJiZjwI3AIeUvaBOB/42M9eUNXy+h1o3AZ/IzE2ZeQ9FsPVnEbE/RQj395m5udz2XeDtu/mcnb0OeDgzL8vMrZn5G+BWipBvd8f4ZmYuA34EjAQuzszVwE+A6QDlUI+/BP4XWAycy67n9MHM/EZmbgS+w87zeDpwa2b+KjM3A58GOp47dybwz5n5dGauAP6pXCdJkiRJkiSpFxmWSdLQd1en5f8DlgMnAuOBhd3stz0z76tY3kD3/27c1UW7iUAD8GjFtsr5rizIzPaK5YXAfsC08pj3lUNCrgLe1amezp+zs2ldvP8t5frdHeNxgMzcVC4/Ub5uAgIgIk4H/pmiN9gM4JxOx+jq/HTUdFPHhizcV7HtmxWf9z939+EkSZIkSZIk7Zu6ahcgSepz2TETEYcDkzLzQ+XyKIrhFne73968R4XlwBaKYRfnl+u665nWYVZE1FQEZrMowqmngHXAuMxMKJ7pRRlW7WG9TwJv6LTuAOCevThGd/4U+HJm3lDWdlin7d0d90ng4MoVEXFCZv4fxWd+S2beXrFt5D7WJ0mSJEmSJKkb9iyTpOFlAzAqIo6LiInA1yi+ONHY22+UmduB7wEXR0RrRBxA0fNqd0YCn4yIpog4GngncHlmLqIYBvLvI6IxIqYC17PrMIw9+SnF8JBvj4i6iHgZxdCM39u7T9al9cAJEdESER3PFtuTc3oF8IaIOLn8XH8L/FtEBMWzy/4hIiZExOiI+DLwr71QqyRJkiRJkqQKhmWSNDSsoOjJ1dlSYE3HQmYuBD4OXAP8AXgYuBrYXjZ5CthIEaot7uI9Vpfzy4C1nfbpsBJYVc5/gKJH2CPADyme17VlN5/jZmAb8CDwY+BjZS8rKJ7vdTiwCLgD+H1m/kdXn7O0sayt47OvAV4FnE1xri6m6Lm1YDfHWFrxOTt/1o3sPEefpuhB9yTwYeBvK9qtLKdn1JWZ84G3AV8ua3ozcHrZe+5zFMM33k4xHOUs4P1IkiRJkiRJ6lVRjmYlSVKfi4jXAedm5qu72HYm8PbMfHn/VyZJkiRJkiRpuLJnmSSpz0TEFRHxoYhojohJFMMw/rzadUmSJEmSJElSB8MySVJf+gRwCsUwgncD8yiek9aVVRTDO0qSJO0iIs6KiPkRcUM321sj4sqIuC8i7oiIV/Z3jZIkSZIGL4dhlCRJkiQNeBFRAzydmRO72PYN4P7M/EJETANuAI7LzJWd20qSJElSZ/YskyRJkiQNeJnZvpvNrwO+WrZbBFwFnNofdUmSJEka/OqqXUBvmjBhQs6cObPaZUiSJEnqJbfddtvyrnoSSR0iYjywKjO3VKyeDxzQTfuzgLMAWlpannfYYYf1fZGSJEmS+sW+3kMOqbBs5syZzJs3r9plSJIkSeolEfF4tWvQgNfVswWi28aZlwCXAMydOze9h5QkSZKGjn29h3QYRkmSJEnSoFU+l2xsRDRUrJ4NLKxSSZIkSZIGGcMySZIkSdKgExEtFQHZ1cD7yvXTKJ5X9j/Vqk2SJEnS4GJYJkmSJEkaLCrHTLwYOK+cPx94YUTcB/wvcHZmrurv4iRJkiQNTkPqmWWSJEnSULB161YWLVrE5s2bq11Kv2lqamLatGnU19dXuxQNYJn56or5syvmVwNvqkZNkiRJkgY/wzJJkiRpgFm0aBGjRo1i5syZRES1y+lzmcmKFStYtGgRs2bNqnY5kiRJkqRhxmEYJUmSpAFm8+bNjB8/flgEZQARwfjx44dVTzpJkiRJ0sBhWCZJkiQNQMMlKOsw3D6vJEmSJGngMCyTJEmSJEmSJEnSsGVY1tva1kPbumpXIUmSJO2zf/iHf2DOnDkceuihNDc3M2fOHObMmcMtt9yyR/t/73vf4+KLL+7jKiVJkiRJ6h111S5gyLn9P+G6f4DDXw/PPR1m/QnU1Fa7KkmSJGmPXXjhhVx44YU8/vjjvOlNb+LWW2/dq/1PP/30PqpMkiRJkqTeZ8+y3jbzRDjqrfDgL+A7fwpfmgO/+Ag8cj1sa6t2dZIkSdI+u+SSSzjvvPN40YtexN/+7d8C8IEPfICDDjqIGTNm8OlPfxqAH/3oR1xyySUAvOpVr+Izn/kMRx11FHPmzGHRokVVq1+SJEmSpK7Ys6y3TTkKXv8leNXn4MGfwZ1XwK2Xwk1fg/oWOPBkOPjlxdQ6rdrVSpIkaYD75P/ey31Pre3VYx6x/2g+8foj93q/xYsX86tf/Ypbb72VxsZGli5dSmby0EMPsWXLFg444AA+/OEPs2TJEpYvXw7Ar371K1772tdy11138dGPfpQrrriC888/v1c/jyRJkiRJz4ZhWV+pb4Ln/FkxbdkAj90ID18LD/8SHry6aDPpSDj01fCc02C/I6pbryRJkrQH3vrWt9LY2AjApEmTmDt3LieeeCIbNmxg3bp1tLXtOppCQ0MDZ511FgAveMEL+PGPf9zvNUuSJEmStDt9GpZFxIeAs4CNwF3AhzJzUcX2icAlwGxgFPD1zPynctsNwERge9n8C5n5rb6st880tMChryqmTFj2YBmcXQu/+yLceBFMOqIIzZ5zGoybVe2KJUmSNEDsSw+wvjRmzJgd87/+9a+57LLL+J//+R8mTpzI/vvv/4z2LS0tO8K1mhpHgZckSZIkDTx9FpZFxKHA8cARmdkWER8DPgKcU9Hss8AdmfnGiBgJ3BkRP83Me4HpmTn0UqMImHRYMZ3wfli/DO77b7j7h3DdPxTT1Llw2GvhkFfBxMPAPypIkiRpAHr88cc59thjmThxIj/72c9YvHgx27dv73lHSZIkSZIGkD4LyzLzQeA0gIhoBqYB93Zqdj3wk7L9+ohYDmyLiHFAe0RcBcwCbgHOy8zefVjDQDByIhz/7mJavRDu+THc+xP49SeLqXkcTH8BTH9h8byzyXOKwE2SJEnqYy0tLRxyyCE7lqdMmcL48eN3LL/xjW/kyiuvZM6cORx33HGccsopbNq0icmTJ1NfXw/A0UcfvaN9d73PJEmSJEmqpsjMvn2DiK8AfwVcC7w1M7d10aYBuBg4MDNfGxEzgd8DrwHuLLc1ZOZ7d/dec+fOzXnz5vXyJ6iSNU/Co9fD43+Ahb+HlY8W60ftDwe/HA55Jcx6MTSOrG6dkiRJ6nX3338/hx9+eLXL6Hddfe6IuC0z51apJA1xQ+oeUpIkSdI+30P26TPLADLz/RHxUeA9wJeBcyu3R8Rs4AfAr4E3lPssiIiDMnNT2ebfgJ92dfyIOIviuWhMnz69rz5G/2udCse8vZgA1i2B+b+Gh35R9D67/dtQ2wAzT4TZL4NZfwITD4faPv9PKkmSJEmSJEmSNGT05TPLpgDrM3NdZq6LiH8Fbu/U5kDgl8B7M/PqivW1QDOwqVxVA2zp6n0y8xLgEii+FdjrH2SgGDUZjvmLYtq2BRb+AR6+tpiu+UjRpr6lGLJx9stg1kkw6UifdyZJkiRJkiRJkrQbfdkN6fXAaRHxBmArcCbwYES0AFszcwvwJeBTlUFZ6XDgJxFxQmYuBd4N/KwPax1c6hrgwBcX0ys/UzzrbOFNsOhWeOR6uOaCol1jaxGezTyheN7ZfnMMzyRJkiRJkiRJkir0WViWmZdExAEUzx4bC9xLMRTjxcAC4HPAc4HZEXFexa7nZ+Y1EfEZ4OcR0QzcQqfhG1VhzPRiOuotxfLqhfD474tp4R/g4WuK9SMmlCHbKXDQKdA6rXo1S5IkSZIkSZIkDQB9+oCrzLwQuLDT6rMrts/Yzb6XAZf1SWFDXUd49tw/L5bXLYFHf1P0Onv0erjnR8X68QcXodmBpxTPPmsaXbWSJUmSJEmSJEmSqqFPwzINEKMmF8HZc/8cMmHp/UVo9sj18Mf/glsugaiFaccVQzZOORqmPLcI3CKqXb0kSZIkSZIkSVKfMSwbbiJgvyOK6YXnwrY2eOKWneHZ/30Z2rcVbZvHFaHZ/kcXr1OOhrEzDdAkSZK0i9e97nX86Ec/4qtf/Sr7778/p59++i7bn3jiCT7xiU/wH//xH1WqUJIkSZKk7hmWDXd1jTDrpGJ66cdh62Z4+l5YfEcxPXUH/P6r0L61aN80pgjPph0HU+fCtLnQMqFq5UuSJKn67r33XrZt28YHP/jBLrevXbuWxx57rJ+rkiRJkiRpzxiWaVf1TTDtecXUYVtbGaDdWQRoT94GN34BcnuxfexMmPo8mHwUTD0WDnh+EcJJkiRp0Dr33HM5/fTTOfHEEwE49dRTOf/883nf+97Hpk2bGDVqFD/84Q+ZNWvWjn2+8IUvcOSRR/LKV76S//zP/+Qzn/kM48eP50//9E+r9CkkSZIkSeqZYZl6VtdYhGBTj925bsuGIjxbdCssmlcM5XjPj4pt9SOKnmf7H1P0Qtv/GBgzw+EbJUmS9sXPPwxL7u7dY06eA6/+3G6bvOhFL+Jb3/oWJ554Ik8++SQPPvggf/zjH/na177GiSeeyGc+8xn+8z//k0984hM79lm8eDHjx4/nqaee4kMf+hB33nknEydO5G1ve1vv1i9JkiRJUi8yLNO+aWiBGS8qpg4bVxah2SPXwaJb4A9f2zl8Y8skmP78otfZ9BcVz0Cr9fKTJEkaqN74xjfykY98hC1btnDFFVdwxhlncNppp/HRj36U97znPaxZs4a3vvWtXe576623csIJJzBp0iQATj/9dL74xS/2Z/mSJEmSJO0x0wr1nhHj4NBXFRMUwzcuva8YtvGJW+GJm+D+/y22NYyCKUcVQzhOPKyYn3xUcQxJkiTt1EMPsL4yYsQIXv7yl/OLX/yCH/zgB1x55ZWcfvrpnHfeeXzrW9/i0ksv5eGHH+5y34ggM3csb9u2rb/KliRJkiRprxmWqe/UNZZDMR4Dx72rWLduCTz+e1hwIyx9AOb/Gu64fOc+rQfApMPL6QiYeChMOKToySZJkqR+deaZZ/KlL32JkSNHMmPGDJ566ile/OIXs3nzZr73ve9xzDHHdLnf8ccfz9lnn83SpUuZMGECl19+eZftJEmSJEkaCAzL1L9GTYbn/FkxddiwApbcCYvvKp7HsewBeOT6nUM4ArROh4mHwPjZxTTuwOK1dRrU1Pb/55AkSRoGTjzxRN7//vdzwQUXAPDRj36Uk046iZaWFl75yleyadMmAI488kjq6uqYMmUKkydPZvLkyVx00UWcdNJJjBs3jjPPPJNbbrmlmh9FkiRJkqRuReXwKIPd3Llzc968edUuQ71h+1ZY8QgsfxCWPVS+PggrH4Ut63e2q22AsbOK4GziIcWQjhMPK3ujjahe/ZIkSc/C/fffz+GHH17tMvpdV587Im7LzLlVKklDnPeQkiRJ0tCyr/eQ9izTwFRbD5MOK6ZKmbD+6SJIWzEfVj6yc/7hayt6owWMnQHjDy5ex0wvpo4eaQ7rKEmSJEmSJEmSMCzTYBNRDOU4ajLMPGHXbdu3Fj3Plj1QPA9t2f1FkLboVti8ete2o/aH8QfBhINh3EEwdiaMmwVjZkDjyP76NJIkSZIkSZIkqcoMyzR01NbDxEOL6Yg37Lpt81pY/XgRpi1/uOyN9jDc8+NnBmktE4uhHTsCtLEzy+UZMHIy1NT00weSJEnDWWYSEdUuo98MpeHhJUmSJEmDi2GZhoem0TB5TjF1tmkVrHwMVi2AVR2vC2DhTXDPDyHbd7atbSxCs7Ezd06tB0DLBBgxvpiax0JNbX98KkmSNEQ1NTWxYsUKxo8fPywCs8xkxYoVNDU1VbsUSZIkSdIwZFgmNY+FqWNh6rHP3LZtC6x5YmeAVjktvAna1nZxwCiOOf6gopfb6GnQOrV4Vtr42UXPtWHwRy9JkrTvpk2bxqJFi1i2bFm1S+k3TU1NTJs2rdplSJIkSZKGIcMyaXfqGorQa/xBz9yWWfRKW7MINq4op5XF64alxXCPD11bzFdqbC3Cs5YJ0DIJRk4qnpU25oBimMeRE4v19X6zWpKk4aq+vp5Zs2ZVuwxJkiRJkoYFwzJpX0XAiHHFtDvbtxaB2spHYPl8WDEf1i2GDcvgydtg/dOwdeMz92tsLQO1iWWAVjmVQVvHfPNYe6tJkiRJkiRJkrQPDMukvlZbD+NmFdPslz1zeyZsWA5rFsL6ZUVPtPVPF+s2LIP1S4uQ7fE/FL3WyGceo6ZuZ3A2Yjw0tXaaxhTrdwRtE322miRJkiRJkiRJGJZJ1RdR9BwbObHntu3by2Eel5XT8iJMq1zeuBzWLobNa4pp26Zu3rcGmsftGqC1TiumplZoHA2Noyqmcrmu0V5skiRJkiRJkqQhw7BMGkxqaotnnI2ctOf7bGsrQrOOIK0jVOvoubZhWRHALb4DHvgpbN/SQw31uwZoTaN39l5rHltOlfMVU+NoqKl5FidAkiRJkiRJkqTeZVgmDXV1jXsesLW3F8FZ21poW1fxuq6b5XWweS2seQKW3A2bVsGW9d0fP2qKYK0jPGtqhYYWqG+BhhFQP6Jcrnwd0fX2xpHQMApq/TEmSZIkSZIkSdp3/pVZ0k41NXs+JGR3tm2BzauL4GzH1Hm5nDavgXVLYMsG2LoRtmyErRv27v3qR3QaLrJiyMiGkV2vd3hJSZIkSZIkSVKpT8OyiPgQcBawEbgL+FBmLqrY3gpcAswBtpTbrym3nQZ8HKgHbgHOycyNfVmvpF5Q17D3Q0VWyoStm8rwrFOItuO1nLrr7bZqwa7r27f1/L41dV2Hap0Dtx3LI4t2Hcsdw1E2jHKoSUmSJEmSJEkaRPosLIuIQ4HjgSMysy0iPgZ8BDinotlFwM2Z+daImAbcEBHHAaOAzwInZuayiPg0cCFwQV/VK2mAiCiGXGwYAS0Tnv3xMovntnUXrHVet2V9ObzkmuJ5bisfhbZy3R71eosiQKutL3q9NbUWQVrj6OIz1TVDfTnVNUF9U9GurmnnuoaRxWcfOQlaJha93iRJkoaxnr5MGREHApcCM4Aa4NLM/Ew1apUkSZI0+PRZWJaZDwKnAUREMzANuLdTs9cB55btF0XEVcCpwATg25m5rGx3EXAbhmWS9lZEGUg1PbvhJQHat5dh2vqKcK3i2W2b15Th23rYvqXoFbd5TTnc5OKyx9wm2LYJtm4uXrO95/etay7q7/a1U9DWOLJ4bWgpeszV1BavUVvO11bM1xVhXG1j8VrXWByntmHncsc2h6mUJElVEBEz6PnLlBcBV2TmJRExCrgtIq7JzHlVKFmSJEnSINPnzyyLiK8AfwVcC3y9Yv14YFVmbqloPh84AJgE3NixMjNXR0RzRNRk7vqX5Yg4i2KoR6ZPn95nn0OSqKkte4q19s7xMmH71iJU27a5DNI2F2HbhmWwYSmsXwZta3aGa1s379p248pdlzuGpyR7p8ZKtQ0VQVpTMeTmLiHbHoZutQ0VAV5NpzCvrlOgV9cp3KtoW1tXHKu2oejJV9tYMd+w830M+SRJGuzeTM9fptwAjImIOmAkEMBj/VqlJEmSpEGrz8OyzHx/RHwUeA/wZcqeZHT9l9zYg22dj38JxXPPmDt3bh/8dViS+khEETjVNfTucTN3Pu+tfVvRIy63F6875sv17duKYSq3t8G2LUXgtr183dZWsa1jvmNbR9uKbR3DXT7jOBVt+110CtTqoaa+CNpqOpbrKtbXl4FcxbaOqXJbl8sdbfdwuaa24j3q92HZZ+NJkoaNGfT8ZcpPAFdSDP0/CjgzM1d0dTC/cClJkiSps758ZtkUYH1mrsvMdRHxr8DtHdszc2VEjI2IhoreZbOBO4BN5XzHscYCGzv3KpMkdSGiGIqxcWS1K9lVZhGgbd9Shnbtu4Z23QZ6nZY75rdv23m87Vu7mO94bdt1Xfu28nVrF8vbymBv68662rfufL/K5fbtO/fP7VU4ofHMQG/HVLuz115U9tSrqeh1V9kzr+6Z6zt6B9aPKIb7rG0s3q+usQjrIna+T1S+X82u0zPa1HSzT23Xx6zsfVjfXNRmb0FJGm725MuU7wA+D/wPxSMAvhYRT2bm9c84mF+4lCRJktRJX/Ysez1wWkS8AdgKnAk8GBEtwNYyILsaeB9wcURMo3he2aeAVuDaiPhGOdTG+cAVfVirJKmvRewcknGoaW8vArOO8Kxj2uPl7RWh3NZnt9y+bWc9lb0Ks31nu47gcOumihCxMmTcWvQU3LqxSkHg7kQZ4pXP6+sIAaNTQLgjmKvdNYCr6RTCdbdul+27Oc4zjlFbEQiWE/HMEDGi02t3++xu3y6Wu3yvjjY9vVcPx96lfcXxOtfa5fEMOCU9KwvZzZcpy3vMUzPz42WT+RFxGcUzsp8RlkmSJElSZ30WlpUPVj4A+D0wFriXYijGi4EFwOcoQrBLI+I+oA04OzNXAasi4mPAdRFRC9wMnNNXtUqS9KzU1AA1Rc+roabjuXqde+9l+84ArmM+txc9CHes296pTXaxrmJbt/u1lwHepmc+v6/zsToCwh3HqzzW9iJIbN8OuaXT9sr9O813Dh07jrPL+5V1ajf2JGAr2+x2exftq3LMcijUvTlmVyFol/vsTQ3dtX+W56JlAsx+ab9cGdIe+CFdfJmy44uYwGZgSkS8hCIcGwucBvyyWgVLkiRJGlz69JllmXkhcGGn1WdXbF8NvKmbfb8PfL/PipMkST2rrYfa1mpXMThk7gzXyIoQsX3ntmfMt3fRtrJNV9sq99ndvl0sd/leHW16eq8yVKSibVd17NjeTZsul/elfU9tKj7X3tTwjP9+z+aY7ZD0cK724Zj9YdrxhmUaMDJzQTdfpvwisCAzPxcRb6UYhnFSudsPgEurUrAkSZKkQadPwzJJkqRhI4IdQzJKfWlPw7VnE0bWNlTzE0rP0M2XKSu/iPkb4Lj+rEmSJEnS0GFYJkmSJA0mO4ZTBDCclSRJkiTp2aqpdgGSJEmSJEmSJElStRiWSZIkSZIkSZIkadgyLJMkSZIkSZIkSdKwZVgmSZIkSZIkSZKkYcuwTJIkSZIkSZIkScOWYZkkSZIkSZIkSZKGLcMySZIkSZIkSZIkDVuGZZIkSZIkSZIkSRq2DMskSZIkSZIkSZI0bBmWSZIkSZIkSZIkadgyLJMkSZIkSZIkSdKwZVgmSZIkSZIkSZKkYcuwTJIkSZIkSZIkScOWYZkkSZIkSZIkSZKGLcMySZIkSZIkSZIkDVuGZZIkSZIkSZIkSRq2DMskSZIkSZIkSZI0bBmWSZIkSZIkSZIkadgyLJMkSZIkSZIkSdKw1adhWUScERF3RsT8iPhtRMzutP2nEXF3xbQ6It5ZbrshIu6r2PZXfVmrJEmSJEmSJEmShp+6vjpwRMwC/hE4OjOXR8Q55fKbO9pk5usq2o8C7gF+Ua6anpmz+qo+SZIkSZIkSZIkqS97lrUD78jM5eXyAmDbbtqfB1yZmU9FxDigPSKuioi7IuLSiBjdh7VKkiRJkiRJkiRpGOqzsCwzH8/MXwFExPHAl4Gvd9U2IiYA76ToeQYwGmgGPgE8F1gLfLabfc+KiHkRMW/ZsmW9+yEkSZIkSZIkSZI0pPXpM8sAIuIC4FvA2zLzt900OwP478xcBZCZC4CDMvOOzEzg34BXdLVjZl6SmXMzc+7EiRN7/wNIkiRJkiRJkiRpyOqzZ5YBRMSXgenA8zNz/W6avhH4VMV+tRQ9yzaVq2qALX1VpyRJkiRJkiRJkoanPutZFhHHAq8B3lIZlEVES0Q0VCxPBOYAN1Tsfjhwc0RMKpffDfysr2qVJEmSJEmSJEnS8NSXPcuOBsYCt0dEx7pHgcXAAuBz5bo5wFWZubWjUWbeExGfAX4eEc3ALcC5fVirJEmSJEmSJEmShqE+C8sy8z+A/9iDdtcB13Wx/jLgsl4vTJIkSZIkSZIkSSr12TCMkiRJkiRJkiRJ0kBnWCZJkiRJkiRJkqRhy7BMkiRJkiRJkiRJw5ZhmSRJkiRJkiRJkoYtwzJJkiRJkiRJkiQNW4ZlkiRJkiRJkiRJGrYMyyRJkiRJkiRJkjRsGZZJkiRJkiRJkiRp2DIskyRJkiRJkiRJ0rBlWCZJkiRJkiRJkqRhy7BMkiRJkiRJkiRJw5ZhmSRJkiRpQIuI0yLizoi4LyIui4gRnbbXRsTnI+LuiLg9Iv6+SqVKkiRJGoQMyyRJkiRJA1ZEzAA+C7wsM48AFgEXdmr2fiCBo4C5wLKIGNevhUqSJEkatAzLJEmSJEkD2ZuBb2fmsnL5IuAtndr8BXAT8MvydUVmruy/EiVJkiQNZnXVLkCSJEmSpN2YAdzYsZCZqyOiOSJqMrO9XH0AcDrweqAVuCki7szM+zsfLCLOAs4CmD59ep8XL0mSJGngs2eZJEmSJGkgyy7WRaflFuBbmbkpM5cAVwEv7fJgmZdk5tzMnDtx4sReLlWSJEnSYGRYJkmSJEkayBYCszsWImIssLGiVxnA48D2iuUaug7ZJEmSJOkZDMskSZIkSQPZD4F3RERHN7DzgSsioiUiGsp1lwN/ExE1ZZj2WuBXVahVkiRJ0iBkWCZJkiRJGrAycwHwMeC6iLgP2B/4NHAxcF7Z7CJgMXA3xfPNPpGZD/Z/tZIkSZIGo7pqFyBJkiRJ0u5k5veB73dafXbF9i3Auf1alCRJkqQhw55lkiRJkiRJkiRJGrYMyyRJkiRJkiRJkjRs9WlYFhFnRMSdETE/In4bEbM7bX95RCyJiLvL6f8ior7cdlq5730RcVlEjOjLWiVJkiRJkiRJkjT89FlYFhGzgH8EXpqZs4EryuVKxwCfzMw55XRCZm6NiBnAZ4GXZeYRwCLgwr6qVZIkSZIkSZIkScNTX/YsawfekZnLy+UFwLZObaYBp0TEbRFxU0S8tlz/ZuDbmbmsXL4IeEsf1ipJkiRJkiRJkqRhqK6vDpyZjwOPA0TE8cCXgb/u1GwksBg4A9gf+H3ZdgZwY8WxVkdEc0TUZGZ75QEi4izgLIDp06f30aeRJEmSJEmSJEnSUNRnYVmHiLgAeDvwtsy8tdPmczNzUzn/WERcD5wEZFeH6ur4mXkJcAnA3Llzu9pPkiRJkiRJkiRJ6tJuh2GMiFfvZtvbezp4RHwZOB54fhdBGcCoLurZAiwEZlccZyywsXOvMkmSJEmSJEmSJOnZ6OmZZZ/rmImIn3Xa9re72zEijgVeA7wlM9dXrG+JiIZy8asR8cFy/TSKXmU3Aj8E3hERE8t25wNX9FCrJEmSJEmSJEmStFd6GoaxcujDybvZ1pWjgbHA7RE7mj5K8YyyBRRB3HuAb0TEu4DNwF9n5tMAEfEx4LqIqAVuBs7p4f0kSZIkSZIkSZKkvdJTWJbdzHe1vOvGzP8A/qOHNiuAP+tm2/eB7/dQnyRJkiRJkiRJkrTPegrLJkTE64B2YEzFM8wagQl9WpkkSZIkSZIkSZLUx3oKyz7Bzp5fNwBv7rRNkiRJkiRJkiRJGrR2G5Zl5qXApf1UiyRJkiRJkiRJktSvana3MSLGR8Q3ImJCufz1iFgVEfdHxHP7p0RJkiRJkiRJkiSpb+w2LAO+BTyUmcsj4iTgCGAa8C7gq31dnCRJkiRJkiRJktSXenpm2ZGZeWo5/yrgvzJzA/B/ETG5b0uTJEmSJEmSJEmS+lZPPcsiIhrL+ZOA35Yr64DNfVmYJEmSJEmSJEmS1Nd66ln2P8API2IRUJ+ZD0bE/sC/Ar/u8+okSZIkSZIkSZKkPtRTz7LzgO8D9wKvLtcdAdwM/G0f1iVJkiRJkiRJkiT1ud32LMvMduA7ndb9CvhVXxYlSZIkSZIkSZIk9YfdhmURcQdQCzQBzeVrlJvvyMyX9ml1g1RmEhE9N5QkSZIkSZIkSVJV9TQM47eAduC3wBmZOSEzx5eTQVkXfnnf07z53/7A/YvXVrsUSZIkSZIkSZIk9WC3YVlmfhk4GvgBcG5E3BcRfxcRE/ujuMFo2/Z2Hl2+gdf9y+/45P/ey9rNW6tdkiRJkiRJkiRJkrrRU88ysvCLzHwT8GJgG/CjiPhGn1c3CL16zhSu++CLOf34A7js9wt46cU3cNUdT5KZ1S5NkiRJkiRJkiRJnfQYlnXSDIwGRgFtvV/O0DBmRAOf/tM5XHXuCUxpbeIDV9zBmd+6lYUrNla7NEmSJEmSJEmSJFXoMSyLiNqIOC0ifgH8CFgCnJyZ7+3z6ga5o6aN4SfnnMDfv/4Ibluwkld86Qa+/ptH2Lq9vdqlSZIkSZIkSZIkiR7Csoj4MHAT8CLgQuBk4HKgPSKa+ry6IaC2JnjHCbP41QdfzIsPmcg//eIBXv8vv+P2hauqXZokSZIkSZIkSdKw11PPslcD9cAbgCuBu4A7yukHfVnYUDOltZl/P2Mul5zxPFZv3MppX/89H/3J3azZuLXapUmSJEmSJEmSJA1bdT1sPxU4A7gjM38HEBGHA38BPLePaxuSXnHkZF40ewJfuPYhLvv9Y1x992LefdKBvONFM2lp7Ok/hyRJkiRJkiRJknpTTz3LvgvMBf4uIv46In5F8dyyNuD/6+vihqqRjXV8/PVH8L/vO5Fjp4/l89c8yEn/fD3/fsMjbNyyrdrlSZIkSZIkSZIkDRs9dWU6IjNnRUQLsBx4d2b+Vz/UNSwcuX8r//GO47jjidV88ZcP8Y8/f4Bv3PgoZ7/4IN7+ghk01ddWu0RJkiRJkiRJkqQhraeeZesAMnMD8PDeBmURcUZE3BkR8yPitxExu9P2iRHxk4i4OyIWRMSHKrbdEBH3ldvujoi/2pv3HkyOPmAM337n8fzoPS/ksMmj+fTV9/PyL97Az+9eTGZWuzxJkiRJkiRJkqQhq6ewLCrmt+7NgSNiFvCPwEszczZwRblc6bMUz0ObAzwHOCsijiy3Tc/MIzJzTjl9a2/efzB63oxx/Ne7ns9//fXzGVFfx3suv523XnIT9zy5ptqlSZIkSZIkSZIkDUk9hWW1ETEvIm4BDouImyumnsKrduAdmbm8XF4AdH4g1/XAPwNk5nqKoR63RcQ4oD0iroqIuyLi0ogYvTcfbDA78eAJXP3+E/nMG5/D/KXref1Xf8ff/fBOlq7bXO3SJEmSJEmSJEmShpSenlk2B5jazbb1u9sxMx8HHgeIiOOBLwN/3anNd8vtDcDFwPLMfDAiZgLNwCeAO8ttnwXe2/l9IuIs4CyA6dOn9/BxBo+62hr+4vkzeN1R+/O16+fzrf97jKvvWsy5L5nNO0+Y5fPMJEmSJEmSJEmSekH09TOxIuIC4O0Uvcxu7WL7bOAHwK+BD2fmtnJ9c2ZuKucPAX6amYfs7r3mzp2b8+bN6+2PMCAsWL6Bz/zsfn5539NMG9vMR15zOK9+zmQiouedJUmSpEEqIm7LzLnVrkND01C+h5QkSZKGo329h+xpGMZnJSK+DBwPPL+boOxA4JfAxzLz/IqgrJaiZ1llnVv6staBbuaEFr7xl3O5/F3PZ2RjHedcfjtv/febuHuRzzOTJEmSJEmSJEnaV30WlkXEscBrgLeUzyPrWN9SDrsI8CXgU5l5dafdDwdujohJ5fK7gZ/1Va2DyQmzJ3D1+0/is2+cwyPLiueZfeCKP/LEyo3VLk2SJEmSJEmSJGnQ6emZZc/G0cBY4PaKoQIfBRYDC4DPAc8FZkfEeRX7nZ+Z10TEZ4CfR0QzcAtwbh/WOqjU1gRve/50XvfcKfz7DY/wzd89xs/vXsLbXzCD975kNuNaGno+iCRJkiRJkiRJkvr+mWX9abiON79kzWa++MuH+MFtT9DSUMfZJx/EO0+YRXNDbbVLkyRJkp4Vn1kmgIg4Dfg4UE/xZcpzMvMZw2tExH7AVcCXMvOKno47XO8hJUmSpKFqQD6zTP1jcmsT//Smo7jm//0Jzz9wHJ+/5kFOueg3XHnrQrZtb692eZIkSZK0zyJiBvBZ4GWZeQSwCLiwi3YBfItiJJPx/VmjJEmSpMHNsGwIOXi/UVx65nF8/29eyOTWJj70o7t5ycU38P1bn2CroZkkSZKkwenNwLczc1m5fBHwli7afRi4FbivvwqTJEmSNDQYlg1Bx88ax0/OeRHf+Mu5jG6u4+9+dBcvubjoaWZoJkmSJGmQmQHM71jIzNVAc0TsuJ+NiBcBrwA+BUTnA0iSJEnS7hiWDVERwcuP2I//fe+JfPPMuYwd0cCHfnQ3p1z0G753y0K2bDM0kyRJkjQodPWg7R2BWESMAP4FODMzt/d0sIg4KyLmRcS8ZcuW9dRckiRJ0jBQV+0C1Lcigpcevh8vOWwSv3lwGV/61UNc8OO7+ep18znnlIN48/MOoKHOzFSSJEnSgLUQmN2xEBFjgY2Z2fENwKOAKcANxWPLGAO0R0RrZn6288Ey8xLgEoC5c+d2FcRJkiRJGmYMy4aJiOCUwyZx8qET+c1Dy/jyrx7moz+5h69dN59zTpnNm+dOo7GuttplSpIkSVJnPwSujYhvlM8tOx+4IiJagK2ZeROwf0fjiPgEsDwzv1adciVJkiQNNnYpGmYiglMOncRPznkR337n8UxubeJj/30Pf/LP1/Mvv36YFevbql2iJEmSJO2QmQuAjwHXRcR9FMHYp4GLgfO62OVpYEm/FShJkiRp0IvMoTPqxNy5c3PevHnVLmNQyUx+N385l/z2UW58eDlN9TX8xfNn8Dd/ciCTRjdVuzxJkiQNcxFxW2bOrXYdGpq8h5QkSZKGln29h3QYxmEuIjjp4ImcdPBE5i9dx9d/8yiX/X4B37npcd469wDO+pMDOWDciGqXKUmSJEmSJEmS1CcchlE7zJ40iovf8lyu/+DJnHbsVK64dSEv/vz1nPvd27njidXVLk+SJEmSJEmSJKnX2bNMzzB9/Aj+8c+O4gMvPYTLfr+Ay29+nKvvWsxxM8fy7pMO5GWH70dNTVS7TEmSJEmSJEmSpGfNnmXq1uTWJj786sP4wwUv5eOvO4KnVm/mrO/cxku/cAP/ddPjbNqyvdolSpIkSZIkSZIkPSuGZerRyMY63nniLG7425P56tuOYXRTHR/773t40ed+zReufZBl69qqXaIkSZIkSZIkSdI+cRhG7bG62hped9T+vHbOFG5dsIpv3Pgo/3L9fP7tt4/yxqOncsYLZ/Ccqa3VLlOSJEmSJEmSJGmPGZZpr0UEx88ax/GzxvHosvV883eP8aPbF3HlvCc4alorpx8/nVOfuz8tjV5ekiRJkiRJkiRpYHMYRj0rB04cyWfeOIebP/IyPnnqkbRtbeeCH9/N8Z/5FRf8+G7uWrSazKx2mZIkSZIkSZIkSV2y6496RWtzPWe+aCZ/+cIZ3L5wFd+9+Ql+8sdFfO+WhRwxZTSnH38AbzhmKqOb6qtdqiRJkiRJkiRJ0g6GZepVEcHzZozjeTPG8fHXH8H/3PEk37vlCS686l4+87P7ee2c/Tn9+AN43oyxRES1y5UkSZIkSZIkScOcYZn6TGtzPWe8cCZvf8EM7n5yDd+75Qn+544n+dHtizh40kjeetwBnHbsNMa2NFS7VEmSJEmSJEmSNEwZlqnPRQRHTRvDUdPG8LHXHs5P73qK793yBJ+++n7++RcP8srnTOb04w7gBQeOp6bG3maSJEmSJEmSJKn/GJapX7U01vHW46bz1uOm88CStVxxyxP85I9P8r93PsX+rU284Zip/NkxUzl4v1HVLlWSJEmSJEmSJA0DNX158Ig4IyLujIj5EfHbiJjdaXtrRFwZEfdFxB0R8cqKbaeV+94XEZdFxIi+rFX977DJo/n7U4/k5o+8lC//+dEcOnkUl/z2UV7+xd/y2q/cyKU3PsrStZurXaYkSZIkSZIkSRrCIjP75sARs4AbgaMzc3lEnAOckplvrmjzDeD+zPxCREwDbgCOA0YB1wInZuayiPg0UJuZF+zuPefOnZvz5s3rk8+j/rF8fRv/e+dT/Pcfn+TORWuoCThh9gTeeMxUXnnkZFoa7QwpSZI0nETEbZk5t9p1aGjyHlKSJEkaWvb1HrIvk4d24B2ZubxcXgBs69TmdcC5AJm5KCKuAk4FJgDfzsxlZbuLgNuA3YZlGvwmjGzkr06YxV+dMItHlq3nqj8+yU/ueJLzvn8nzfX38Ioj9+NPj5nKSbMnUFfbpx0jJUmSJEmSJEnSMNBnYVlmPg48DhARxwNfBv66Y3tEjAdWZeaWit3mAwcAkyh6pXUca3VENEdETWa291XNGlgOmjiS815xKP/fyw/htsdX8ZM/PslP71rMVXc8xYSRDbzuqP35s2OnMmdqKxFR7XIlSZIkSZIkSdIg1Odj2kXEBcDbgbdl5q0Vm7oa/zH2YFvn458FnAUwffr0Z1GpBqqIYO7MccydOY5PvP5IfvPgUv77jif57i0Luez3CzhwYgtvPHoqr3/u/syc0FLtciVJkiRJkiRJ0iDSp2FZRHwZmA48PzPXV27LzJURMTYiGip6l80G7gA2lfMdxxkLbOyqV1lmXgJcAsV4833yQTRgNNTV8IojJ/OKIyezZtNWfn73Yn7yxye5+JcPcfEvH+KIKaN5zZzJvGbOFA6cOLLa5UqSJEmSJEmSpAGuz8KyiDgWeA1wRGZurVjfAmwtA7KrgfcBF0fENIrnlX0KaAWujYhvlM8tOx+4oq9q1eDU2lzPnx8/nT8/fjpPrt7Ez+9ezM/uXsxF1z7ERdc+xGGTR/GaOVN4zZwpzJ5kcCZJkiRJkiRJkp6pL3uWHQ2MBW6veJ7Uo8BiYAHwOYoQ7NKIuA9oA87OzFXAqoj4GHBdRNQCNwPn9GGtGuSmjmnmXScdyLtOOpDFazbx87uX8PN7FvPFXz3EF375EIfsN5LXzJnCa+dM4eD9RlW7XEmSJEmSJEmSNEBE5tAZuXDu3Lk5b968apehAeTptZuLHmf3LOHWBSvJhNmTdgZnh+w3koowV5IkSQNMRNyWmXOrXYeGJu8hJUmSpKFlX+8h+/SZZVK17Te6iXecMIt3nDCLpWs3c829S7j67sV89bqH+cqvH+agiS07hmo8bPIogzNJkiRJkiRJkoYZwzING5NGN3HGC2dyxgtnsmxdG9fcu4Sf3b2Yr10/n3+5bj4HjGvmJYdO4pTDJvGCA8fTVF9b7ZIlSZIkSZIkSVIfMyzTsDRxVCNvf8EM3v6CGaxY38Y19z7NdQ88zZXznuDbf3icpvoaTjhoAqccVoRnU8c0V7tkSZIkSZIkSZLUBwzLNOyNH9nI254/nbc9fzqbt27npkdXcP0DS7nuwaX8+oGlABw2eRQnHzqJlxw2iWOnj6GutqbKVUuSJEmSJEmSpN5gWCZVaKqv5eRDJ3HyoZP4+0weWbahCM4eWMqlNz7Kv93wCKOb6viTQybyksMmceLBE5g0qqnaZUuSJEmSJEmSpH1kWCZ1IyKYPWkksyeN5N1/ciBrN2/l/x5eznUPLOX6B5fx07sWA3DofqM48eAJnHjwBF4wazzNDT7rTJIkSZIkSZKkwcKwTNpDo5vqefWcKbx6zhTa25P7Fq/lxoeX87v5y/jOTY/zzd89RkNdDc+fNY4/OXgic2eO5fApo2mqNzyTJEmSJEmSJGmgMiyT9kFNTfCcqa08Z2or7zn5IDZv3c7Nj63ktw8t44aHlvGZn90PQH1tcOz0sZx08AROmD2BOVNbfd6ZJEmSJEmSJEkDiGGZ1Aua6mt58SETefEhE7kQWLxmE3c+sZrbF67mdw8v56JrH+Kiax9iVFMdLzpoPCfOLsKzWRNaiIhqly9JkiRJkiRJ0rBlWCb1gSmtzUxpbeZVz5kCwIr1bfz+kRX87uHl/G7+cq6592kAJo9u4oUHjeeFB47nhQeN54BxI6pZtiRJkiRJkiRJw45hmdQPxo9s5PXP3Z/XP3d/MpPHV2zkd/OX84dHV/Dbh5bxkz8+CcDUMc28oAzOXnjQeKaOaa5y5ZIkSZIkSZIkDW2GZVI/iwhmTmhh5oQW3v6CGWQmDy9dzx8eWcEfHlnBdQ88zY9uXwTA9HEjdvQ6e+FB49lvdFOVq5ckSZIkSZIkaWgxLJOqLCI4ZL9RHLLfKM580Uza25MHn15XhGePruDn9yzmynlPADBz/AieN2Mcc2eOZe6MsRw0cSQ1NT7zTJIkSZIkSZKkfWVYJg0wNTXB4VNGc/iU0bzzxFlsb0/uX7yWmx5dwc2PreT6B5fu6Hk2ZkQ9c2eMZe7McRw3cxxzprbSUFdT5U8gSZIkSZIkSdLgYVgmDXC1NcFzprbynKmtvOukA8lMFqzYyK0LVnLbglXcumAlv7p/KQCNdTUcfcAYjp81jrkzx3Hs9DGMaqqv8ieQJEmSJEmSJGngMiyTBpmIYNaEFmZNaOEtcw8AYNm6Nm57fCW3PFaEZ1+7fj7tCTUBh04ezbHTx3Ds9LE8b8ZYZowfQYRDN0qSJEmSJEmSBIZl0pAwcVQjr3rOFF71nCkArG/bxh8XruLWx1Zy+8LVXHXHU1x+80IAxrU0cMwBY5gzrZU5U1uZM62VSaOaqlm+JEmSJEmSJElVY1gmDUEjG+s46eCJnHTwRAC2tycPL13HHxeu5vbHV3HHE6u5/sGltGfRfr/RjcyZOoY5U1s5alox5OPEUY1V/ASSJEmSJEmSJPUPwzJpGKitCQ6bPJrDJo/m9OOnA7ChbRv3LV7L3YvWcPeTa7hr0Wp+/cDTZBmgTWlt4qhprRw1bQxHlb3QxoxoqOKnkCRJkiRJkiSp9xmWScNUS2Mdx80cx3Ezx+1Yt75tG/c+2RGeFa/X3Pv0ju0zxo9gztRWnjutGMbxOVNbGdnojxFJkiT1rYg4Dfg4UA/cApyTmRsrtk8ELgFmA6OAr2fmP1WjVkmSJEmDj3/llrTDyMY6nn/geJ5/4Pgd69Zs3Mo9T63hzkWruXvRGv64cDU/vWsxABFw0MSRRQ+0qa0cObWVwyaPYlRTfbU+giRJkoaYiJgBfBY4MTOXRcSngQuBCyqafRa4IzPfGBEjgTsj4qeZeW8VSpYkSZI0yBiWSdqt1hH1nDB7AifMnrBj3fL1bdy9qOh9dtei1fz2oeX8+PYnd2w/YFwzh08ezRH7j+bwKaM5Yspopo1tJiKq8REkSZI0uL0Z+HZmLiuXLwJuY9ew7HrgJwCZuT4ilgPb+rVKSZIkSYNWn4VlEXEW8HfAk5n54i62/xSYUbHqAOC8zPyPiLgBmAhsL7d9ITO/1Ve1Sto7E0Y2csphkzjlsEkAZCZL1m7mvqfWcv/itdy/eB33L17LL+/f+Qy0kY11HLLfSA6dPJrDJo/i0MmjOHS/UYxt8TlokiRJ2q0ZwI0dC5m5OiKaI6ImM9vLdd8FiIgG4GJgeWY+2NXBynvVswCmT5/e17VLkiRJGgT6LCzLzEsi4lLg6W62v65jPiJGAfcAvyhXTc/MWX1Vm6TeFRFMaW1mSmszLz18vx3rN27ZxoNL1nH/4nU8sGQtDyxZx8/uXsz3blm4o82kUY0cOnkUh+y3M0A7eL+RjGiw46skSZIAyC7WPWPIgoiYDfwA+DXwhm4PlnkJxfPNmDt3blfHliRJkjTM9OlfozOzfQ+HXTsPuDIzn4qIcUB7RFwFzKJ4ePN5mbm2D0uV1AdGNNRxzPSxHDN97I51mcnTa9t48Ol1PLRkHQ8sWcdDT6/j8psfZ/PWdqB4Ftr0cSOKAG2/URwyeRSHTR7FrAkt1NfWVOvjSJIkqToWArM7FiJiLLCxo1dZue5A4JfAezPz6v4vUZIkSdJgVvWuGxExAXgncHS5ajTQDHwCuJNiCI3PAu/tZn+H0JAGkYhgcmsTk1ubePEhE3es396eLFy5kQfL8OzBJet48Ol1XPfAUra3F1/4ra8NDpwwkkMmj+LQ/Ubu6I12wNgR1NT4PDRJkqQh6ofAtRHxjfK5ZecDV0REC7A1M7cAXwI+ZVAmSZIkaV9EZt+OOhERyzJz4m62/3/AzMz8QMW65szcVM4fAvw0Mw/p6b3mzp2b8+bN642yJQ0Qbdu288jSDUWA1hGiLVnHk6s37WjTXF/LwfuNZPakkRw0sZhmT2ph+rgWGursiSZJ0mAWEbdl5txq16Hqioi3ABcCtcDNwDnAF4EFmfm5iHgc2MDO514DnJ+Z1+zuuN5DSpIkSUPLvt5DVr1nGfBG4FMdCxFRS9GzrOMv4TXAlirUJWkAaKyr5Yj9R3PE/qN3Wb++bRsPP93RC209Dz29jt/PX8GPb39yR5vammD6uBEcNLGFgyaO5MDy9aCJIxnb0tDfH0WSJEn7KDO/D3y/0+qzK7bP6N+KJEmSJA0l/RqWdRomg4iYCMwBbqhodjjwk4g4ITOXAu8GftafdUoa+EY2PvN5aFCEaI8uW8+jyzbwyLL1xbR0A799eDlbtu14rAVjR9TvCM4OmtTCgRNGctCkkRwwtpk6n4smSZIkSZIkScNGf4RllWNaXAwsAD5XLs8BrsrMrR0NMvOeiPgM8POIaAZuAc7thzolDQEjG+s4atoYjpo2Zpf129uTJ1dt2hmglWHarx94mivn7ey8Wl8bzBzfsksvtIMmFb3SRjfV9/OnkSRJkiRJkiT1tT4PyzLz1RXzZ3fadh1wXRf7XAZc1te1SRo+amuC6eNHMH38CE45bNIu29Zs3Mojy9fzyNKdIdr8pev59f1L2da+87mOE0c1cuCEFmaMH8GM8S1MHzeCGeNHMH3cCFqb64mI/v5YkiRJkiRJkqRnaSA8s0ySqqp1RD3HTh/LsZ2GdNy6vZ2FKzfuHNJx6XoeW76B6x9cxrJ1i3ZpO6qpjgPGjmDa2Gb2H9PMtLHNTB3TzNTydVxLg2GaJEmSJEmSJA1AhmWS1I362podQzG+nP122bahbRsLV25k4cqNPFHxumDFBn43fzkbt2zfpX1zfS37j2li6tgRTO0iTNtvdBO1NYZpkiRJkiRJktTfDMskaR+0NNZx+JTRHD5l9DO2ZSarN27lydWbWLRqE0+u3sSTqzbx5OqNPLl6E3cvWs2qjVt32aeuJpjc2rQjQJtWvu4/ppxam2luqO2vjydJkiRJkiRJw4ZhmST1sohgbEsDY1saeM7U1i7bbNyyjSdXbWLRjiBt5+vv56/g6XWbydx1n3EtDew/pokprUVvtI75/cc0Mbm1mf1GNVJXW9MPn1CSJEmSJEmShg7DMkmqghENdRy83ygO3m9Ul9u3bm9nyZrNPLV6E0+t2cRTq4v5J1dvYuGKjdz0yArWtW3bZZ+agEmjmpjc2lQEaKM7grQiVJvS2sQkAzVJkiRJkiRJ2oVhmSQNQPW1NRwwbgQHjBvRbZu1m7eyePVmFq/ZxOI1m1m8unxds5kHlqzj+geWsWnrrs9O6wjUpoxpYkpFiDaltXlHyDZxpIGaJEmSJEmSpOHDsEySBqnRTfWMnlzPoZO77p2WmazdtI3FazexePVmnlqzqQzXNrNk7SYeWLyO6x5Yyuat7bvsV1sTTBrVWIRnZYi2I1grQzYDNUmSJEmSJElDhWGZJA1REUHriHpaR9Rz2OTRXbbJTNZs2loEaGuKQK0Y/rEI1O5fvJZfP/D0Xgdqk0Y1MmFkI031tf3xUSVJkiRJkiRpnxmWSdIwFhGMGdHAmBENHD6l50CtY8jHPQnUAEY11TFxZBGcTRzVyISRDUwc1ch+o4tnqU0e3cR+rU2MaqwjIvr640qSJEmSJEnSMxiWSZJ2a18CtaVr21i+vo3l67ewbH0by9a1cf+StSxf18bazduesf+IhtoiOBvdxKTRjUwc2cik0Y1MGlX0Ups4qpgf3WyoJkmSJEmSJKl3GZZJkp61PQnUOmzeup2n1xa905as3VzOtxWvazfzx4WrWbpuc5c91RrqaiqCtCJAK4K0xjJkK8K28S0NPlNNkiRJkiRJ0h4xLJMk9aum+lpmjG9hxviWbttkJuvatrFsXRtL17axdN1mlq0reqgtLV8fW76Bmx9byeqNW5+xfwSMaa5n/MgiOJswspFxLQ2MH9nA+JGNTGgpXse1NDBhZAOjm+qpqbHHmiRJkiRJkjQcGZZJkgaciGB0Uz2jm+o5aOLI3bZt27ad5eu3sHTt5h1B2tJ1bazc0MbKDVtYvn4LDyxZy4oNW7oM1gDqaoJxLQ1leNZYhGotHa9FsFY539JQ63CQkiRJkiRJ0hBhWCZJGtQa62qZOqaZqWOae2y7dXs7qzZuYcX6ctrQ1ul1CyvWt/HEExtZsX4L69ue+Xy14j1rdoRq41qKYG3CyIZyuVg/oaWRcWXA1lRf29sfW5IkSZIkSVIvMSyTJA0b9bU1TBrVxKRRTXvUfvPW7azcUARry8tAbWX5urwM2VZu2MLDT69n+fo22rY98zlrACMaastQrQzXRjYycVQjE0YWIdvEkY1MKJfHNDskpCRJkiRJktSfDMskSepGU30t+49pZv896LWWmWzYsp2VFcHaivVtrNiwhZXltGLDFpaua+P+xetYvr6Nbe35jONEQGtzPa3N9Yxprmd0cz1jRjQwpmPdiHJdub5jXWtzvT3YJEmSJEmSpH1gWCZJUi+ICEY21jGysY7p40f02L69PVmzaSvL17exbH0by9dvYfm6NlZv3MLqTVtZvXErazZtZfWmrSxatYnVG7ewZtNWusjXdmisq9kRnI1pbqB1xM7QbUeo1hGwVawb1VRPrb3ZJEmSJEmSNEwZlkmSVAU1NcHYlgbGtjRw8H6j9mif9vZk/ZZtrOkI0jZuZfWmLTvm13Za98TKjdyzqWi7ccv2bo8bAaMa64oebGXA1loRpo1pLgK2HeFbxbqm+hoiDNokSZIkSZI0eBmWSZI0SNTUBKOb6hndVM8Be7nvlm3trNm0lTWbtuzstVbRe21N2XNtdRmuPblq04757bvpztZQV7NLuFY5je5yXV3xGZrraWmoNWiTJEmSJElS1RmWSZI0DDTU1TBxVCMTRzXu1X6Zyfq2bbuEa7sEbWXItmbTVtZu3srSdZt5eOk61mzcyrq2beRuho2sCRjVtDNAG9W0M0jbsdxcz+jytWN7a3M9LY11jGiopbHOnm2SJEmSJEl6dgzLJElStyKCUU3Fc82mjd27fbe3J+s3b9sRpnUEams3bWXd5m075tdu3rZj3cKVG3esW9+2rcf3qK0JRjTUMrIMzzpCtGK5jpbG2vK1jpaGWkY21RU93Jp27f02uqmOutqafTxLkiRJkiRJGsz6LCyLiLOAvwOezMwXd7H95cB3gGXlqrXAyZm5NSJOAz4O1AO3AOdk5sa+qlWSJPW+2poonnM2on6f9u8I29ZurgzaiuWNbdvYsGU7G7dsY0Pbdja0bWPjlu1s2LKNDW3beGr15mLblp3betLSULsjQNsZou0cPrJyuXVEMT+yqY4R9bWMaKylodZebpIkSZIkSYNRn4VlmXlJRFwKPN1Nk2OAT2bm1ytXRsQM4LPAiZm5LCI+DVwIXNBXtUqSpIGnMmzb22e0ddbenmzcun3X8K2yx9umbTsCuY5tT6zcyLqyZ9ye9HKrqwmaG2ppaahjRGMtIxrKXm3la7Fcy4iyl1tzx7bGnYFbS0e7HW0M4SRJkiRJkvpanw7DmJntu/njzjRgckS8C9gK/ENmXg28Gfh2Znb0OLsIuA3DMkmStI9qaoKRjXWMbKxjcmvTXu+/bXv7juCsI1Bbs2krG9uK3mwbK3q5bdqy67oVG7awcOXGcn2xbuv23TzMrZO6cqjJEWUI19JQV4ZyO4O2juEnR1QGc92EcPaEkyRJkiRJ2lU1n1k2ElgMnAHsD/w+Io4HZgA3djTKzNUR0RwRNZnZ3vkg5XCPZwFMnz69XwqXJEnDS11tDWNbGhjb0tArx9uyrb0iVCuHkGzbvmN+R/C2decwkh3DSm4sl5ev38KGjhCuHJZye/vehXDPpidcc0MtTfW1NJfzzfW1NNXX0FRfS2OdQZwkSZIkSRo8qhmWnZuZm8r5xyLieuAkoKu/8nT715bMvAS4BGDu3Ll7/hciSZKkKmmoq6Ghrmafn+fWnS3b2neEapvKwK0jaOsphOsI3Zav38LGLRvLAK9os20vQjiACMrwbGeI1txQS1PdriFbU31N8bojbHtm6NZYX+zXWF+z87Xc1lRfS1NdDXW1Nb16HiVJkiRJ0vBSzbBsFLCpYrkG2AIsBGZ3rIyIscDGrnqVSZIkaacihGtgzIjePW5HCFcZuG3eWoRuO1+L3nId6zrWb9rSvsvy+rZtLFvXRtu2ne03bd3Olm37/qteXU0UwVl9DY11O8O2XUK1HWFbxfZd2naEcGWgV7fzGB0BXkNdcfyGuhpqa+w5J0mSJEnSUNGvYVlEtABbM3ML8NWIuDkzL46IaRS9yj4AzAOujYhvlM8tOx+4oj/rlCRJ0k59FcJV2t6etG0resK1bSsCtrat7WzeVgRtbdvaadu6c9vmrRWv2zq17di2rdi+ZtPWHW3bKtruzbPjOqutCRpqa3b0EmwsXxtqK+Y7wrWKdjva1NfQWLvruoa62l32bajb2aYjpNu1ffletTXUGN5JkiRJkrTP+iMsm1cxfzGwAPgc8B7gGxHxLmAz8NeZ+TRARHwMuC4iaoGbgXP6oU5JkiRVSW1NlM9K67/vcm3b3r4zfCtfN23ZTtu2nWHcpopgrm1bO1s6pu3bd8x3rG/bXrG97Dm3ZtPWXdZt2d5O29adbXtLfW3n8K52l1CtI1hr7BS2FfPF8JYdIV5zfS1TWpt51XMm91p9kiRJkiQNZH3+14jMfHXF/NkV8yuAP+tmn+8D3+/r2iRJkjR81dUWzztraazOyOSZydbtyZbtHaFbpwBu+65hXFchXVtFCLdz/fZd1nW0Wbd5Gys6tS3Cu+1s2d6+S0+7uTPGGpZJkiRJkoaNaj6zTJIkSRq2IoKGuqChrgYaq10NtLcXwd3mrdvZ3r7vQ1RKkiRJkjTYGJZJkiRJoqYmaKqppam+ttqlSJIkSZLUr2qqXYAkSZIkSZIkSZJULYZlkiRJkiRJkiRJGrYMyyRJkiRJkiRJkjRsGZZJkiRJkiRJkiRp2DIskyRJkiRJkiRJ0rBlWCZJkiRJGtAi4rSIuDMi7ouIyyJiRKftrRFxZbn9joh4ZbVqlSRJkjT4GJZJkiRJkgasiJgBfBZ4WWYeASwCLuzU7CLg5nL764B/jYhx/VupJEmSpMHKsEySJEmSNJC9Gfh2Zi4rly8C3tKpzeuArwJk5iLgKuDUfqtQkiRJ0qBmWCZJkiRJGshmAPM7FjJzNdAcETUAETEeWJWZWyr2mQ8c0J9FSpIkSRq86qpdQG+67bbblkfE49WuA5gALK92EcOA57l/eJ77h+e5f3ie+57nuH94nvuH57l/9HSeZ/RXIRqwsot1sRfbd90QcRZwVrnYFhH3PIvapN3x3xH1Ja8v9SWvL/U1rzH1pUP3ZachFZZl5sRq1wAQEfMyc2616xjqPM/9w/PcPzzP/cPz3Pc8x/3D89w/PM/9w/OsPbAQmN2xEBFjgY2Z2Q6QmSsjYmxENFT0LpsN3NHVwTLzEuCS8lhef+ozXl/qS15f6kteX+prXmPqSxExb1/2cxhGSZIkSdJA9kPgHRHR8eXI84ErIqIlIhrKdVcD7wOIiGkUzyv7n36vVJIkSdKgNKR6lkmSJEmShpbMXBARHwOui4ha4GbgHOCLwALgcxQB2qURcR/QBpydmauqVLIkSZKkQcawrG9cUu0ChgnPc//wPPcPz3P/8Dz3Pc9x//A89w/Pc//wPKtHmfl94PudVp9dsX018KZ9OLTXn/qS15f6kteX+pLXl/qa15j60j5dX5HZ1bOQJUmSJEmSJEmSpKHPZ5ZJkiRJkiRJkiRp2DIs60URcVpE3BkR90XEZRExoto1DRURcUN5Xu8up7+KiNaIuLJcf0dEvLLadQ5GEXFWRMyPiBsq1nV7br3O91435/jlEbGk4pr+v4ioL7d5jvdBRJxRnrf5EfHbiJjttdz7ujnPXs+9LCI+FBGPlOfz8oiY5vXc+7o5z17PfSAizomI28t5r2X1q56uK+8r9GzswfU1MSJ+Uv6bsiAiPlStWjU47em/jRGxX0TcFBF/3t81avDag59htRHx+fJn2O0R8fdVKlWD0B5cXwdGxHXlPdljEfHRatWqwSe6+Htrp+17/Tu+wzD2koiYAVwLnJiZyyLi00BtZl5Q5dKGhIh4LDNndVr3DeD+zPxCREwDbgCOy8yVVSlyEIuIGuDpzJxYLnd5boFReJ3vky7O8d8B6zLz653a+bNkH0TELOBG4OjMXB4R5wCnAKvxWu41uznPt+L13Gsi4lDgs8DbMrMtIj4G7A/U4/Xca3Zznhfg9dyrIuLFwAeAYzJzlr9nqD/tyf+73ldoX+3F9bUoMz8ZESOBO4FTM/Pe6lStwWRPf/+IiACuBtYCN2bm1/q/Wg02e/gz7P8DpgAfAgJ4D/A9/41UT/bw+vox8IvMvCQiRgG3UdyfzatO1RpsOv+9tdO2vf4d37Csl0TE+UBDZn62XB4D3JaZB1W1sCEgIsZR/CH2HmAWcAtwHvAgMCMzt5TtvgDclZmXVanUQS0illUEOYvp4twCE/A632edzvFXgMnAQcBW4B8y82p/luyb8pewgzPzV+Xya4AzgJPxWu41uznPy/B67hMR0Qx8EbgX+Ahez32i03k+GK/nXhMREyhukl9D8XvaJH/PUH/ak/93u7smva9QT/bw+nob8JPM3FQu3wz8ZWY+WIWSNcjs6e8fEXEB0ARsB1YYlmlP7OHPsHkUXzA7BxgNfCEzr6hCuRpk9vD6+g5wN/AFYCLwW+AFmbmi/yvWYFX599ZO6/f6d3yHYew9M4D5HQuZuRpoLtNNPTujgWbgE8BzKb4p9U/Aqo6LvTQfOKD/yxtaImI83Z9br/PeMxJYDLwIOB24NCI8x/soMx+vCHCOB74M/Btey72qm/P8dbye+0QZqi+luGn4Pl7PfaLTefZ67kXlt9y/BXw0M5eUq/w9Q/1tt9dVD9ek1JMef25l5nczc1NENETEvwDLDcq0F3q8xiLiRcArgE9R9PyR9tSe/O51AMXvxK8HTgU+FxGH92eRGrT25Pr6BPBmYDmwCPikQZl6w77+ju+NZ+/pqouev6T0gsxcAByUmXdk0RXy34CXdtHU8907dncte533nnMz8wOZ2ZaZjwHXAyfhOX5Wym9Ufgt4G8W3k57RpHz1PD8Llec5M3+L13OfyMz3UwwLeDPw91008XruBZ3O85fxeu5N51IMe/HzinX+nqH+1tN15XWnZ2OPrp+ImE3x70wb8Ia+LkpDym6vsSie//MvwJmZub3fqtJQsSc/w1qAb2XmpvLLT1fR9d/kpM725Pp6B/B5ipE9DgXOiIhT+rguDQ/79Dt+XR8UMlwtBGZ3LETEWGBjZrZXr6ShISJqKXqWbSpX1QBbgPER0VCREM8G7uj/CoeWzFwZEWO7Obeb8DrvLaPYeU3DzuvanyX7KCK+DEwHnp+Z68t1Xsu9rKvzjNdzr4qIKcD6zFyXmesi4l+B24FRXs+9Z3fnGa/n3vJS4JiIeHO5PA64CTjAa1n9aLf/7/bwu6/Ukx7/bYiIA4FfAu/NzKv7v0QNcj1dY0dRPE/qhqJDN2OA9oho7Rj6TNqNPfn99nGK4T071ND1H6GlznZ7fUVEC8UzPD9eNpkfEZcBr6P4wqK0z/b1d3x7lvWeHwLviIiO8THPBxzDt3ccDtwcEZPK5XcDP6N4eO37AKJ4SN+pwP9UpcKhp7tz63Xee74aER+EHef4JOBGPMf7JCKOpXgezlsqAhzwWu5VuznPXs+96/XADyOiqfzCyJkUz+n0eu5d3Z1nr+dekplvzMyZmTkrM2cBKzPzEOByvJbVf7q8riKiJSIaynXeV2hf7cn19SXgUwZl2ke7vcYy86bM3L/i39ovAR83KNMe2pOfYZcDfxMRNWXY8VrgV1WoVYNPT9fXZmBKRLwkCuOA04CHqlSvhoBn+zu+Pct6SWYuiIiPAdeVf3C5meLhl3qWMvOeiPgM8POIaAZuoRjWp57iOSL3UQxncXZmrqpiqYPdvIr58+n63K7yOn9WKs/xe4BvRMS7KH5B+OvMfBrAc7xPjgbGAreX36gEeJTij99ey73naLo+z+/E67nXZOYlUTwj6/cU5/teip8Z6/B67jW7Oc8b8XruK3eUr/6eoX6zm/u0LwILgM/R/TUp7dYeXl/PBWZHxHkVu56fmdf0d70afPbwGqv0NLCsX4vUoLWH19dF5fLdFD3KPuFzF7Unerq+MvNzEfFWimEYOzpI/AC4tCoFazCr/HvrxTyL3/GjeASUJEmSJEmSJEmSNPw4DKMkSZIkSZIkSZKGLcMySZIkSZIkSZIkDVuGZZIkSZIkSZIkSRq2DMskSZIkSZIkSZI0bBmWSZIkSZIkSZIkadgyLJMkdSsi6iLigYi4NyLujogFEfHziBjTy+/z3xXzH4yI03vz+H0tIn5V7RokSZIkSZIk7ZvIzGrXIEkaoCJiBvDjzHxeuRzAN4E7MvMrvfg+yzJzYm8dr78N9volSZIkSZKk4cyeZZKknrRXzDcBE4D5ABHxwoi4tex59oOOHmcRcVhE3FCuvyYippXr31O2/2NE/F3Zc+13wNiy59qfRcR5EfHKiJgQEZdHxFci4v6I+L+IaCmP85Ky/U0R8d6IuLRz0RHxnIj4dvn+v4yI2t3Ue23Ffq+MiP+vnP9FRHw0Iu4q36/jcxwbEbeV00cBv3kiSZIkSZIkDVKGZZKknhxZhlt/BJ4E1mfmzyKiAfg+8M7MPBJ4EPh4uc93gM+X638AdPRC+yTwcuB44BCgITNPBFZl5pzM/DEwBZgMtABvBX6ZmYcDjwGvLt/3O8BbMvMFwBzgoC7qHg/8OfA3mflyoHY39R5Tsd9kYP9y/mXA2sw8Cvif8ngA3wY+VPa4WwuM29OTKUmSJEmSJGlgMSyTJPXk3sw8JjOPAaYB6yLifcBhwIrMvLtsdwVwckSMBg4Eflau/wFwYsX8j4D3AJ/KzI09vPfCzPzfcv5m4EjgUGBpZt5f8b7d+W1mLijnu6y3h/ffAlxSzt9EERy2UgRqvy7XX97DMSRJkiRJkiQNYIZlkqQ9VoZbVwAvoRh6cHvF5tpyOdl16Maach2ZeS5FULYd+G1ETO7hLVdUzHccs6bT8et2s//qyvK7qbfjmB3GV8xvyMy2Tu+fQFS0qem0LEmSJEmSJGkQMSyTJPWkoWMmImqA1wKLKIYxnBgRR5Sb/xy4LjPXUQyZ+Ipy/VuA30RES0T8AViSmV8D7gWOK9tsj4iGiGjag3oeAPaveN+37OHn6LLecn5xRBxe9hp72+4OkplrKT7/y8pVZ+IzyyRJkiRJkqRBa3ffxpck6UmgOSLuo+iFNZoirPrLzNwSEW8Fvh0RI4B7gHeX+/0l8O8RcTHwBPBXmbkhIr4BXF+Gbr8HrinbX0oRsH0BWAwsATYAD1fUsgTYnpltEfEXwPcjYgNwLTCpi9pXlO8NQA/1XgT8hiJQ+zZQX66/o+J4y4Cnyvkzgf+IiH8CfszO0E2SJEmSJEnSIBOZfhlekjS4RMQbKZ5HtiIiPgw0ZebfV7ksSZIkSZIkSYOQPcskSYPRgcDnIqKdovfZX1S5HkmSJEmSJEmDlD3LJEmSJEmSJEmSNGzVVLsASZIkSZIkSZIkqVoMyyRJkiRJkiRJkjRsGZZJ0hAUEZdHxKaIWNdpOrrate2piPheRPxTteuQJEmSpIHCe72hLSJOi4jLK5ZHR8TXI+KhiFgdEb+NiJO72feHEXFjF+tPjYjvlvOzI2JZRJzRqc3YiFhUzh8TEb+PCP9uLGlY8YeeJA1NbwHmZuaoTtMdz+agEbEoIsb2Tok9ehRY2E/v1a2IuDkinlPtOiRJkiQJ7/V6zUC714uIBuCfgR+XyyOAm4DRwMuAg4CrgGsi4oWd9p0N/AkwMSJO7HToscDEcn4qsAn4fESMq2gzotwGcFe5zy6BmiQNdXXVLkCS1CfqgNV9cNypFL9Er+qDY+8iMz/a1++xhyYBrdUuQpIkSZLwXq83DbR7vb8BlmXmj8rl9wLtwNszM8t1F0fEJOA5wB8q9n0H8L/AknL+d7t5nzXA7cCnyvfYRWZuj4gLgK9ExBWZ2bbPn0iSBhF7lknSMBQRZ0TEfRGxvBwCY0y5flQ5dMPKiFgTEddGxMRyqIa15e4PRcQfo3BLRMypOO77O4bTiIj3lcNFXFUe75XlPudHxPyIeDoi/jUimrqp8QsR8Z5y/gcRcV5E/CYilkTEBeVQEgvLY/+/st0xEfF/EXFmRNxbtv18RNSV21vKmhZHxBPltvpu6n1DRDwNzAB+WX7TsiUinhMRf4iItRGxIiK+FhFRHuPmiPiziLi+HCLjZxHRWm6rj4h/Lt93WUT8S8V+B5XnemV5bv+kl/+TS5IkSRoGvNcb1Pd6pwL/XrH8JuBbFUEZAJn5ocz8RsX5rAH+Evgv4LvAW6LoldadeuDjwDsq/xt3eo//BhqAo3dzHEkaUgzLJGnoui8iVlVMHwWIiBOAfwH+AtgfWA78W7nP8ymGxJhJ8S27lcB7M3N+Zo4u2xySmceUv7BPpBgSosNYoGMoh3HAX1P8cj8uM68B3gacA7wCOBCYDPx9N/VXHmsCcCbFN+TeRPENuPcBz6P4Jtw/ljdCo4E5wAnAC4DjgdcCZ5XH+QLFNyaPAI4FXgR8qJt6r8rM/YDHgZdn5rTM3FDW/s2yptnAa4CTy2NMorjpeB9wGDAFOLvc9mGKYTHmAs8tz/W7IqIWuBr4LbAf8AHgJ1F8W1CSJEmSOvNeb2je6x0O3FexPBuY303bSi8DArghM++l+O982m7aR2Y+DnylnLpzL8VnlaRhwbBMkoauIzJzbMX0mXL9mcB/ZeYfM3ML8EngT8tvnv0aeIzil/mngD+luMnaV3eU30jrcCbwr5n5aHkz8qly3Z64MjMXZObvKG7svpmZy4CfAE3sHIO9CfhgZq7LzIXAF4E/q/i23Qczc1W577+X67qrtyv/TnED1nETcgC7nqNLMvOezFwC/Bw4pFz/l8DXM/PpzHyK4sbrKoobqYnA5zNza2b+FrgReOMenhdJkvQsRMRZZU+IG3poNzki7oyIuyumzRFxcH/VKkkl7/WG2L1eRIyiCPse6K64sgfeqrJn4EcqNv0VcHlmtpfLl1OEj93p+HvwZ4BDIuLNQHbR7n6KAE+ShgXDMkkafqYBf9XxLUSKb6ptBEZSfCPuncC7KH5R/9xeHrvzvyt3dfHen6h47xuAbeU37nryeMX8RuAJgMzcVK6L8nVpZq6raLuQ4lt8Eyhurh6r2HZLWVN39Xblx8B44FUU3y7sPBZ85TE2sPOcTKN4ODNl3cszc2m5fjSwpOK8vBzYk3MiSZKepcy8hOIPnkf00G5JZj43M+dk5hzg3cADmflwf9QpSXvAe72dBtu9XkdYFRXr5gMHVRz3C5k5FvhoeRyiGGbzT4FzyiEglwEfA06JiJm7+7BlqPl3FNdCXRdNguKZaZI0LBiWSdLw8xRwYcW3EMcA08tf5v8U+HRmzsvMzfQ85EI7uw7NcVyn7Z2/nfYUcGbFe7cCh2fm9n39MF2YVH4rr8Msiput5UBbudzhgHJbd/XuojzuS4D3Z+YjmbkVOLRTs+6O8SQV30qM4vkAh1Cck/srvxlKcbP3790cR5Ik9bKKb+MDEBH7R8Q1EfFwRNwYEQd1sds/Ah/pYr0kVYv3ejsNqnu9zFxfHqeyJ9dPgHd2PP+sQmWP5j+nCNUOp3i+2NEUX/64kV171nUpMy8HlnTT9jCK3mWSNCwYlknS8PNt4L0RcUT5IOK/BG4vv/G3HnhxRIyIiNMpbqgaK/ZdDzwvIp5f/sL+EMVDgfePiHfS88N/vw1cEBHTI6I5Ij5EMYZ7b2oDvhgRIyNiFnAexVAk7RQPPP7niGiNiMkUw5Jc1sPx1gNHR8Rx5bG3AS8rj/FPFGPxN+7uAKXvAJ8sb5z2o/jW4qkU30BsiIh3R0RdeVP1R+DFe/m5JUlS7/kK8NXMPBj4B+CrlRsj4hVATWb+rBrFSVI3vNcb3Pd6D7JrL+evAA3A5RExLSIaIuJtFL0DO/wV8O3MfLJyonj22ju6CNq68n6K3tKdHYFhmaRhxLBMkoamhRTDQjxDZt5I8WDiH1KMB/8+4K3lN/4+BLya4ht4f04xVMeWit0/SfEtuH8HWimGbDicYjiKk4B/BlaVbVcCKzq9/X8B3wOuA54u36u7b7utKo9BeZzVFduWAWsrlp+iGK4DYDEwD7id4tt0He8J8P+ApRS/8N9GMazGRbupF+CfgL+luAGaCbwH+FfgboqhPr5BcVNFeezKulZWfIZ/pBgK5I7y/R8CvpiZ2ygeTH1a+bmupxgL/7ouapEkSf3jZOBTEfFH4PNAS6ft5wJf6ueaJAm81xvK93r/A/xNR8BVDpN4PEWo95vyXLyF4r/NiohoBMZQPKOssx8BWymGk1xF0fsOYE35WXbIzNsozuOO56VFxBuAzezZ8JWSNCRE5m57IUuSNGhExIuBS8tvgUuSJO2xiFiWmRPL+RXAYZm5rFxu7nh2TkS0AIuAA8phsyRJfWw43OuV4ddDwPmZ+YMq1lFLEZL9Y2b+V7XqkKT+Zs8ySZIkSZJ29RuKnhcdQy5+v2Lbq4CbDMokSb0pM9uAC4DXV7mUIyl6oH23ynVIUr+qq3YBkiT1orUUQ35IkiTtrXkV8+8HvhkRZ1MMefb+im1HsWt4Jknqe8PiXi8zv0uVQ6rMvAt4UTVrkKRqcBhGSZIkSZIkSZIkDVsOwyhJkiRJkiRJkqRha0gNwzhhwoScOXNmtcuQJEmS1Etuu+225Zk5sdp1aGgazveQGzZsoKWlpdplDDiel+55brrmeeme56Z7npuueV6657npmuele8P53OzrPeSQCstmzpzJvHnzem4oSZIkaVCIiMerXYOGruF8D/mb3/yGk08+udplDDiel+55brrmeeme56Z7npuueV6657npmuele8P53OzrPaTDMEqSJEmSJEmSJGnYMiyTJEmSJEmSJEnSsGVYJkmSJEmSJEmSpGHLsEySJEmSJEmSJEnDlmGZJEmSJEmSJEmShi3DMkmSJEmSJEmSJA1bhmWSJEmSJEmSJEkatgzLJEmSJEmSJEmSNGwZlkmSJEmSJEmSJGnYMiyTJEmSJEmSJEnSsFVX7QKGopkfvrraJfS5BZ97bbVLkCRJkqQhoRr3kB+cs4139OP7eg8pSZKkgcyeZZIkSZIkSZIkSRq2DMskSZIkSZIkSZI0bBmWSZIkSZIkSZIkadgyLJMkSZIkSZIkSdKwZVgmSZIkSZIkSZKkYcuwTJIkSZIkSZIkScOWYZkkSZIkSZIkSZKGLcMySZIkSZIkSZIkDVv9FpZFxAcj4tGIeCQifhMRh0VEa0RcGRH3RcQdEfHKivanRcSd5bbLImJEf9UqSZIkSRp4IuKciLi9i/Xd3ltKkiRJUk/q9qZxRMwAjgIOAxJ4ALgrMxf2sN9RwN8AR2Xm+oj4G+AiYDFwc2a+NSKmATdExHHAKOCzwImZuSwiPg1cCFywdx9PkiRJkjRQ7Os9Zbnvi4GXAWO72HwRXdxbZubK3qtekiRJ0lC1Rz3LImJmRFwO/Bx4A9BOcWPzp8DPIuI7ETFzN4fYADQCIyOiDhgDPAi8DvgqQGYuAq4CTgXeDHw7M5eV+18EvGVvPpgkSZIkaWB4tveUETEB+CJwDtDSRZPu7i0lSZIkqUd72rPsa8D3gLdnZnbeGBFvB75CNzcjmflIRHwRWASsB+YDpwGvzswtFU3nAwcAk4AbK/ZfHRHNEVGTme2d3vss4CyA6dOn7+HHkSRJkiT1o32+p4yIAL4FfDQzl5TLldvHA6u6ubeUJEmSpB5FF/cp3TeOOD4zb6lYrgfemJnf72G/2cD5wOeBJ4E3UgRc+2XmERXtzgXGAROB31UeNyIWA1M7h2WV5s6dm/Pmzdvjz9NXZn746mqX0OcWfO611S5BkiRJw0BE3JaZc6tdh3rHvtxTRsR7gemZ+Xfl8rLMnFixfRzF/eMz7i0z8x+6ON6OL1zut99+z7viiit64ZM9O3c/uabf33O/Znh6U/+935yprf33Zs/C+vXrGTlyZLXLGJA8N13zvHTPc9M9z03XPC/d89x0zfPSveF8bk455ZR9uofcq2eWAd8AnluxvB24GNhtWAacCfwmMx8pl78XER8HJkdEQ8U3AGcDdwCbynkAImIssHF3QZkkSZIkacDbl3vKlwLHRMSby+VxEfEQcHxmrs7MlRExtpt7y2fIzEuAS6D4wuXJJ5+8zx+mt7yjCl+4/OCcbVx8997+SWDfLfiLk/vtvZ6N3/zmNwyEa2Ig8tx0zfPSPc9N9zw3XfO8dM9z0zXPS/c8N3tvj34zjohvAnOA2RFxc8WmUcAv9+AQq4HTIuJaYBXwEqAZ+DHwPuDi8iHMpwKfAlqBayPiG+Vzy84Hqv91P0mSJEnSXns295SZ+cZOx1qWmYdEREtFQHY1Xd9bSpIkSVKP9vRrZOcDo4GfAW+uWL8hM1fswf7/AhwE3AVsA54C3go8CFwaEfcBbcDZmbkKWBURHwOui4ha4GaKBzlLkiRJkgafZ3tPWemO8vViYAHwufL4Xd1bSpIkSVKP9jQsW5uZqyLihMxcvbdvUn7T7xy6Drze1M0+36fn4R0lSZIkSQPfs7qnrJSZLy9fz65Yt5pu7i0lSZIkqSc1e9juh+XrtyPi0Yh4pHx9NCKu6qviJEmSJElDgveUkiRJkgasPe1Zdlb5+uYu9tnWe+VIkiRJkoYg7yklSZIkDVh7GpadERH7dbNtCfDFXqpHkiRJkjT0eE8pSZIkacDa07DsXmB5N9ue7qVaJEmSJElDk/eUkiRJkgasPQrLMvOajvmImA08H9gK/CEzn+ij2iRJkiRJQ4D3lJIkSZIGspq9aRwR/w/4FfAS4LXAjRHxzj6oS5IkSZI0xHhPKUmSJGkg2tNhGDucDTwnM9cDRMQY4PfAf/RyXZIkSZKkocd7SkmSJEkDzl71LAMWA5srljcAK3qvHEmSJEnSEOY9pSRJkqQBZ297ll0B/DIifkQRtL0J+HVEvBpYnpm39naBkiRJkqQhw3tKSZIkSQPO3oZl04DHgGPL5fnA9HJaBHhjI0mSJEnqjveUkiRJkgacvQrLMvPCjvmIqAMOzcx7e70qSZIkSdKQ4z2lJEmSpIFor55ZFhFnRMQlETEeeAC4PCI+2TelSZIkSZKGEu8pJUmSJA1EexWWAe8DzgfeCPw78DzgNb1dlCRJkiRpSPKeUpIkSdKAs7dhWS2wnuJm5uflur197pkkSZIkaXjynlKSJEnSgLO3Ydn/Ak8CdZl5D3ANcGWvVyVJkiRJGoq8p5QkSZI04OzVN/gy8+8j4uLMXFeuemdmLuyDuiRJkiRJQ4z3lJIkSZIGor3tWUbFTQ3e1EiSJEmS9ob3lJIkSZIGmr0OyyRJkiRJkiRJkqShwrBMkiRJkiRJkiRJw9Zeh2URMSci3hsR74mIw/dwn9dExN0V00MRsTAiWiPiyoi4LyLuiIhXVuxzWkTcWW67LCJG7G2tkiRJkqSBZR/vKT8UEY+U95OXR8S0TttfHhFLKu45/y8i6vvmE0iSJEkaavYqLIuIc4ErgRnAwcCPI+LdPe2XmT/LzDkdE/BN4L+Bi4CbM/MI4HXAv0bEuIiYAXwWeFm5bRFw4d7UKkmSJEkaWPblnjIiDgWOB44o7yfvBz7SqdkxwCcr7jtPyMytvf8JJEmSJA1FdXvZ/m+AuZm5ESAiPg7cBHxjTw8QEWOAc4G5wJ3lPJm5KCKuAk4FJgDfzsxl5W4XAbcBF+xlvZIkSZKkgWOv7ykz80HgtLJ9MzANuLdTs2nA5Ih4F7AV+IfMvLr3y5ckSZI0FO1tWFYL/P/s3XeYnFX58PHvHWqohiotgCIqgiBiBSQovhYQC0Wq0kSK2EDFAjZE9AcoiqiACKJIU0QEFRQDKILSm6IoQUF6Dz3kfv84Z8iwzCY7m92dst/PdeXKzjxlzpw5zzNzzn3KE02PHwOyzXN8EjgOeBq4PzOfbNp2E7ASsAxwUePJzHwgIiZGxITMnNl8sojYHdgdYPLkyW0mRZIkSZI0hoZdp4yIbwE7A+cC3x2weRHgdmBHYHng4oh4dWb+t8V5nqlDLrvsskydOrXNtzDy9l1rxpi/5rITx/Z1uyGfh2L69Ok9k9axZt60Zr4MzrwZnHnTmvkyOPOmNfNlcOZN+9oNlp0CXBARp1KCXVsDpw314IiYF9gNWIPWFaKo/89u27Nk5tHA0QDrrbdeu4E7SZIkSdLYGXadMjM/HBGfBfYEjqDOUlLtnZmP1b9vjog/ABsCJ7U4z7PqkFOmTBnmWxk5O+0/9oPg9l1rBodd226TwPBN237KmL3W3Jg6dSrdUCa6kXnTmvkyOPNmcOZNa+bL4Myb1syXwZk37Wvrl3FmfikiLgPeRAloHZKZv27jFG8A/pGZ9wBExKSImL9pdNlqwFWU3oWrNQ6KiEnAowNHlUmSJEmSesdw6pQRsRwwPTMfzsyHI+Io4IoBuy1KqUc2TACeRJIkSZKGYEI7O0fE3pl5Tmbum5n7AX+IiMPaOMW7gbOaHp8N7FPPvSJlvbJfAqcDO0XE0nW//YCT20mrJEmSJKm7DLNO+Q7g9IhYMCLmAd4P3BgRC0fE/HWfIyNi3/oaK1JGlV3U+nSSJEmS9GxDCpZFxLwRsRCwZ62gTKwLK0+iTJsxVC8Bzmh6vB/wuoi4gRJE2yMz78/MacDngPPrtuWBg9p4HUmSJElSl5ibOmWdNvEvwMWUda7fBuwFHAZ8vO62J7B+RPyNUrfcNTPvHJ13I0mSJKnfDHUaxp8ArwJWAK5n1vphDwNfGeqLZeabBzx+ANhykH1PBU4d6rklSZIkSV1rruqUmXkAcMCAp/do2n4v8J4RSakkSZKkcWdIwbLMfC9ARPwsM7cY3SRJkiRJkvqJdUpJkiRJ3aytNcus1EiSJEmShss6pSRJkqRu1FawTJIkSZIkSZIkSeonBsskSZIkSZIkSZI0bhkskyRJkiRJkiRJ0rg171B2iogfAKsMsvnmzNxtxFIkSZIkSeor1iklSZIkdbMhBcuAzwOLAZsCrwB+Wo/dHvjL6CRNkiRJktQnrFNKkiRJ6lpDCpZl5q0AEXF0Zm7QeD4izgQuAo4eneRJkiRJknqddUpJkiRJ3azdNcuWioglmx4vASw52M6SJEmSJDWxTilJkiSp6wx1GsaGg4FrI+KPlEDba4H9RjxVkiRJkqR+ZJ1SkiRJUtdpK1iWmT+KiPOAVwEJ7J2Zd45KyiRJkiRJfcU6pSRJkqRu1NY0jBGxBHAgsBVwHnBAfU6SJEmSpNmyTilJkiSpG7W7ZtnxwBXA2pn5OHAl8P2RTpQkSZIkqS8dj3VKSZIkSV2m3TXLXpKZm0fEngCZ+YOI+MgopEuSJEmS1H+sU6otq+x/9pi+3r5rzWCnMXzNaYdsOmavJUmSpMG1O7Ls7ohYmzK3PBGxGfDgiKdKkiRJktSPrFNKkiRJ6jrtjizbFfgh8NKIuB+4CXjfiKdKkiRJktSPrFNKkiRJ6jptBcsy8+/A6yJi0fr44VFJlSRJkiSp71inlCRJktSN2pqGMSLWiYhPA48DP4qIOyJi69FJmiRJkiSpn1inlCRJktSN2l2z7NvAr4HNgduAVwB7j3SiJEmSJEl9yTqlJEmSpK7TbrBs0cy8Cngz8NPMvB2YNJQDI2LdiLg4Iq6JiPMj4sURsXhEnBIRN0TEVRHxlqb9t4iIq+u24yNioTbTKkmSJEnqLsOqU0bEpyLiXxFxbUT8JCJWHLB90LqlJEmSJM1Ju8GyayPiVOANwKURcRBwzZwOiogFgOOBHTPz5cA+wFrAocClmbkGsBlwVEQsERErAwcDm9RttwIHtJlWSZIkSVJ3abtOGREvBl4NrJGZawF/Az4zYLeWdcsRT70kSZKkvtRusGwP4ETgdZk5A7gB2GkIx70F+AOwX0RcAXy6Pt4MOBIgM28FzqRMx7EVcEJm3l2PPxRwHntJkiRJ6m1t1ykz88bM3CIzn4iIicCKlIBZs8HqlpIkSZI0R5GZQ9854l3AcgOeviMzz5jDcR+hjAzbOjPPj4jPAesDK9eef4399gKWBJYBLsrMU5u2/Q9YMTNnDjj37sDuAJMnT37lLbfcMuT3M1pW2f/sTidh1E07ZNNOJ0GSJEnjQERcnpnrdTodGhnDrVPWY78F7AycC7y3BtuIiCUp9cfn1C0z88stzvNMHXLZZZd95cknnzzMdzNyrr3twTF/zWUnwp2Pjd3rrbXC4sM6bqzzplfypROmT5/OIoss0ulkdB3zZXDmzeDMm9bMl8GZN62ZL4Mbz3mz8cYbD6sOOW+b+y9d/0EJar0D+P4QjpsP+Hdmnl8ffw/4PPDPAftF/b9VBC9aPEdmHg0cDbDeeusNPfInSZIkSRprw61TkpkfjojPAnsCRwB7Nza12L1l/bGe51l1yClTpgwp4aNppw50uNx3rRkcdm27TQLDN237KcM6bqzzplfypROmTp1KN1wv3cZ8GZx5MzjzpjXzZXDmTWvmy+DMm/a19QswM49pfhwRXwd+AHxtDofeAjzdfChwHzApIubPzCfr86sBVwGP1b8brzMJeHTgqDJJkiRJUu8YTp0yIpYDpmfmw5n5cEQcBVzRdM77ImKwuqUkSZIkzVG7a5YN9Ajw8iHs92tg9YhYtz5+P2XqjLOBfQAiYkXKnPK/BE4HdoqIRo/D/YDOz40hSZIkSRpJQ6lTvgM4PSIWjIh5KPXJGyNi4YiYv+4zWN1SkiRJkuaorZFlEfFj4EX14TzAssB35nRcZk6PiM2B70fEQpTpFz8APAUcGxE3AE8Ae2Tm/cD9dV2z82tl6FJgr3bSKkmSJEnqLsOpU2bm0RGxEnAxMAm4njIV42HANOAQSgfLVnVLSZIkSZqjdifi/giwcNPj/zUWVZ6TzPwT8KoWm7YcZP9TgVPbTJ8kSZIkqXsNq06ZmQcABwx4eo+m7Q8wSN1SkiRJkuak3TXL7gXuHaW0SJIkSZL6mHVKSZIkSd1obtcskyRJkiRJkiRJknqWwTJJkiRJkiRJkiSNW20HyyJihfr/OhGxd0QsPvLJkiRJkiT1I+uUkiRJkrpNW8GyiDgM+GhEvBj4KfBS4FujkTBJkiRJUn+xTilJkiSpG83b5v5vzMxXRMQngEMy84SIuHI0EiZJkiRJ6jvWKSVJkiR1nXanYXw0ItYAtgR+FxErAk+MfLIkSZIkSX3IOqUkSZKkrtPuyLLDgROBn2bmbRFxAfClkU+WJEmSJKkPWaeUJEmS1HXaCpZl5s8i4lJg6frUtpn5v5FPliRJkiSp31inlCRJktSN2pqGMSJ2AM4BTo2IBYFzIuJ9o5IySZIkSVJfsU4pSZIkqRu1u2bZAcD6wMOZ+TiwIbD3iKdKkiRJktSPrFNKkiRJ6jrtBssAHgWy/v04sOjIJUeSJEmS1OesU0qSJEnqKu0Gy06mLMb8vIj4GPAH4LQRT5UkSZIkqR9Zp5QkSZLUdeZtZ+fM/HxEbArcDqwIHJKZvxqVlEmSJEmS+op1SkmSJEndqK1gWXUV8FDjQUSskZk3jFiKJEmSJEn97CqsU0qSJEnqIm0FyyLi58CLgL8za475/wL7jnC6JEmSJEl9xjqlJEmSpG7U7siyNYGXZObM0UiMJEmSJKmvWaeUJEmS1HUmtLn/2cB2ETHfaCRGkiRJktTXrFNKkiRJ6jrtBstuBn4I/C8i7qr/zh3KgRFxQUTcEBHX1n87R8TiEXFKff6qiHhL0/5bRMTVddvxEbFQm2mVJEmSJHWXYdcpJUmSJGm0tDsN44eBlTPzf8N4rcmZuWrzExFxDHBpZr43IlYELoiIVwGLAgcDG2Tm3RFxEHAA8OlhvK4kSZIkqTsMq04ZETsC+wELA/8DdsnMm5q2vxk4Ebi7PvUQMCUznxqRVEuSJEnqa+2OLLuEEshqS0QsAcyMiDMj4pqIODYiFgM2A44EyMxbgTOBzYGtgBMys1HRORTYut3XlSRJkiR1lbbrlBGxKvBV4E2ZuRpwcn3c7BXAFzNzrfpvfQNlkiRJkoaq3WDZBKAxjeKl9d+Ph3DcYsBE4PPA2pRefl8D7s/MJ5v2uwlYCVi5/g1AZj4ATIyIdtMrSZIkSeoew6lTzgR2ysx76uNpwIwB+6wIbBwRl0fEJRGx6cgmW5IkSVI/i8wc+s4RS1KmvWj2SGbeO4RjJ2bmY/Xv1YFfATMyc42mffYGlgCWBv6Ymac2bbsdWCEzZw447+7A7gCTJ09+5S233DLk9zNaVtn/7E4nYdRNO8S6pyRJkkZfRFyemet1Oh0aGXNTp6zHvxr4CbBrZl7Y9PxxwMPAJ4HlgYuBV2fmf1uc45k65LLLLvvKk08+eThvZURde9uDY/6ay06EOx8bu9dba4XFh3XcWOdNr+RLJ0yfPp1FFlmk08noOubL4MybwZk3rZkvgzNvWjNfBjee82bjjTceVh1ySGuWRcR7M/MUYF3g+QM23wGcN4fj56GMLGv85JwAPAksGRHzN40uWw24qu63WtPxk4BHBwbKADLzaOBogPXWW2/okT9JkiRJ0piY2zplPcengR2A7TLzrwM2793onAncHBF/ADYEThp4noF1yClTprTzVkbFTh3ocLnvWjM47Np2lzEfvmnbTxnWcWOdN72SL50wdepUuuF66Tbmy+DMm8GZN62ZL4Mzb1ozXwZn3rRvqL8AV6j/rwksN2Dbksy5YvNS4IyIWD8z7wI+AJxDGUW2D3BYRKxIWa/sS8DiwLkRcUxdt2w/yrz0kiRJkqTeM1d1yog4ApgMvCYzp7fYZVFmdc6EWR00JUmSJGmOhhQsy8zD6583Z+Y3Gs9HxALAXkM4/rqI+Arw64iYCPwF2BuYDzg2Im4AngD2yMz7gfsj4nPA+XVU2qVDeR1JkiRJUveZmzplRKwLvB1YIzOfanp+YeCpOlPJkRFxaWY2OmJuCHxkpN+HJEmSpP7U7twCXwR+0fT4aeDjwDda7t0kM48Hjm+xactB9j8VOLXVNkmSJElSTxpOnXIdYBJwRUQ0nvs3cDswDTgE2BM4JiJ2Ax6nrGl25wimW5IkSVIfG+qaZWcA6wPPi4hGhSOAGcAPRiltkiRJkqQ+MDd1ysw8DjhuDvvcC7xnBJIqSZIkaRwa6jSM7waIiIsyc8PRTZIkSZIkqZ9Yp5QkSZLUzSa0uf+U0UiEJEmSJGlcmNLpBEiSJEnSQG0FyzLz6dFKiCRJkiSpv1mnlCRJktSN2h1ZJkmSJEmSJEmSJPUNg2WSJEmSJEmSJEkat9oKlkXEz+r/241OciRJkiRJ/co6pSRJkqRuNO9QdoqInwLLA2tGxGeBnSPiauCWzJw+mgmUJEmSJPU265SSJEmSutmQRpZl5raZuRFwN3AxsCDwAeAnEfGDUUyfJEmSJKnHWaeUJEmS1M3aGVm2LLAksB7wGPAd4D+Z+cToJU/9ZpX9z+50EkbVtEM27XQSJEmSpK5jnVKSJElSNxtSsCwztwWIiH8A1wALAfsCkyPitsz8wOglUZIkSZLUy6xTSpIkSepmQwqWNdkmM6+IiGMy8wujkSBJkiRJUt+yTilJkiSp6wxpzbKIWAsgM6+o/3+hxT5rjmjKJEmSJEl9wTqlJEmSpG42pGAZsH9E/Dki3hgRCzaejIiJEbFJRFwCHDg6SZQkSZIk9TjrlJIkSZK61lDXLNs+Il4P7A/8MCKeAqIefzWwX2b+cfSSKUmSJEnqVdYpJUmSJHWzIa9ZlpkXA5sDRMTCQGTm9NFKmCRJkiSpf1inlCRJktSthhwsa5aZj4x0QiRJkiRJ44N1SkmSJEndZKhrlkmSJEmSJEmSJEl9x2CZJEmSJEmSJEmSxq22g2UR8eqIeEf9+1VtHrtXRFxR/148Ik6JiBsi4qqIeEvTfltExNV12/ERsVC76ZQkSZIkdZ+5qVNKkiRJ0mhoK1gWEZ8ADgYOj4j5gK9GxH5DPHYjYBNgUn3qUODSzFwD2Aw4KiKWiIiV62tsUrfdChzQTjolSZIkSd1nuHXKiNixdqi8KSIujIjVBmwftDOmJEmSJM1JuyPLPgBsCjycmU8BbwO2ndNBEbEU8A1gL2Dh+vRmwJEAmXkrcCawObAVcEJm3l33OxTYus10SpIkSZK6T9t1yohYFfgq8KbMXA04uT5u1rIz5kgnXpIkSVJ/ajdY9nT9P+v/E4EFZ3dARATwQ+CzmXlHfWpJ4P7MfLJp15uAlYCV69/lhTIfACZGhOurSZIkSVJva7tOCcwEdsrMe+rjacCMAfsM1hlTkiRJkuZo3jb3PwL4LbBMRBwBvB34vzkcszfwt8z8ddNz2WK/GMK2526I2B3YHWDy5MlzSIokSZIkqYParlNm5i3ALVDWO6vn2LWxfQ6dMZ+juQ657LLLMnXq1OG+lxGz71oDY3+jb9mJY/u6w83nsc6bXsmXTpg+fXpPpXesmC+DM28GZ960Zr4MzrxpzXwZnHnTvraCZZn5vYj4I7BxfWqLzLxmDoe9CXhFRGxVHy8BXAKsFBHzN1VoVgOuAh6rfwMQEZOARzNz5iBpOho4GmC99dZrFWiTJEmSJHWBYdYpAYiITwM7ANtl5l+bT9tq99mk4Vl1yClTpgzl5UfVTvufPeavue9aMzjs2nb7zw7ftO2nDOu4sc6bXsmXTpg6dSrdcL10G/NlcObN4Myb1syXwZk3rZkvgzNv2tfW1IYRsQCwbmZ+OzO/DbwxIibO7pjMfHdmrpKZq2bmqsB9mbk68BNgn3reFSlTZPwSOB3YKSKWrqfYjzInvSRJkiSphw2nTlmPOwJ4NfCaAYEyMvM+YFJEzN/09GrAf0Yw6ZIkSZL6WLvrgJ0OLNf0eH7gjDbPcVX9fz/gdRFxA3AWsEdm3p+Z04DPAefXbcsDB7X5GpIkSZKk7tN2nTIi1qVM17h1Zk5ven7hpgDZ2bTujClJkiRJc9Tu3AKrZ+Y7Gg8y8+sRsevsDhgoM99c/38A2HKQfU4FTm0zbZIkSZKk7jacOuU6wCTgiohnZlf8N3A7MA04hNIZ89ja4fIJamfMkU26JEmSpH7VbrDsnoh4Y2aeDxARGwN3j3yyJEmSJEl9qO06ZWYeBxw3h30eYJDOmJIkSZI0J+0Gyz4AnBgRK9XH/wHeN7JJkiRJkiT1KeuUkiRJkrpOW8GyzLwBeGVELApEZj40OsmSJEmSJPUb65SSJEmSulFbwbKIWB7YBnhZfQzwv8w8YOSTJkmSJEnqJ9YpJUmSJHWjdqdh/DVwFnAGMLM+55plkiRJkqShsE4pSZIkqeu0GywjMz83GgmRJEmSJPU/65SSJEmSus2ENvc/JSL2ioi2g2ySJEmSpHHPOqUkSZKkrtNusGxD4AjggYi4q/777SikS5IkSZLUf6xTSpIkSeo6bfXmy8y3jVZCJEmSJEn9zTqlJEmSpG7UVrAsIhYD3gksUp+aByAzjxzhdEmSJEmS+ox1SkmSJEndqN1pGM+kTJtxALAisCew8kgnSpIkSZLUl6xTSpIkSeo67QbLVsjM3YF7M/OzwBuA14x8siRJkiRJfcg6pSRJkqSu026w7OmIeDFwY0S8HngAWGrEUyVJkiRJ6kfWKSVJkiR1nbbWLAM+AmwLfA44A5gfOGakEyVJkiRJ6kvWKSVJkiR1nXaDZRdl5rn175dGxERguRFOkyRJkiSpP1mnlCRJktR12p2GceqAxwmc22I/SZIkSZIGmjrgsXVKSZIkSR03pJFlEbE3sDKwakR8rWnT8yhzzEuSJEmS1JJ1SkmSJEndbKjTMP4NeBh4Ari+6fnpwL4jnShJkiRJUl+xTilJkiSpaw0pWJaZ5wNExCOZ+bPRTZIkSZIkqZ9Yp5QkSZLUzdpds2zYPf4i4lMR8a+IuDYifhIRK0bE4hFxSkTcEBFXRcRbmvbfIiKurtuOj4iFhvvakiRJkqSuMKw6ZUTsHhE3RcQFg2x/c0TcUeub10bEnyJivrlLqiRJkqTxot1g2W0R8Zp2XyQiXgy8GlgjM9eiTMHxGeBQ4NLMXAPYDDgqIpaIiJWBg4FN6rZbgQPafV1JkiRJUlcZVp0yM48GVgfWGGSXVwBfzMy16r/1M/OpuUmoJEmSpPGj3WAZwJ8j4h8RcWn9d+KcDsjMGzNzi8x8IiImAitSAmabAUfWfW4FzgQ2B7YCTsjMu+spDgW2HkZaJUmSJEndpe06JUBmzpzN5hWBjSPi8oi4JCI2HZmkSpIkSRoPhrRmWZM9eO60GY8M9eCI+BawM3AucCqwZ2Y+2bTLTcBKwDLARY0nM/OBiJgYERMGVpAiYndgd4DJkye38VYkSZIkSWNsruqUs7EIcDuwI7A8cHFEvDoz/zsC55YkSZLU5yIz2zsgYi1gI+BpYGpm/q3N4xcF9gRWBTaq0yw2tu0NLAEsDfwxM09t2nY7sMLsehOut956edlll7WTnFGxyv5ndzoJo27aIcPrqNnveTPcfJEkSVJrEXF5Zq7X6XRo5MxNnTIi7s7MpVs8PzEzH2t6fBLwq8w8qcW+z3S4XHbZZV958sknD+NdjKxrb3twzF9z2Ylw52Nz3m+krLXC4sM6bqzzplfypROmT5/OIoss0ulkdB3zZXDmzeDMm9bMl8GZN62ZL4Mbz3mz8cYbD6sO2dbIshrM2hs4G5gH+HBEHJ6Zx8zhuOWA6Zn5cGY+HBFHAVcAi0bE/E2jy1YDrgIeq383jp8EPDqHaTckSZIkSV1suHXKIViUUo9smAA82WrHuv7Z0VA6XE6ZMmUuX3ru7dSBToX7rjWDw65td7KZ4Zu2/ZRhHTfWedMr+dIJU6dOpRuul25jvgzOvBmcedOa+TI486Y182Vw5k372l2z7IPAepn5icz8OPAq4CNDOO4dwOkRsWBEzAO8H7iRUkHaByAiVqSsV/ZL4HRgp4ho9BjcD+h8dz9JkiRJ0twYbp3yOSJi4YiYvz48MiL2rc+vCGxI09T+kiRJkjQ77XaXmgd4ounxY8Ac53HMzKMjYiXgYmAScD1lKsaHgWMj4oZ63j0y837g/oj4HHB+Da5dCuzVZlolSZIkSd1lWHXKJs3z7h8GTAMOodQvj4mI3YDHgV0z8865S6okSZKk8aLdYNkpwAURcSplfvmtgVNnf0iRmQcAB7TYtOUg+5861HNLkiRJknrCsOuUAJn5tqa/92j6+17gPSOYTkmSJEnjSFvBssz8UkRcBryJ0vvvkMz89aikTJIkSZLUV6xTSpIkSepGw1m19h/AEpTFkq8b2eRIkiRJkvqcdUpJkiRJXWVCOztHxMeA3wEbA5sCF0XELqORMEmSJElSf7FOKUmSJKkbtTuy7IPAmpk5HSAingdcDBw3wumSJEmSJPUf65TSCFll/7PH9PX2XWsGO43ha047ZNMxey1JkqS2RpYBtwOPNz1+BLh35JIjSZIkSepj1iklSZIkdZ12R5adDJwXET+jBNq2BH4fEW8D7snMv450AiVJkiRJfcM6pSRJkqSu026wbEXgZmDd+vgmYHL9dytgxUaSJEmSNBjrlJIkSZK6TlvBssw8YLQSIkmSJEnqb9YpJUmSJHWjttYsi4j9IuKfEXF3RNxZ/507WomTJEmSJPUP65SSJEmSulG70zB+GHhNZt4+GomRJEmSJPU165SSJEmSuk5bI8uAK4GJo5EQSZIkSVLfs04pSZIkqeu0O7LsaOC6iLgKeKw+d3Nm7jaiqZIkSZIk9SPrlJIkSZK6TrvBsiOAXYFrgZn1uYdGNEWSJEmSpH5lnVKSJElS12k3WPZv4BeZ+dgc95QkSZIk6dmsU0qSJEnqOu0Gy64DroyI84BH63N3ZOY3RjZZkiRJkqQ+ZJ1SkiRJUtdpN1j2G+CqAc/dMTJJkSRJkiT1OeuUkiRJkrrOkIJlEfGSzPw7cBmw8IDNj4x4qiRJkiRJfcM6pSRJkqRuNtSRZfsDOwHfAlYHEoi67UZgxxFPmSRJkiSpX1inlCRJktS1hhQsy8yd6v87jGpqJEmSJEl9xzqlJEmSpG42YaxeKCJ2jIirI+KmiLgwIlaLiMUj4pSIuCEiroqItzTtv0Xd/4aIOD4iFhqrtEqSJEmSukdE7F7rkhcMsn3QuqUkSZIkzcmYBMsiYlXgq8CbMnM14OT6+FDg0sxcA9gMOCoiloiIlYGDgU3qtluBA8YirZIkSZKk7pKZR1Omb1xjkF1a1i3HKn2SJEmSettYjSybCeyUmffUx9OAGZRKzJEAmXkrcCawObAVcEJm3l33PxTYeozSKkmSJEnqMpk5czabB6tbSpIkSdIcDWnNsobaM+8rwMLA7pQg1oGZed/sjsvMW4Bb6jleDRwB7AasnZlPNu16E7ASsAxwUdPxD0TExIiYMLCCFBG717QwefLkdt6OJEmSJGkMDbdOOYdzLgncP0jdstX+z9Qhl112WaZOnTrclx4x+641Y8xfc9mJY/u6w83nsc6bXskXMG+6xfTp03smrWPNvBmcedOa+TI486Y182Vw5k372gqWAccDZwH7ZObjEXEl8H3KSLA5iohPAzsA2wH/arVL/T9ns+1Z6nQcRwOst956rY6TJEmSJHWH45mLOuUghlx/hOfWIadMmTIXLz0ydtr/7DF/zX3XmsFh17bbJDB807afMqzjxjpveiVfwLzpFlOnTqUb7iPdyLwZnHnTmvkyOPOmNfNlcOZN+9qdhvElmXkMZQpFMvMHwIuHcmBEHAG8GnhNZv619hycFBHzN+22GvCf+m+1pmMnAY/OYdoNSZIkSVJ3G3adcjBzqFtKkiRJ0hy1Gyy7OyLWpvbci4jNgAfndFBErAu8Hdg6M6c3bTob2KfusyJlTvlfAqcDO0XE0nW//YCT20yrJEmSJKm7DKtO2UpELNwUIBusbilJkiRJc9Tu+PldgR8CL42I+yhTKe44hOPWASYBV0Q8MxvGv4H3A8dGxA3AE8AemXk/cH9EfA44PyLmAS4F9mozrZIkSZKk7jLcOmXDZU1/HwZMAw6hdLBsVbeUJEmSpDlqN1j2v8x8XUQsCpCZDw/loMw8DjhukM1bDnLMqcCpbaZPkiRJktS9hlWnbMjMtzX9vUfT3w8wSN1SkiRJkuak3WkYfwOlQtNupUaSJEmSNO5Zp5QkSZLUddoNlt0WEa8ZlZRIkiRJkvqddUpJkiRJXafdaRgB/hwRNwGN+d//kZntzDEvSZIkSRq/rFNKkiRJ6irtBsv2APYd8NwjI5QWSZIkSVJ/s04pSZIkqeu0GyxbBVhmwHN3A/eOSGokSZIkSf1sFaxTSpIkSeoy7QbL3gmsWP9eEtgQ+D5w2UgmShpvVtn/7E4nYdRNO2TTTidBkiRJnWedUpIkSVLXaStYlpkHNj+OiDWBL4xkgiRJkiRJ/ck6pSRJkqRuNGEuj/8X8JqRSIgkSZIkadyxTilJkiSp49oaWRYRhwIr1YcTgLWBX410oiRJkiRJ/cc6pSRJkqRu1O6aZT+kzCvf8PfMvGsE0yNJkiRJ6l/WKSVJkiR1nXaDZZ/IzJ0aDyJiwYi4ODNfP7LJkiRJkiT1IeuUkiRJkrrOkIJlEbEOsDywUUS8rWnT84ClRj5ZkiRJkqR+YZ1SkiRJUjcb6siyzYFVgMWBrZqenw7sOMJpkiRJkiT1F+uUkiRJkrrWkIJlmfklgIj4R2YeMrpJkiRJkiT1E+uUkiRJkrpZu2uW/V9EbAgsUh/PA5CZvxrRVElStcr+Z3c6CaNu2iGbdjoJkiRJY8U6pSRJkqSu026w7BRgMWBd4LfAm4CzACs2kiRJkqQ5sU4pSZIkqetMaHP/tTPz/wG3Z+b2wGuBySOfLEmSJElSH7JOKUmSJKnrtDuyjIhYBrg1ItbIzBsiYqVRSJckaQ76fYpKp6eUJKk/WaeUJEmS1G3aDZZ9Cfgs8EXgrIi4B/jdiKdKkiRJktSPrFNKkiRJ6jptBcsy80TgRICIeDmwTGbePKfjImJ34JPAbZm5UX1uceBoYC3gSeBTmfnbum0L4EBgPuAvwF6Z+Wg7aZUkSZIkdZe5qFPOto4YEW+u5727PvUQMCUznxrZdyBJkiSpH7UVLIuIVYHvABOBtwLfjIiPZea/Z3dcZh4dEccCdzY9fShwaWa+NyJWBC6IiFcBiwIHAxtk5t0RcRBwAPDpdtIqSRqf+n16SnCKSklS7xpOnTIiVmbOdcRXAF/MzO+OXuolSZIk9asJbe5/PPAN4HmZ+QSlkvPtoRyYmTMHPLUZcGTdditwJrA5sBVwQmY2egQeCmzdZjolSZIkSd3neNqvUw6ljrgisHFEXB4Rl0SEPUskSZIkDVm7a5Y9PzPPi4gEyMxzI+Kwdl80IpYE7s/MJ5uevglYCVgGuKjxZGY+EBETI2JCi4BbY4rH3QEmT57cblIkSZIkSWNnOHXKlZlzHXER4HZgR2B54OKIeHVm/nfgyZrrkMsuuyxTp06d6zc1t/Zda8aYv+ayE8f2dYebz2OdN72SL2DedIvp06f3TFrHmnkzOPOmNfNlcOZNa+bL4Myb9rUbLLu59tDLiFgC2A64dRivmy2eiyFse+6JMo+mrH3Geuut1+pYSZIkSVJ3GE6dcih1xL0z87Gm1/gDsCFw0nNONqAOOWXKlDaSPzp26sA00vuuNYPDrm23SWD4pm0/ZVjHjXXe9Eq+gHnTLaZOnUo33Ee6kXkzOPOmNfNlcOZNa+bL4Myb9rU7DePOwE7AZOBGYCNg13ZfNDPvAyZFxPxNT68G/Kf+W63xZERMAh5tNapMkiRJktRThlOnHEodcdEBx0wAnkSSJEmShmBIwbKIWBQgM2/PzK0yc+n6b6vM/N8wX/tsYJ96/hUp65X9Ejgd2Ckilq777QecPMzXkCRJkiR12FzWKVvWESNi4aYOmEdGxL71tVakjCq76LmnkiRJkqTnGurIst83/oiInefi9S5r+ns/4HURcQNwFrBHZt6fmdOAzwHn123LAwfNxWtKkiRJkjpr2HXK2dQRDwM+XnfbE1g/Iv5GqV/umpl3jkC6JUmSJI0DQ51sesGmvz8E/HA4L5aZb2v6+wFgy0H2OxU4dTivIUmSJEnqOnNVpxykjrhH0/Z7gfcMO3WSJEmSxrWhjixrXlB54ELKkiRJkiTNjnVKSZIkSV1rqMGy5v1y0L0kSZIkSXou65SSJEmSutZQp2GcPyIuA2YCL4mIS5u2/S0zdxrxlEmSJEmS+oV1SkmSJElda6jBsjWAFQbZNn2E0iJJkkbRKvuf3ekkjKpph2w6rOP6PV9g+HkjSSPIOqUkSZKkrjWkYFlmPg38Z5TTIkmSJEnqQ9YpJUmSJHWzoY4skyRJkiRJkrraWM8asO9aM9hpjF/TWQMkSRp5BsskSZLUUr9PUWlDkyRJkiRJAoNlkiRJkiRJUt/r91F3doSSJM0Ng2WSJElSmxx1J0mS1B868bvOQKIkdZ8JnU6AJEmSJEmSJEmS1CkGyyRJkiRJkiRJkjRuGSyTJEmSJEmSJEnSuGWwTJIkSZIkSZIkSePWvJ1OgCRJkiRJkiSpu6yy/9lj+nr7rjWDncbwNacdsumYvZak7ufIMkmSJEmSJEmSJI1bjiyTJEmSJEmSJGkIxnrEHTjqThoLjiyTJEmSJEmSJEnSuOXIMkmSJEmSJEmSNFf6fZ07cNRdPzNYJkmSJEmSJEmSNEr6PZDYD0HErp6GMSK2iIirI+KGiDg+IhbqdJokSZIkSWNrTnXDiFg8Ik6p26+KiLd0Kq2SJEmSek/XBssiYmXgYGCTzFwDuBU4oLOpkiRJkiSNpSHWDQ8FLq3bNwOOioglxjalkiRJknpV1wbLgK2AEzLz7vr4UGDrDqZHkiRJkjT2hlI33Aw4EiAzbwXOBDYfsxRKkiRJ6mmRmZ1OQ0sR8W3gosw8tem5/wErZubMpud2B3avD18M3DimCe0eSwH3dDoRXch8GZx505r5MjjzpjXzZXDmzeDMm9bMl8GN57xZOTOX7nQi1DlzqhtGxJJ1+xpN2/cClszML7c4n3XIYjzfV2bHfBmcedOa+TI482Zw5k1r5svgzJvWzJfBjee8GVYdct7RSMkIaRXFi+fslHk0cPToJ6e7RcRlmblep9PRbcyXwZk3rZkvgzNvWjNfBmfeDM68ac18GZx5o3FuTnXDIdUdn9nZOiTgfWUw5svgzJvWzJfBmTeDM29aM18GZ960Zr4MzrxpXzdPw/gfYLXGg4iYBDzaPKpMkiRJktT3Zls3zMz7gEkRMX/TMavV4yRJkiRpjro5WHY6sFNENIbL7Qec3MH0SJIkSZLGXsu6YUQs3BQgOxvYByAiVqSsV/bLMU+pJEmSpJ7UtdMwZua0iPgccH5EzANcCuzV4WR1s3E/jcggzJfBmTetmS+DM29aM18GZ94MzrxpzXwZnHmjcWs2dcNvANOAQygBtGMj4gbgCWCPzLy/Q0nuFd5XWjNfBmfetGa+DM68GZx505r5MjjzpjXzZXDmTZsis9X07pIkSZIkSZIkSVL/6+ZpGCVJkiRJkiRJkqRRZbBMkiRJkiRJkiRJ41bXrlk2HkXEW4DPNT21DDATuKfpuWMy80cRcQbwfGBG43Bgvsx8TT3XUsC3M3PbiHgrcBxwb93vAeDTmXnRKL2P9SnrBjQsBzwCPNT03Icy8+qmY35Un2vep7FtIiX9AC+krEHQfO6NgCeBrwC3AhcC78vMXSLihMx8f0S8BnhHZjbnb+P8awKHAd/JzLYWAY+IjYFNM3O/iHgT8NKa1t8BqwM7ZOZv2jnnWIuIH1DS2vBi4Mamx1dk5kcGHLMO8K7M/MKA5zcGNgUWBpYF1gTubNplUeC3mfmpuu7EecBHgMvrvyWApzPzzDmk+Tjg85n53/p4KeDIzNxmKO+5xfl2Bz4J3JaZGw2yz1HABsB3KeXw7Mz8wxDPvwRwaGbuMoy0tbqeFgCmM+veMFbX09nAla2uo3ruXYCFgBdk5sfbfa8tzvd7YBvKHMurUcrPdzPza3N77tHQuM8A6wCL16fnAyYD/6JcE08BJ2bmIQOOfQewTGb+oMV5NwYOBU4EPk75fG6vm5cAjsrM70TEt4DTgM0onWGOpVyPv28uH4Ok/VfAFpn5xOz2G6qI+CKwEzC13oNH7D7TtP3jlO+0xnG31P+b7zMHA+tRrpX/AisCP6ON+wzlOjiL1p9pw9lz+kwjYkfKejoLA/8DdqlpOpjy3XFCZp7SdPx8wE8o+RaUNXnuolwLLweuBW7NzB0GSf9If6afAN5X038DsFNm3jP7o3pfN/6mmUN61wR2GYl7cD3fqsCPgaUo5e/EzHTee0mSRllERLpuSlfws5Ck8cNgWRfJzN8Cv42IZYDjMnNDgIg4E/hoZt7ctPvSmfm65uMj4o9NDxemNKgBvAb4WmYeUfd7MXBWRLwjM5sbK0fqffypNhI2Ghb3pDSsXVAf39+iAWklSgNkK++lNG7umZmvjojlKA1BX4mIAynvcw1K4+ubgXcCj0bE64HXR8R5wIuAfw9y/iUpDagrtflWqcGSRsBkRWDlzHy0vu6XKA1hXS0zd42IlZueOhnYYdbm/E+Lwxan5NvAc/0hIi4ALqMEyb6cmSdGxKGUQPATwHkRsSilQfDNlEbiFwKPA68Hlq+BtLuBzTJzZovXXxVYrOnxwsxFXmfm0RFxLM8O7A20HbBkZj49jJdYFFhrmGlrdT2tDlwJ/IixvZ7WAdaJiHdSgpU/b/G6z6NcCyPh5ZRAwlWZ+e6IWAS4OiJ+lZnXj9BrjKSlgBWAd9X/AZYGDgT2oQR17hkYVKmWoORdK5cB/6SUgROB+YGFMnPviJgMfDEiXkgpY6+qr307cAmwOfD2iFgSuDwzdxvkNV5G+U0wIoGVzPx8RJwAnFIfj9h9BiAiJgBTgDOAU+t+9/Lc+8yGwNqU+8s99f8taOM+k5kZEe+i9WcK8FRm/q/F8c98pjXg8FVgncy8JyL2Ar4HPAZ8jRKsP6QGRr+QmXdQrqfbKZ0vfka5dh8HrqZ0pvkZJYg3mBH7TGvA6P2U3xOPAr8EroyIaZT72/Mogb+GA4D7gbcA1wHnU+7TW1KCTP8P+DSlw8+wO/ZExPLAryn3nI0y87r6fADfoJT/h4C/Avu2Cl7NSZf+pnk+re/BUK6Ftn/PzMa+wJcy87cRMQm4MSJ+kZl3jeBrSJI06iJiwiC/+bpKRCxE+X38WKfTMl7VesuKUH4LGigbH3rlHqH+1s3B+YhYAIjMfLyb0zm3DJZ1mYg4jdLgulYNfs1LabA7LiKeAN5dfzTNW0cyNY8sW6zVOavGfmTmjbW3/T7Ah0bhbUBp4Fm38ZKUgMhL6+OLgHOGcpKIWBj4LLAJ8MGI+CTwbmCFiNiEEjAgM78bEbcAX6YExQ4Bvg8sCEykNEjeUkfvHUHJ19sojVBvoQRaPltHxiwMLEJp/LsLuJjSQH0opTf5jykNnrtFxKspI0kuoPQCXzAiplAacScBO0TEx4C/U0ZZAHybMmLwdTUtq1JGf1yfme8aJB/mq6/7SmAeShDquFpG3paZD9e8+m1mbhARb6znflZ6Bzn3JGCPpqemNj3OiPhSZj7e6tgW53o18FNKQ96LgJdFxG6Uhr8PUHrLQxkZtT3wp/p+XkxpiF6fMvImgd1a/VCpveZfDvw8Im6iNASvCywXEedSftQ+CNxU8+sOSqPxw5Ty8oHMfCAizqI0NO9K+cz3Gfha9fUWqXmyCHBVROxDaYA9rp77e3XXFYG3Ah+t22dSPuuzKI25q0bEtfX1L2nxOltQGiUXBs7PzI81fe5voJTjPwHXUBrrN6CM4poREe+pefor4GOt3kd9jeFcT41RND+v72MeyvVycER8nVnX0TuBver2GbWx+qOUAMEESsP5yzPzzhbl8/X1vbyA0lD+/qbgyaLAeyJiK+BtlIDHSlFG+i1FGXmxa2beWkcCXQzsXLd9PjNPHiQvFgdOB1au6fswJSjxR+A1NUCyCvCNGqjblhIguZcSnFk6Mw+IiE9RytAdwO/r6bcHDqfcl6fX1zkSeCPwdL1HvKW+7+9QytZdwGAjnb5FGXHyBKVsLVzeQqwJ/IVy/9iQElB7sH4GC1Aa9V9JCSDtBWxcG7nfNcjrzBsRP6eU4/mA+2pefgbYtqb1BTVvf0sJVH2Zwe8zzwSWR/I+U51GCbi/GLiZEhCbl+feZ94HXErJ3xUpefMiyj3hS5T78N8j4mFgywGdUhppn6++19dTykrjOtybklfPj4gLKdfCMY2AzQAzqSOxaoDz/ZTvmMsoedjwJ+AHEfEfyj3pfEpQ8aWUANnfKCPoNgdeQim3sx1B1KqsZ+Y59TM5gfIddAklCLvvIKPFHqFcl4/Uc64BnNYYvRQRr8zMy5tec13gg8AvgNfW15iXkt8PAFdRPrtpzEXHnhqkXDvKSK6VKYE5KNfa/PXcSRmdugfw9dnl1Wx0w2+afSjl+1LgpEECZQNf7zm/eer335B/I2Rm8+/E5esx9w3l/UoaP6LMojBvPwfS6z18ocy8u+m5vm0o6hcR8SJKx6hvZObMbm8Mr7/vDy1/xgmZeZLlbGxFxMso9b3LKb/zT8rM4zubqtFlGXum7rFfRPwT+Hu2OeuUhqbb78GdUIPzLwGmd3NwPiJeQmmzXCEivpmZ53Y6TaPFNcu6SJSe/4tk5saUXsdbUBphNqL00r6DWdNY7QmsQmkIWo3SeL13Gy93BeViHHERsTqlgfW1g/zbs/YGH2hqRFxZ/+1Xn/s6TVMdZebXKdOynVLz6d76mutR8uOceswOlIa4b1Aalxatp1ifEnBcDfgHpYH/t5TptL4CnA1sTWlw/iXwReA/lB9LW2fmaymjNxqjmJYGVszM31EawH6Yma+qDYrvqulcrZ7rIGb1rP8KpXH7MErj2EuBF0XEYL3BVwf+Vs+1AaU8QGk4fVf9+53A3yJifkpwr1V6W9mbwT+r1wG7D3LcNk2f12W1QXYtStDjckrZ/XSWaQ0TeGtmLk0ZyXEQZTqyVer73xbYn9IA12gUnkFru1A+9/dQAhtPUK6RoARj1qA0iK9CGQn1EkogeVfKtG8H1vOsSZku8DXAjpRG4+fIzOmZuR5l9MBamTmV0mt/ScrnuQVweM3rmZQG2VcDbwI2zMz7KEGe6+rxzwmUVZ8Fds/MtYGZtSysTmnkvxK4ntIA/FpKo/+qlDL0kZqGl1BGsry3nm8krqcrat59nzIyY0PK5zsPA66jzDwBOIpS9i6iBFs+Xd/7ApRp5749SPlcD7gpM19OCa69tSZxceDCmie/pARi76E0IP9fZr6MEjT5Vt1/1ZpHG1AC1t+OiOcNkt+vBH6ematTAhefyMwnKdf+BnWf7YDLooz2Pbyec6OavhUj4pWUe90rKI3pb6rHfYgSrL2a0kj9NmYFxG4HflDz52hKsP1F9X19pukz+0NETIiI11GCNItRGs2/Q2nk/kW9tt4I/Lnm4wxKGbyGUr6DEsA7jNJh4MXM/j7zIkpQeV/K5/808AnKfeZoShk8mFIWtqMEisf8PhMRm9c8/SWlTK8C/IbW95k9Kd+d/6SM3noI+EMNxr4M+EgtAz+nBNZaWZ1yT7qaZ1+H76JMrXkdpcPEB3juSLiPR8SVlKDRZ6OMiFuF8t3xgczcHtgNmJaZG2fm5zJzU+CblA4E76Rcb8tRgnvzUILQx1K+BwYbvdTsOWW9Pn8ApZPGWpRRqttR7mnPkZlXNYJhEfH2mrbvN+0ycLTkmpRA8bcpZfdcSlBoHcrn/R5aB1ye1bGHUt5admQY4FkjfjPz95m5V62ILUrpwPG3IZznObrkN83GlA44a1ICVo0pR+fkOb95hvEbgYhYozYc/IUyvfRg38+SxqHasHwl8MnaIajvRMTRlKnYL4yIr0fEzlCG/nQ2ZZqdKKO3T6d0UPkdQCNg1tmUtRZlJoLjKb/LvgPsExFrWM7GTpSZKb5HGVX/Psp1/4LOpmrk1A6tRMRrImKriHhVRCw63stYDZSdTalDTgDeWMuCRkBErBYRn4qIJbv5HtwJtYPEOZT2yj2jDODoOvUaOY1Srz8bODwi+nYAVt++sR72ZEQcDjxaR2D8htKIeD+lsfN5EfFrSsPrk5TGv+fX/ydExHRKb+45mY8Rmm6rhQ0pI5yOqL2XL6I0Nj6Umf+NiIMoU4UNHEUxJTPvHfDc/1ECVgBExBsoP17ujohTKY2XUKZEupwy2uc6ymiTH1Aa4e9k1nRJPwUOqhf6UpTe7c1+Sml4f0lN84aUkU6PZ2ajoe1kZgVcWoqIxSifyw5Rppaarymt/8jMG+p+D1IayWZExN8pP8T+2+KUfwceioi/UgIPi9TnT6CM+jiREnD6GqVB/K420vseSnBnQWDtLFNOvS0zfx1lbZWpzApGNDs5M5/ViFnfz19pmk6uluElgT9EmbJrRUrj6aGUwM6J9bXup9x0X0op20sNkt5fUAJSHwVOopTjX1Iayuep+8xPCVa8hBKg+GdTXvyo6VwnAWTmlRHxNMPrQPCvzLy4nuehiLiM8iVyDiVYNFQnU0aQnk4ZnfLfiJiH0qj+MkoQ5wnK9GXfoARdrqWU2XPq9oUpow5gZK6nSyj53RgldVFNz1nM/jqCkvcTKSNyFqBcA3fTunyuTWnEnZfy5Xts3TYDODYiVqOUlccowaWbmDWS4zRKg/oz+Vh/7N8eEddRGoJbTeP2Z8qP4CtqOqfX50+gXEsX1dd8B2XEyyWZeWfNt9Moo/02AH7VNNrmdMrok4UpDfMvo5TF71ACta+vr7FN3b4ipexCCbpclpmNgBv1nP+mfMbvbHp6R0pg/F+UoMUq9f3MSxn1cRolIPEOStDlYeCOIdxnbqQEgw6gfObzUUajnEC5NvenBKC+RgnA3Neh+8xLKB1KZtR/l1JG6gx2n3kXpRxPq/teHRFrUe4z20bE/1ECs6fQ2j8oAYrbKZ/tY/X1plICT9dTrq2/tjj28Mw8rCntn6Z05rgA+Eqtr06kjJZsLqcHUtaqOo8yiuwxShlYiFkdPJap6ZqTwcr6RpTOB2TmRRHRairJZ0QZYXcoJUB0Yz57tNeLmtL/FCWYeR7lPvBNyjX+E0ow99b6Ptal5OHsXMGs6TrbVq/Jd1DWOpzdlJWz0w2/aa6l3Ed+QPk9eM0Q097qN0+7vxGov1leVCtzJ0XEW7P11KOSxqcplN9uDwBvioinc5TWxu6EKLMZLEHpULkW5TfbhyNigcz83mwPVqctQpmB4rKIODMifpOZb+3iEWYbAFdn5kkAEfE+yu+lGzqaqvFlEUqdqjGCf35aBMsienMkVmZmlFlpvkb5DfhhSr3vjI4mrINq4GZHyowy34myvMHPKO15D3c0cX2gqdPC/4C3RMQ2mXlXl96Dx1QNyB4GHJCZP4+yxvkrOpyswWwG/DIzT4sy0v6NwM4RcRulA+5QO3P2BKO53ScpX8yviog/UBot183MxjRNU4BfZ1nP7AjKVF5bUNa+2JDSgLzBc876XK9l9H50nUVpoAH4PCWAsQmloRRKb/SpQzlRZk7j2eV0OWaNVoqm5/egNB6+nNKz8WOUBnooveUb6xudAXwzM9ekNEINdAalQe8GSqPWIZTe8Lc17TPUIPMMypRSr6iv15ji6NGmfWbmrKnHZvdFsRPlh/ImlAAgAJn5R8r0fi8EXlgfTxhwrjml9321l/irKdO5QQnAkWXKz13ncHyzx5g1BVrDw/XfHpQRK3+mBHdPoDT07xAR91A+r1MpP9wGNjA+IzMvpDQiXkWZ2m47yrR31w7ctf57mll5MA/PHoHQXLbmqfu36/4B6duMUu4nAZfUXvxzVEcYbEEJ7v0iyjRmO9V0vZHyuTdGdb2AEiCBMu3gKzLzFZRA49GzeY1ptHc9LUcZbfNnSuBhQn3t9zP766hhPuAdmTmRMjribFqXz8cojeln13MfXrc9Qgl6nEcJCP+VWaOnGibw7M9t4Gf61CBp+wyl8rM+zw5E/ZzyI24tyvpi/xkkzdTXjQHPJyUg9M36nv9CKdsfrvvcW18b4D/1/vAKSmD1OcPYa4Bu4DV1fx2FcgolP86iBBQeoIx6/BClMX+Lum0hZlX4Znef2YFSUTiV0hD0UE3DH+t7WJ5Z95mB18tY3mce59mdPRqf82D3mRfU93YmZQTNzyn3mZUoQbS1KUHAQdNO+V7ZhBKono9SkV6Nkp//ru9ltnkQEUdQ3v9rMnNn4I31e/u9wDn1740pa2/9gTJSch3KPeYJSlmbRAkgTaWUraEYrKwPLNfzMHun1fNsQPkMmv0zMzes/95IuSf/iFKOfkj5bv4o5bM5hHKPHIq56tiTmVtSA4oR8Yk57D6YbvhN07gHQ/nO+9kQ097qN09bvxFqoK2R/uso96kpQ3x9SeNAZn6n1lXPoHbsiogNO5ysEVE7sa6XmVtm5szMvDrLtENfBd4QEet0NoWancz8Z2ZeVv9+J/BE7cjZrSPMkmfXLR+laTR57bikUZSZjXWfG9NvLwU81jQia7m6X88EyiJiscbvudo59Z2U33KXUjrS/S4iXhYR8zfe53hSAzYTmTWL1zRK/a5V51K1bz7gQ5n5dsrMUD+OiGW69B481hag1O+vrI/vpkVn2C65Lh9lVif5n1Nmb1mQ0jF18w6ladSM94LZjbI2RD6QZUqmKczqBd7wYNP/Uf81nnuIZze4NDzzw6r+qN+XMj3SiMvMu2pPgUUpc9cPbFQLSuPkcNwOfDUiLqCpoTYzD6c0FL+PEuh6L7OmJ1uScgFDuRldEmXKy3cxq3FwBuWH0IKUBsGl6jnmofQiXC3KGi3Ux61MBxaPiImZ+RDlh+7LACJiV0qPgeFamTKy5UFK0K25UfMkZvUMgjIKbfkhpBd4pvELykiZy1rsMpyeDTMoPzjmq//PQ2mMvpNy8/8MpaH/PspIhSMoAZO/U6aZuvu5pywi4juUUSBTKaNz5qU0ki/FrHy5idLIeCOlMbIRYNyGsgZQw/b1nK+hNP7O1Y/eiFg5In5f03UYJdjz4vr/ohExz2DBsyhrHkWW+dB/T2nYXhm4IDP/RZnGLgZcT5dSrqU31h8aP6AEd4dqTtfTRymf4dso95jFmTVKrNV1NJ0y8mYCZfrSeYFH6n4foFyTrcrnZEow5byab80NLN+kTD/315q2hylrVP2/puOnNu2/XRSNaSxbrR8FJW8vbArUzFPP/2g93+eZNTXnpcAGEbFsHe23RX3+QmCziFg4IhYEtqrPB6WS9WbKyJrm/FmMMurrRmCJiHh53fZuhtbR4VFgqfqZbU35Eb9ivY7vpOT7mcB/M/PEmsb56/9zMrme/3eU4FLj2oUSoPkGcHJEbES5ry7bofvMhZSyNC+lLL6m/j+n+8xP67Z/Ue4z81JGeC1IGU04WLBoZcrIogcpI7EagYZ7ge1qBevtlNF2LdXg99spU981vtM/GhE7NO0zDyVQ2aiE31TTdAql88brKGXzs5TRSF+jXE9z0rKs1/feuAduROk9OVj63wNMysw9M/Op+tyCEXFxHVG2fERcFBF/iYjdKVMIfY8yNevXKSOhvk65vvev72EohtWxJyJWbdxr6+iuYynXY9u65DfNRjVwfgGwRC2LsxURC9H6N8+QfyPURrlzIuLN9fGKlHvvP9p7m5LGg8y8lXL/f5wSMHtDZ1M0Iv4H/DKeO4X1lZR69wpjnyS1qwYImgNm59bH3Taq4Wzg4oiYGGUq+TUpZZCIeC2w41A7YmqufIVZo/iXAS6tI7LWp6ybvmrnktaeWr94J2Vq+9fWzosTKL/RP0Gpvy4L7J+ZT/ZSEHAkNAUhjmdW57h1KfWnrPusExFveu7RGorM/E9t44aynMYlwE8MmAGlzfhnzJo5bBWeve77itA1wfmfZOZp9e8vZOb/y8xvU9K/ZUQsMptje854LpTdbsGIOC/K3NqNXr8TBvm/1XOPUAIGUBp6PxkR10bEDZTAxNZ1xMSoqA3H36c0jjU03scmlLVx2jVPZl6YmWtk5kaZuTXPbtz8BCUQ0ghO/JVZowcmUBqljqH88DkT+HU97l7KSKWtKQ39V1CCNYtQGvVWoPyIODUiLqWsf9NYc+RuypRSUHpbvxK4Psq0cd8HNooyFdzbKdMcPcKsKQFh1pRL1PMMNqLqBMoPnGsoN9B/N32pnEBpZD8BIDOfoDSAtkpvS1HWF1ifWdNIzVufn0AJcgzV3fX1bqvp/Dol4PIaSg+d4ylBrQMpIzoepdxcF6c0CJ5FaWB9MeVza+Wweo7T6/9JGVH5KLPy5TRKAO0qSsPg45Re9S+g/Pht+F+UaRO/TxlR06oRv+HKpr//S/msHqGp0bAOPf49pez9hTLF3nWZeQ8lcPsfynR0rXwdOC0irqYEVH7MrM/9WsrncEt9fw8DD9fG01Moge+rKEGB01qcu5WhXE/fre/x0nr+pym9rBrTgDVfR1B6FL+RMjrmaUrw7k/1fd9JCfy1Kp83UoJO11J66u5fz3cVpRH6E5T71qZ1n+8Dn46I6ylBpo82paExCuO3lDWhBnY2aDgS+EKU9aQe5dnXyNGUMvszgMy8g7I23IX133WUKdiuotxTrqQEmH5MKfvHUAJ6b2LWlBbz1r93oHxe81MC8j+s94glKT2tWnmgKX2XU/JzJcraaR8C1qrnmJ9S1g8E7ouIyZRK902UH6QvYfD7zPWUkUCNEZ6fp5Snxut+kHJNN0ZyXk8J3M/uPjOdpmDOSN1nar6fQunB9EnK6MoTmPN95hFKHt1LKbdPUq75qZS8H9hgcnN9T43r8BbKveqflADJrZQGl7/VdExtkQcN61BHhdXv4msp9+3me85CwOczs3kk8zzALZn57/r3GfXfeyg9umY3/dP1lI4Lg5X1LwJr1LQ0PtPB0r82sHrT74g1KPfUxoiylzWNktuY0lFkN0p5/Dol3z/JrJFlX2nxGjByHXs+AHw3IuaLiAXq+7txDscMqgt+07wgIrahdPj4c0T8KAbvXX4vcGsN/D/nN087vxFqYHRLSmD3b5QyflSjl74kDVQDZidQRphtEhHD7UzQURGxa+3ochLlu/99jU4GETFPvcfexuD1FXWRLFORNzrGvRN4NCJGpePwcEWZkux+4IzawWkC5bfCv2rHzhOB27OssaxRlJn31M8ASrBsWpQ1bL8LfCUzb+5c6tqTmU9T6uNQ6tuN2QZWpkx3fzOlw9xiETGpKXg0LjQFIW5g1qiZBP6dmU/UINnxDD5bjdpQ6xZfpNTfT2oEzDqcrI6p1+dPKJ15odxv/gwQZYT+8RExnHrmiIqIqANCGn//uWnzE/VfX418ju4IUAqg9h76TmZuP+D5F1OmMZpIaRQ+ltK4sQylMWUVZq1rtiywbWZePlbpHqhWLD5PeS/n1ufWp/y4eIDSePc9nr2e02qURs7mReP3z8zfRpn+4muU3i+r8OwpyZahjFSYQunt/DVKo8+2lJEKu2Tm4xHxcUqv+AOG+Z7eTemZf29E7A8smJlfGM65xkI76a35G8CBddROY52XZSgN5ydS1ko5j9KgDyWQ+DxmBQoBpmXmuyNiBUpg4eeUCuadTfssQglsfb5u+x0loDKFMk3dYZl5fj3Hz4C31UpDu+//JTWtF1MaqC8F1sm6tlTd52ZgzebnupHXU1vn/QNliP/1c9y5vfPORwnMnJuZD0fE9ygjPY9vsW9Qrpl/Agc3jcL5E+XzWIiyxtp5wB+Y1eC+RP37rqbTXZyZe0fEqymjim6lfCbNIy8nUYIhp1OCfD+lXDubUIKOH8rMayLiZZTvjvWH84M0Il4FPJWZV9XelOdQRm71/X1mNp/pFfU9zaCst/Z5ygjMRkeGlp8p5Xv8t5TvdCgj21Zg1o9kKNMtfykiPkiZku8pSrDnMcr6W1dGGe21ObBb7SXallafaWa+dAjHNdbxOxH4TGa+v2nbqpTyfSjwrsw8ICLOoASSG2tA3Er5Mb0kJZj77czcNsr6nsdRgj3zUMr5vkMJzETEl4EzG/vWa/ablFGqi1EaCfbOzAfmmDHPPbf3YElqU5Te0G+jfLfcNqf9u01EvBf4OKXjxz2U7+4HKGvVXlv3+QVwatb1pdT9aqDz6fr3izLzn3M6plNqZ7JjKb+NPwV8NjPP7myqxp+I+AxlpP9TlKmzf1Ubi7u+IbU5nVGmj9yd8lu8scb8Jyidt18PfDgzf9WhpHaViNgc2JkS1DmWMorGvBlBtfPCgZQZVfYaTl22H0XE8cCvKB1qz6LeczqaqBYiYhJlQMn9lHrygZn589kf1VsMlnWZcJHDYRnqD5bag/5ZMnOtIRy3L6UBMCkNW/+l/MC4b8CuR2Xmd4eS5kFeZ3lKI2qz/2XmW9o8z76UH0MzKQ2821NGtOw1YNe5Su/cmNNnVvPir8xqOIfyI/WTc0pzbbA9mtIQ+RTlB86pAz7/lSlrRq3ZdNw7ee7aRWdm5ueG8JaGZCxeY26N5vVUj1mKEkhoXE8rMALlMyKOo4zM+W/Tc+tROhs0+2tm7tLGeScwa/3Cpym9fXaj/IB+54DdP5eZZzJMw71H1WPn+LlFxEGUaSSbpw+5A7hxdveZiHgFpXF/CUrA5qOUEUT78+z7YvDsdQGhi+8zdZ89mVX2lqUEV+5l1v295Wc6xPsMMPTPcDjqZ9p2ORzkM/0Hc/gOqsGe4ymjTK+mTg9ULVm3nUQZFXULpbfqJ+vrPE3pof8hSsXooqG9y/FljH7TdPVvBEndbwi/5edrdDDpRRHxPsp98YOUji8foQTOzqH8/nkqM9tZc1VdoBvbW1pdSxExkfKb7OXANpn5m44kbpxqfCYR8VLKetrfysxfz+m4btGU/jdTZoi4ljJDwP6U33+/p8xW8gLgwcy8uleCgKMtyvTlP6PMlLFrZv7SvBkdEbFCL3aoGS0RsRulnjwR2C8zz+p02Rvk++n5lDa2S4A/ZebvOp3OkWawTJIkSUMynB/CdXReDLdxargBQUnSyIsybdJd9e/ZBh56qfEkIg4F7s3MrzY99z7KusEfpIx4/ghlGuzrMnO7uk/XBV/Gq4hYGlgoy9T4A7c9qyx2w+c22LXUFOhYDlglM//cDentR7MrM3X7vMBimXlfRMzbSyNgIuLtlJFRn6ZMa74D8Jv6eAHgR5k5lPWPx5Uo06/vTLnPX9RL32PqbVGm3P0MTTOadDAtc/p+an6u776fDJZJkiRJkqSWBjTefwX4Y2YeV7dNyMyZA6b8ehFlBocnOpjsIaujppcAFgX+lZmfbHrP2wAfowTMrgHWy8y/1OP6roGo1zR9Tq+hTF29NLAfZdaEbN6n/r0VZZaJaR1O72yvJcp6ZTMi4gWUtcoem915NXRDLTNQ1rTqdJlpV50VZWHKNOjfoswA8l3KPe4TlOn7dwHOyrLO5LhSRwu+HjhhYPAzmqZqbXrOYFkbImIl4PHMvLvFtnGdl7PLm6Z9FsvMh+p1nGOZX8P8rfffzHx8rNI4VibMeRdJkiRJkjQe1caTzSlTic8PfDAiPly3zawjLhqNJztRpq9fqFPpbUcNlC2WmTsBBwAzImJN4PD6ni+mrMV5BLCCgbLuUsvmmyhr+P6KMt3zJ4CXNu8DEBHbAV+nrBfbEUO9lmqg7P2U9Us7lt5+NNQyU/freJlpV2bOzLJG9E+BNShLB7ycMkr2B8Cbge+N00DZGymdHw4GfhwR8zdti5y1puFOEXF883ebBtcILkfEK4H/A34TES8cuE/TvXjLKGtA972h5k0NjgG8JSI2qNfxmJa9Yf7WmzjY+XqZwTJJkiRJktRSlDVcPk9Z6/JDlGm83h4RuwM0eufXhuUPA1/KzPs7lNwhi4ivAwtm5m618Wp1ynSL3wEeBSYDm1DWq901m9bFNVDWebWBMShTNZ+cmT8D3kBZk+nbEfGSpobK7Sijhzbt5NRzbV5LH6FHrqVe0YtlZiia0vz6iPhwRKwCnAb8jjLKdyZljeLjgfvGYwAoItamjLD7DrABsBzw3YiYb8B+OwD7AIf20rSbnVSDLJtQ8vcnwG2UYOQL4JkRe40gyw6Ujin3dCq9Y2moeVODUdsDn6P8Dhlz/fpbbzgMlkmSJEmSpMHMCzwO/CMzHwD+ClwG7B4ROwNEma7ws8AOmXltpxI6VBExD7AS8LeIWBR4HWX0yATgzMz8LHA+ZQqzKcC/63HRkQSrpdoA+09guYhYqj7+ELA8pSxmRGwGHFIf39DB5EIfXku9pgfLzBzVNL8FOJoynexLKAHAAB6JiC9QgkTHZ+bFHUtoZ80DXA5ck5n/pIyw2wT4HjyTh7tTggDvy8zrOpbS3vQW4BeZeVZmbg5cCvw6IlZpGrG3PfBRYLvMvLFzSR1zQ82bj9HZvPH7qTJYJkmSJEmSgGeNUli59nq+E7gOOCEi5q9TfF0N/BbYOCLWpUxhtlUvNCxHxC6U9Xs+BGxHWb9nKeAW4GFgkYhYsr6XlYH3AwvArCn91BnNI2iAbeqIwN9RggNviIgVgFUo5XWDiNiLMpXmJp0om/1+LfWCXiszw1GD/1OA3SlTLS4GnExZm+svwE3ABzLzkk6lcaw1fe4L1tFj9wDLAGvXa/FJyrR4r4mIr0XEAsC2wG6ZeX3HEt4jWnQc+Q+waGOkXmZ+FHgE+HpEzBsRr6NM2/e+fs/fXskbv58GF/7WkyRJkiRJMWsR980p6yVdQBlhdQWlx/0bgW8DnwTeRwkkfRG4o9FDuptFxJGUERcfaZoWah3KFEm/AZ4GXgxcSRlVthWwZ783DPWSiHgHcAxlFM0uwKuAlwG7UUbVTAbeAbyWss7cYR1KZ19fS72kV8rM3IiIb1MCgBMpAbNlgRWBgzLzjk6mbaxFlPWxIuLdwDbA/cD+wPbADpT8mVG3fYUy7eIOwHyZ+UhnUt07mvJ3CmU6y38DdwBnAocD5wErUIK3SwA3UO51C2fmtA4kecz0St74/TR7BsskSZIkSRrHImLhRiNhRLwK+DlliqrPA0sCJ9bn3k+Z2us6SmDpO8BmmXlrJ9Ldjog4EFgP2IIyjdB1wIWUQNn5lMb0t1J6Ti9BmZLouH7vBd/tImISMCMzH46IpYGfAXtTRon8iFIO35iZN0XEqsAL6ravAO8Y689vPFxL3a7XysxIqeXtxsx8KCJeDBwHvD8zb+pw0sZcXSfq25SA6P5AUgJimwCvBlYDvgnMB3wZeIuBsjlrCrJsBnyfEgDamZK3Mymj9e4H1gbeA6wJrJaZX+1QksdML+SN309DM2+nEyBJkiRJkjojIuYHPhYRd2Xm0ZRGxaOAVSmjrE4Etq67nwBMqo93pkwb1PWNJxFxcv3zUWAqcCul1/cjlHU57qB0Jv5lna7tQuD3mfl4B5KrZ9sNeGFEfDEzb4+IMyhTzm2bmStExMHARRHx5sy8rk6r907gnR0IlPX9tdQjeqbMjIQ6hdrTmfnXiNihjqh6AfCF8RIoi4hlgPdm5rfrU+tSAhaTKKPs/gCcRrnOfh4RawLrU9bQ2tZA2ezV/H0kMx+pAehPUTqeLEcJqhwDfJzS4WRJysjNVwAHUkZo961eyRu/n4bONcskSZIkSRq/ngb+RFmTYmfKFIR/BV5HWefmm5SpvdYCJmXmbZSRWG/NzKs6kuI2RMRngfkzcxvKmmR/pzRc3QNsRpmC7QXA+hHxPuDtwN8NlHWHzPw/StvVlyNieUoD+JXA5XWXa4E/AgvV/f8I7JyZ13YguX19LfWKHiszQxIRy0TEBvXvSXWdMgCap0XLzB9TpkvbMjPPbLF+Ur+6H9giIr5XH59D6RSxLaWx/6uU0YPH17WjHgT+RwmQXjX2ye057wHOjbKe593Aj4GXU75LNwJ+SgmwbJ6Z91ACMZtQ1rfq2utqhPRK3vj9NESOLJMkSZIkaZzKzKcj4hJK+8CulOnLToyIPYDHI+IhYEHghNp4QmZe3bkUD11tOJ0EzBsRp1Ma0C+hNKD+hzL91qGUntXzU9bmeGdm/qszKVazphEzu0fEMZSpoj5P6a2/UER8nNIQvmNzY15mPtaJ9PbztdQreq3MtGE5SvDvL8DzKetsPdS8Q0TMl5lPZeY1EbFQJxLZCXX6u6ci4i3AeRFxRGZ+JCKuB94MPEaZ8u5c4EeZ+RTw3/pPQ5CZ34uI1YFfRcTbKev/vRa4JDPviYg7gTMoo7ShBKMvzcwnOpPisdMreeP309C5ZpkkSZIkSeNcRCwIbAjsApwCXAX8AJgH+FZm/rxzqWtfbTh9a2Z+LCLOpYwqeHtm/i8ifgK8kDJN2W8i4nfAN4CLM/P+DiZbAzSCH/Xvo4EngCMp68+tClyRmed0MInP0W/XUq/pxTIzFBHxZWA/4GuZ+YX6XGRmNv6vz70XWAM4qAaG+l7jM4+IBYDzgOuBvSjTzD1BmWZz78w8pzmvNGeNtbjq39+grPm2OeU79FOUIOR+wDaZefl4yt9ezBu/n+bMYJkkSZIkSWpuRPkAcFpmnhYRS2fm3d3QyNOOiFgoMx+NiD0powoOozQOLURpPP0eZSH7b1LWDnnbeFnfpxsNLF8DGiGbgx8/oEyn+fnMfLDVsd2gn66lbtVvZaaVAUGwvYAlKGsfHZGZp9XnF2iMUomI7YH9KVO8/b1Dye6IAQGz31HWnvwSsBKwQPbgenTdYsC1dThlTbjNKdMJrgLckJm/6VwKO6cX88bvp9kzWCZJkiRJUp8b0KCzYg6yWHtETAReD2wKHFzX2OhZEfFy4CvALZR1yt4MfAzYgNKIeiCwcGbeMehJNGYiYqtGEGDA883Bj4OAH3Zquszxei11q14oM8PRNGrsVZR71V8z878RsSlwAPA5yn3tZZn5i4jYBdgT2KmfA0MR8QJgSmYeVx83BxSbA2a/B36VmYd0MLk9pUUAujlvm+9736KsbfWOzJzemdSOrV7JG7+f5p7BMkmSJEmS+lxjPZuIOBiYnpkHz2bfeYGJmflwc4Nzr4qIdYFPAicBNwMnABdl5kc6mjABz/RyfwmwNTCFMl3mAy32m68bppUbz9dSt+i1MjNcEfFWyhSxxwP/BM6oAbS3UToBPB/YCbgOOBb4VGZe25nUjr4aKPs2cDbwi8z8X32+VcBsfuCFmfm3zqW4N0XEjpQRR48PeL45EPNJ4JzMvK4TaeyUbs8bv5/mnsEySZIkSZL6XER8hTKa6j7gaeBLmXnNgH2aGxzfkJkXjn1KR0dEvJoyPdkXgH83enw75VBn1QbtDYBdgenA7ZQRNGcP2K+5bG4JTKKMFJoxxkke99dSp/VimRmO+j6PpkwV+w/g/wG7AUdn5i8jYnnKqNh/RsTiwITs4zUXI2Iy8EvKtLrvATYCrsrME+v2Z9Zvo+TF0/WYezLz0Y4lvEdExMKUa2Rv4A2ZuX59/pkgUH3c0wHo4eilvPH7ae5N6HQCJEmSJEnS6IiIJesohOOB2zLz3cBBwHsiYtmm/SY0NZ5sD5wUESt1Is2jITP/Anwd+BrwKjBQ1mkRMYmyrstqwFOUdYa+DOxeR9Q09osBZfPLwAVjHfTwWuq8Xisz7aqBHiJiQ8qosQeA04GfAisAlwMfi4hlM/N/NVAWmflgnwfKlgc+AVxAmYLyWsp6UJtHxE4ANVA2IYunI2JnylqV83Ym1b0jIuYBNqaMTlwU+GNE7AMwIBgUjWBQRGwTEbvW0Ul9q1fyxu+nkWOwTJIkSZKkPlSnKluVMjLh/wFPRcTuwAuAfwHzNe3eaDzZgbKm11sy879jmuBRlpmXAJ+ijNLAQFnnRMR8wJcoU8m9EDgS2A/YEPgJpYFvAZj1OdXG748CW2TmP8Y4vV5LHdZrZaYdETEBngn4rEsJ6j8vMz8KfBzYOTO/Q3nPT9FU3vr9PhYRLwG+BzwG/Bm4mxIEmB/4LfCyiNgKZgUvahBgb8qomoc6ke5eERFLAbsAM4Ebgasz81PAKxpBobpfc5BlO8oo7Yu7PQA9N3olb/x+Gll9Hf2VJEmSJGk8ioglKYGhFwEzgKuBVwALAW8DXg6sHxHfBBYAJkTEy4DPAFtm5g2dSPdoGzgdkcZeRCwKvBZYGFiaMkLkhcBZwBpAUEYP7VMH2kyq23YEdhnrsum11Hm9VmbaUUdNvTwizgcWBA6kTC94DUCdcvEdEfFhYAngoMy8tXMpHjtR1ij7ESVIOAF4A/AIsCxwHGVk4eeAlZuO2ZIy5e57u/lz7wY1SLsFZdTeeZQRjF+q19DhwPsb+zYFIncG9gHe08/rwfVK3vj9NPIMlkmSJEmS1H+WAF4PLEJpSF4W+Epm3lyn5HkzsDPwYUpP44uBe4BtbDzRaKmjg54A9qAEBq6glNVLgNOA7TPzxNqYtyxlJMmSwN8oZfOuDiTba6mDerTMDEktP+8FzqeMXplOCfJ9KyJ+nJkX16kZfwc8BNyVmX8bD1PI1mDFPsAdlNFkb6Hk00OU6xBKg/9ywJ0RcQhlFM2LMVA2RzUA/X5KsHkBSnDl+cCJwGKUQMsHI+IhyvSDi1KmBN0OeF8/52+P5Y3fTyPMYJkkSZIkSX0iIl5IaSx5ADgK+A/wHkpDypMRcS2wA/D7+v/BwB39PJWSukOdTm1f4E/Atpn5ZET8P+A+SuPjh4ArKQ2SCwAnZ+aFHUyv11KH9VqZaUdETKYE+7YF7qWUnwsoaw49DXwjIj6WmRdTpiC8oHHsOAiURWbOjIifAusBm1JGziwOTAOeBF5a/x0CXAi8LDOvqVPizWx9ZkEJQGfmwxGxFrAU5Rp6ErgeuBl4V2aeFBGLUcrihcA8lDJ4bTcHoOdWr+SN30+jxzXLJEmSJEnqAxGxGnAmZR2f1YDfAA8C32fWlGVPUKbqWQC4vU7n9XQ9PlqcVpprtWHvh8C5wFspI0agBAEWo/R6/y7wj4jYiDLK5sEOJBXwWuoGvVZmhmE9ShmaB9iIMirq/ZQg4E+BbwPfjIi1O5bCDmkKBt6WmUdRgqN3U0bwTGZWe/a7gMUy82mn2B2aiHgpcFRdc+tLmbkF5V53NGX6vosoU5pCGVF1aWb+CjgrM2f0eaCsJ/LG76fRZbBMkiRJkqQeVxuWjwQOAL4BvI7SAPsEpcfxq4FLKSMwTgLeCFwLsxom+320gjojIl5Eafg/DTiHUg5fGxGfyMyLgM0ojXgvBD5JmVbt/My8ukPp9VrqsF4rM8ORmT+njAy5nhIU+j5wNmWdtf8HnEJZv2zcjJKKiEUiYmJEvDYi3gkcUUcX/hA4mTJ93O+BzwLLUIIDKzSfw1Flg4uIVYFjKaOgJgG71GlOLwUmAl8EfgVcHxGbUK6rJ6H/87VX8sbvp9EX5o8kSZIkSb2rNiyfAPyYMi3V6yhTl70euIrS2DoZeCWlcfY3tWG2cXzfr3+jzoiIFwC/oDRuvwvYEFiHUiaPozTq/QB4HjAFWCkzv9p0/JiWTa+lzuu1MjMctRG+sRbZgsB/MnPLuu39lGkHv5GZf+5cKsdWHS3zecq0d+8A/kqZXm4J4J+UwOh7gRvqGnUnAH/NzCM7lOSeEhGrA9+k5O93KdN/LkkJ+PwfsBewNSU4/SlgS2BGZp7TifSOpV7JG7+fxoYjyyRJkiRJ6lERsSLwCUoDz4+BNwFXUxqSX05pUInMPCYz9wD+AVzSfA4bTzQaImJxYHPgz8BtlPJ4FWXtlO9Rppp7A/ChOkXUCsCjzecY40CZ11KH9VqZGa7MfCoznwTemJmvBhaJiLPrthMo7/PeTqZxrDRNCfd24BhKMHRmZn6SEgQ4iPKxXkUZSbNmRJxCefLIAedQC3V9vDOBQylBoD9SAs8HU6bv+xRl9N5WwGXAQZn5y0YwqJ/zt1fyxu+nsWOwTJIkSZKk3rUDZTqqLYGPAhsAdwHbA9cAGwNrRMRWtVHo8l6aqkw97XXA2pRp09YFPkTpFb8MZbqrjYC9KeXzPcDRmXlEh9IKXkvdoNfKzNwKgMx8KzBPRPyqPj4jM//R0ZSNkaYG/Pko19xXgBvr5/teYNfMnF73vQv4AnBIZr4fHC0zJxGxFGVaz58DtwAvAf4EHAacB9wAPA7snZn/plxrjzefo1/zt8fyxu+nMWKwTJIkSZKk3nUu8B1KQ/JrgUUodf3vUBp1/kJp9PkMMCEzT4X+7imu7pCZv6GsMbQeZT2VFwJvBu6mlNWXZeYdwPzAcpl5P3S0bHotdVgPlpm5kplPRcQ89e+3AtPqlHDj0QuANShT3i1MCVh8HFgpIg4EiIjPACtm5pX1sYGyOVsXeDdlir43UwLQx1NGJO1FCT6fBDwRETsAp2bm1zuS0rHXS3nj99MYMVgmSZIkSVLvmk5pLLkfOIUyVdm/Kb2O35yZX6b2PM7MaY2DbGDUaGoEAIC/A2tRpoiaSRkxtAAwKTPfFhGHAY9k5ncax3awbHotdVCPlpk5mlNjdWY+XdcwIzM/lJn/aMqLcSEiNgQmArtQRvesDJwKvDcz/wIcExE/Bp6fmf9sHNfNn3u3yMxzgbMoAZWlgdWAVSnrcb0beKKOmnot5Rp7EMZHkKXH8sbvpzES5pkkSZIkSb2luUd9Y+ouyvRUqwK7Ap/MzGsi4nBgwczcq+47ITNndiLN6n+1ETEaZSwiNqH02H+Csh7MhcAjwDTg/cDCnS6bXkud1Ytlpl0RsQHwfODpzDxjwLbm8vcy4G+98J5GSv3830oJAixHGdGzPWW9qA8Ay1Ouxdsz8+ONYwwCzFnj+oiIj1GmML2CMt3l+ZSg5FqZuVdEHAoslpm7dzC5Y6pX8sbvp7FnsEySJEmSpB4SEfNm5oxGI0pEvBpYFNiRsvj7i4BtKb2l72008tjAqNESEYtl5kNNjycAK2fmzRExEVgC2B1YCriYsv7KfZm5fd2/I2XTa6lzerXMtCsiXgecBhwD7Al8NzO/WLfNk5lP17/fB2wB7JKZ93YqvWMlIubLzKeaHi9JmQbvaODFlKk2rwD+CFzotdeeAWVrfeBdlDW4XgTMoASFpgEfAebPzL3rvn0fZOmVvPH7qTMMlkmSJEmS1OUiYhlg7cw8rz6ehzIlzwmZ+ef63HrAPsCRwArA6o31NWw80WiJiHmB3Si97xvl7Q/A+XVqqMZ+kyk94RP4UZ3easzLptdS5/VamRmuiHgpsCHwr8z8fUS8APglcGJmfq1pv+2ATwA7ZuZ1nUnt6IuIxYGXZ+ZF9fF8wMHAFzNzeiOIEREbAddSrr97M3Ofun9PfO6dEhGLADMz89H6eAJlhNTVNRi5AmVKwVWApyhrAk7IzO3q/n2bv72SN34/dZ5rlkmSJEmS1P0WAfaPiHfXxz8HHm9qPJmQmZdRGk8+BFze1HgywcYTjaKngUuARSLi4xHxC+BPjaBHDYyQmf8Bvg/8pcNBD6+lzuu1MtOW2hAPZdTHgcCytez8m7Iu1yZ19BwRsQPwWWD7fg6UVa+gvPcp9fFJwMTMnF4fJ0BmXgAcDtzVFCjz2puNiJifss7WTk1Pnw1sBZCZ92bmNcBlwM2UaU6/Ok4CZb2UN34/dZgjyyRJkiRJ6gERsRalAXFRYGpm7l+fXyAzn2jab7nMvL1DydQ40jQ91LzAy4FPAatk5mvq9oWaevK/PjMvHnhsh9LttdQhvVpmhqLpvS2ZdSrFiDgMeCVl1Nh/I2JD4PPA5sC8wB7A2Zl5fccSPkYiYjHg7ZRpFrcAfpOZn6zb5s/MJ+vfKwP7ZOZ+9XFXf+7dIiI2Bl5NWe/tXcBlmfm5um2+zHyqBnJXB5bMzD/VbX2fv72UN34/dZYjyyRJkiRJ6kIRMSkiFq5/T8jMaylr3twHXFqff6bxJCK+HxFTGo0nEREdSrr6XEQsANBoRMzMGcBVwFeBcyPiQ7UBshH0+CmlcfwZY9kA6bXUeb1WZoajBsreCpwaEcdGxKHAfsA1wB8i4v+AjwLfz8xHs6zZ9s1xEiiL+n7PAa4Hbqx/N7Y1AmUHATOaAmWOlpmDxkjGzPwD8BdKcHaBpmDQxJy1PtzGmfn38RIo64W88fupuxgskyRJkiSpy0TE6sDPgJUBMnNmbUS5iTISY6+I2Kmp8eTHwIKZObVxjn5uAFPnRMRqwKERsUTTdHNk5kzgOuAMYCXKWlNExAnAg5m5b4fS67XUYb1WZoYrItamTI92CPBDysiQUzLzw8CvgDcCH8/M06Ks10UjSNSPImKZiFgBnhUkfQj4NXA6sEFEbNHYFhGnUEb13NY4Ry0jaiHqVJ7NeVSDQscDv4mInSNi6cx8rO5/ErBJ8zn69d7WK3nj91P3MVgmSZIkSVIXqQ3L3wZ+mpk3RMTzYFajT+11vB+wVURsExE/AqZn5vvr8fYy1qioZfMY4K+ZeV9zQ2TthT8DuAE4DlghIm4EHsvMPeo+Y9oO5bXUeb1WZubSROCszDyPsibbp4CZdRTIR4F/At8EaBrN0pci4kXAscDk+jga/2fmI5SA2b+BF0bE22sQ4L7M3LN5f7VWr6uDImK55rwFqCOj/gqsCLwlIiZGxPGUe9unO5XmsdIreeP3U3fqpS8cSZIkSZL6Wm08OZ7S6/7ciDgQ+FZE7FobHxvra1wN7Av8CHiyuWHZXsYaDbVsHkVpAL80Ij4cEe+JiPXhmSno5s3MxzPzRuD1wB8HlM0xGyXitdR5vVZm2hVl3TUiYp761KPA9hHxxsx8OjMfAO6iNMyTmdsAV0TEMp1I71hp+tzPAq6tjfrzNm1vTMn4U0rA7DuUAGkjUOa1Nxs1f38AXJ2ZtzeN2sumsngBcC6wAiU49ERm7l6P79t4QK/kjd9P3SvMV0mSJEmSOq82nvwUuJcy+mAKZVqgc4F7KFP1PJKZ99T9jwZobuTp5oZl9a5aNs8AHgbeTVl36Z2UqeXWBt6fmXc07f8j4NEOB8q8ljqo18pMOyJiycy8t/69ObAzpbz9EXgD8BHgCGAacDSwR2Ze3JnUjq36uZ8CfB34E2VkzFKU9Zd+nJl/qQHSGXX/n1FGlH2gPu7az70b1Pw9GvgecDmwFXAz5do5q+4zf85aA+6PwI2Z2ZjitG/zt1fyxu+n7mawTJIkSZKkDouybsW3KL2ME1gN2BA4JzMPrvssBmwM3EnpafzQeGgAU2c1jRL5KbAW8ASwEXBGZv5fLZcrAgtl5mUR8QOATpVNr6XO67Uy066I2J9Srg6ljIqaSgkIPQqcQ1mrbF9K+To1M3/RkYSOsaZRPZMoQYsVKdfZ6cBVwEPA3+qoskag7M7M3Ks+7urPvdNq/p5aH25DCdJuSglALwF8PTP/3bT/8cDjvRCAnlu9kjd+P3U/g2WSJEmSJHVQXafiJOBM4GzgQ8AGlCmsfgW8GLgmM2+KiB0oUwf9MzN/Xo8Pp+PRaIiIVSlT6J1CGSW0L/A64LfA1cBVmXlbRDyfEhjZGVgwM/9ejx/rQNnz8FrqqF4rM8MREYsD36CUp+Mz85iIWAd4M7Ak8N3MvCUi5snMp8dDuapTxx1FmS7ub8AWwPrA1Mw8sO6zBLA3cCHwOLC7QYChqcGg71ECQotSgrMbAWdm5tfqlKAvAl6cmb+IiG8DkzJzh3p83+Zvr+SN30+9oW/nKJUkSZIkqUc8BnwcOAbYBXgj8HtK7+N7gJcD69aGlrdQRmM0Gk9ct0KjaWXg25SGvAMp00VdUHvAPwKcERHL1en07gJWbgp6RAcaZ72WOq/XysyQ1bW3yMwHmTVy7EP1uasoAcEHgHdHxCKZ+XTd1tflql5Px1CCn5cA76EEK84Cvh8R34+IVTLzPkqgbEpmXmqgbGgi4gWU9a1OAn4DLE0JspwJXBkRG2TmjMz8G7BtRLwS+FpTMKirr6u50WN54/dTDzBYJkmSJElSB2XmE7WxeCFgVeB84CBKA9DKlGmtXkCZtufRzPw89HcDmLpDZk6tU8gtSGnI+yGl8fszwKXAYcBPI+JkylRRFzQdO+YNe15LnddrZWaoGqM6ImJKROwMrALsBvyrrrdGZl4DnAackpnTO5fasZWZDwAfzMzjgB2Bt1OmozyLEgS4EvhyRCwNbA/MHHC8197sTQQOB35BWQNuY+B39bmbgS9ExBvqvjOBRTPzVhgXo5F6Jm/8fuoNBsskSZIkSeoCtXH1IuBfmfkUcCLw8cz8L6UxZd/M/CCMiwYwdZcn67+fAvdS2pPWycxTgCOAv2TmB2DW6JtO8lrqCj1VZuakBsreTEn7kpSpI+8DPgg8FBE/rvv9KzNv72BSx1TTZ/ePiJifErw4JTMPoqzF9LbM/B4lUHoxMD0zv9yZ1Paeen+6vo4weh7wUso0p43RSfcDnwM+GRHnAQ9k5tTG8f18b+vVvPH7qbsZLJMkSZIkqXv8CriwNjpeADwZEQtn5h2ZeTXYeKKxV3vi/xxYOzMfpwQ/dqnbzsjMw6HryqbXUgf1aJkZVERMAN4KfBk4GXhhRBwFfAI4GJhW104aVxqfXRZPUkbzvLJufhj4f3X7kcA2mflxeCY/NQcDro3plFFJl1OmNH0J8NLMvAQ4APh9Zu4JvRGAnls9njd+P3WpeTudAEmSJEmSVGTmXZR1fKA0ntwLPDVgHxtPNGYaDXaZ+d2IaLQjXQA8pxx2U9n0WuqcXi0zc5DAfcA2lADZ8cBllMDQhMz8XOeS1j0y86iI+EN9eDrw9ohYKTP/m5mXg2uUDVdm3lWnM31eZj4YEZcDO0fEnzPzSsp0l+Myf3stb/x+6l4GyyRJkiRJ6jK15/OGwIO1t740JiJi/sx8stGoOKDB7un6/9bMaujral5Lo6/fykwrdRrGY4HFgPsz856IeCVltNw8nU1dd2gKkv6t6ek7gAeb9+uGYEUvqvl7QdOovGuAhTNzRvN+4zF/ezVv/H7qPgbLJEmSJEnqMrVh9k+ZeSE4HY9GV6N8RcRKwP4R8fXMvKW5F/6AMnhBZv5h8DN2D6+l0dHPZWYwmXkncGdEbBsRrwHeBHw6M2/pcNLGTKvrpylIlo3HddNrKWuUPTTW6ex1EbFIlrWtntHivrUVPRyAHq5+yhu/n7pPmP+SJEmSJEnjW0S8GXgN8BbgAeCjmfmv2lO/uSF8O2At4EBghg1741c/lpmhNFZHxMpAAAtl5g3jpYG7KUD6NuANlNE759Rp7xrboulzXw14oI7CGxd5NDea8nBZ4KPA6Y2pKwfuU/9+5cDt/cq80VhxMUVJkiRJkqRxLCJeBhwL/B74PPAX4NsRsUodJRR1v+2AzwAnZuZTNn6PX/1aZmqD/CsjYkqr7RExT2bekpnTMvOGsU1dZ9W82Rw4HLgbeC/wwYh4Xt02b1OwYhfgRGrbc7d/7t2g5uHbgf2B1wP7RcTrG9tr2Wvk787AeyNi/qaRfH3LvNFYMVgmSZIkSZI0viXwp8z8M/AH4DhgBvCNiJicmTNrA+QngK3HW5BALfVlmYmINwCnA0dFxBciYpWmbRMy8+n69/si4v+gvwNBETFf09/zAG8FPgz8C1gTWB7YIyImNdaHiogdgD2A3TOz66fC6xYR8SLgCOCHwBeAv1KCkesBNJW97Sijq36UmU/2c/lrMG80VgyWSZIkSZIkjUMRsXZEfAG4GXhdROxalx66DbgYuAf4aB1F9P+AHXsl6KHR0Y9lpjH6JCKWpEwvuAuwLrAKsENETK5TvDXWYtsB+AhwQmdSPDYiYhFg34hYJCImAxOBK4AlgD0pgbPLgPdQ8oOI2AL4FLBzZl7bkYT3tlsy85q6vt85wPOAj0fEKwEiYkfgk8C2mXld55LZEeaNRp3BMkmSJEmSpPHpZuBVwAspjd+7RMTn61pU76E0SM4E/k4ZJWIDpPquzDRN8XYasDuwbmY+TplecnXgfcAkeGaKt32A9/XCe5sbmTmdMt3iryjTbAL8BPgT8K/MvAm4E/gz8OO6fVlgi8y8foyT27MiYrWI+CZwKzBvRHwMIDP/TsnbO4BNawB6M2D7bg9AjxTzRmPNYJkkSZIkSdI4UqdTA5gOXAmsl5m/oUyZ90rKWkR7Aw9R1odZJDMf7kRa1R36ucxExJrAtsCngc9R10PKzFuAA4DFgIl1SsYPA7v1czAoIp4XEQdFxLzA9cBrgeuAeTLzsbrbuyLia8DRwDk1cAbwvcz8x9inuvdERKNd/l5gIUoA+lBgvYg4NiL+H7ANZfTeyrXM7drPZa/BvFGnhFN3SpIkSZIkjQ8RsTJwDPBd4GpgfuAMYMvMvL42Uq5DaSDfF3h3Zl7ToeSqC/R7mYmI3YGDgDUz8646euwQygipP0bE/Jn5ZN138cx8sJPpHQsR8Q1gVeCzwHyUz/YNwMGZeV1EvBx4KXBzZv5l8DNpMBGxaGY+HBELAN8A/pOZh9RRUvsDD1BG7M0DfA3YPDPv71iCx5B5o04xWCZJkiRJktTH6npLGRGLZeZDEbEZZfTPBsCRwIspjd4/rvsvTJnS6rLM/FfHEq6O6ecy0/TeJlOmGVwE+ALwIuBdmfloDZjtCGwOPJaZT3cswR0QEfMDX6cEzLagBEg/QgmK/hFYDfhcr4we7DYRsTRlytKjgPOAR4GTgS9n5kV1n5dT1v3bHdgqM6/uUHLHlHmjTjJYJkmSJEmS1KeaAgNbUdaYmgl8KDP/HhHrUkYCLQksTgmGRGbO7FyK1WnjocxExOaUNcnOBV4AfAzYjxIE3DozH4mIpTPz7g4mc8xERMAz67dNBu6irNO2O7AuZZrNp4EPADsAh2fm6R1Kbk9quq4mZeb9EfEqYCdgKeA/wC3APZl5ct1/AvBu4MrM/Hen0j0WzBt1C4NlkiRJkiRJfSwi1gd+ALyZsu7Lq4DtMvOSiFiUMpXVCzPz8g4mU12kn8tMRKwOnEUZNbYNJeD3bmBRyrprSwN70YNBwOFoBCrq340g4m8p60R9nBJEfAGwTWY+FhGLZOb05uM0e03BoPcAnwT+C3wnM6fWkVSHAc8HJgNrAzPGy2hG80bdZMKcd5EkSZIkSVIPmwz8Ang5sDxl7amfR8S6mflwZj7Qi0EPjaq+KTMRsXhELNf01EOUad3WAt4O7EYJBu6amQcCX8tiPATKFgP2johJEbEMcDiwHfAE8Dzgfsr6bdcAh9cRPY9BGYXWkUT3oBoM2hD4CrArcAdwZERsU0cvvh94H2W9vyfGUzDIvFE3mbfTCZAkSZIkSdKoOh94EHgbsG9m/iUi3gUcFRFvysxHOpo6daO+KDMR8TxKIGzdiLiHMnrsx5SA0POA5TPzqTqq5XGAzJzWkcR2xuLAssDngOMpQdF1KEHELSl5t35mHhARq46HAOIoWgQ4FVgdeC1wMLBfRDydmadRgkR3dDB9nWTeqCs4skySJEmSJKmPZeadmXkOZc2hlWov/iuB3Xsl6KGx1Q9lpgbKPgM8QAkAHQLcl5n3AFtR3ttnImJnyoiWqzqRzk7KzP8CFwILAB8BPgF8FXhD3bYOs4KIN3comf3ib8AfgI2A/TLzJOB24GMRsUxj3bhxyrxRVzBYJkmSJEmS1OciYl7gFuB1wLHAbzPzms6mSt2sD8rMU8ARwL3A+pQRZStFxOqZeRWwMWVU1YuB/TPz/E4ltFMiYjNgf8oou8WBP1HWh9o7Ij7AOA0ijobMnJaZU+H/t3fvwXaV5R3Hv08SiCS0UktELlLRIakMw1iRtkmh1VoicilQlaFEMwMoFttpQzvYVi1FQwFRnGrR0rEwGLVFBGqL2gFSbhHpYC/SIRlGKEUQ5C5XE0jCr3+stZvTwAntsM9Ze5/1/cww7MtaM0+e8+x/3me9z8smYIf2XMD7geVJHuzzWEtzo1FR1pokSZIkSdJ4q6p6sQXFqpoH7AD8VJI7/i/3aObqS81U1Ttozjz6B2A+zRlsH6dZmJ+T5EftdWP3b3spqmoBcCnwgSRrq+ow4I00Z7ltBNYB/97uMNSQVNUfA3sABwGnJflatxGNDnOjrtkskyRJkiRJmgGqagnwmnaE1dbfzU6yuX09L8mP+9Yc0PP1oWaqakfgGJrdZetoGkG/BhwMHJTkXzoMrzPtmMqraHfVtTsJP0pzZtTVSc5urxu7v/moqKpZg3PeJuaxql4JzE1yz8Rr+sTcaBQ5hlGSJEmSJGnMVdViYBWwoqo+X1X7Ds55aRccB02P5cD5VTXHBfB+60vNJHkK+CrNiMF9gIeBM4BD+9ooA0jyGPAVYGlVvSHJJmA18E80IysH143d37wLE347C6vqgKrafmKjJ0nahiRJHgR+PPhq+qOdXuZG48JmmSRJkiRJ0hiasAC5C7AEOCXJzwNPAycAr293Bw2e3n838HvAOe3CuHqmrzWT5EmakYM3AvsB301ybbW6ja5TF9OMozyvqs4CLgRuTvKDbsMaP23D51DgSmAl8O2q+pVBE6jdPbWpfb0MOLeqXtaHZqS50biwWSZJkiRJkjSG2gXIpcA3gd8GDm+/+iNgDvA+YFeAqjoR+F1geZJbOwhXI6DPNZPkCeBvgE8mebb9LH1ekE9yL3A2zfjFe2n+1qu7jWo8tQ3oxcDRSQ4BLgdW0JyRN/G6ZcCpNA3oDdMdZxfMjcaFzTJJkiRJkqQxVFX7AMuBk4B3AUdU1QntIuOpwKPAnKraGVgGvDfJ2s4CVuf6XjNJNiR5uOs4RkmSp5JcneS8JGu6jmfcVNWsqno5cA7NuXjbAyQ5E3gM+LP2fdoG9CnAcUnWdRPx9DE3Gjc2yyRJkiRJksbTQuBIYG6SfwXeAZxZVe9N8kySlUnuapsDhyf5j06j1SiwZqQhGIzvTPJckseBc2maQYuratf2sr8FHmqvfyVwMHDCTG8GmRuNq+rxTmNJkiRJkqSx0Z7rkqpaRPNU/mya3UHLaMan3VZVi4EzgGOBRwZnT6mfrBlp+Cb8rpYCxwCPA18D7gf+GngYWAOcCHwkyd9X1Wxg+yTrOwp7WpgbjTN3lkmSJEmSJI2BdgHySOBS4HeAVcDXac5huqCq9klyE3BkkodsesiakYav/V29Gfhz4AbgNppzuPamaQLtAuwBvLttBlWSzX1oBpkbjTObZZIkSZIkSSOuqmZX1auBlcBRNLuEimaM1VeBC4EPVdXLABcdZc1IU2tv4K+SrEryeZodm+fQ7KQ6Ddgf2AuaBlJnUXbD3Ggs2SyTJEmSJEkaQVW1U1UtbN8+BzwCXAMcCPwGzVP6ewMrklwAnJpkQ5LNnQSszlkz0vBVY+t19PnAEYM3Sa4DbgR2SnINcBbwlqraYdoC7YC50Uwyp+sAJEmSJEmS9IJ+GXhjVT0B7A58EHgT8H5g5yRPV9Vbgc0ASX7YWaQaFdaMNERVNTfJM8DgHK5FwLeAvwAOq6pLgPfQ7JZaAuwAkOSqqrohyYaOQp9y5kYzTbnTUZIkSZIkaXRU1W7Axva/TwOHAacnOa+qdgauBW4BbqZpgqxIcnVX8ap71ow0fO3Opy8AnwL+i2Z31BrgWeB24HzgizTN5z2BlUmuaM/hmtGL7uZGM5HNMkmSJEmSpBFRVdsBJ9EsOt4JfAT4aeAB4BtJbqqqucCHac6gujXJVR2FqxFgzUhTp6reCXwW+Dvg0iSrq2oJcBxwe5JPt7/BVyR5oE/NIHOjmcZmmSRJkiRJ0gipqh2BAKcCXwbuBlYAuwJ/CTwJzElyd1cxarRYM9JwVdWsJM+1r48ALgE+nuT0dlfVm4BjgXVJPtunRpC50Uxls0ySJEmSJGnEVNXuwPHAbsBHaRohxwOLgUOBtyZZ012EGjXWjDQcg+ZOVe0PPAPcRjPa9IvAUUmuaXdrvg54Osn3Owx3WpkbzWQ2yyRJkiRJkkZQVS0CjgFeRdP8eAjYD5iX5KYuY9Nosmak4aiqQ2jO4/oScF2Sb1fVocBFwLI+n/lnbjRT2SyTJEmSJEkaUVW1kGac1a7AGUnunfCdo630PNaM9NJU1U7AxcDHgDuA/YFfAK4AdgT+FDgKeLJvvydzo5nMZpkkSZIkSdIImKyR0TY/DqR5gv/O6Y9Mo8qakYavqmbTNIP2A9YDdwKbgAOSvK2qXpHk0S5j7Iq50Uw2p+sAJEmSJEmS+mjC2S97Ag8A84AfbX1dku8B32vvmZXkuemNVKPCmpGmXpLNVXUZcCtwc5L/rKp9gYVV9RO8wG+uL8yNZjJ3lkmSJEmSJHWkqo4C/hBYA8wHVia5v/3uf5ocVbUc+H6S67uKVaPBmpGmzta7NdvzuX4VeDvwoSRXdBZcx8yNZrpZXQcgSZIkSZLUR1W1N/AJ4DhgNrAn8FR7JgwTmh7HAR8GHukmUo0Ka0aaGlVVAC8w1jTADcBvJblicF2fmBv1hWMYJUmSJEmSpkFVzQfmJXmo/ehp4BvAvsAS4F3AAcAi4Pz2nmXAKcDRSdZNe9DqlDUjTY3/x0jTKyfcM4umQTSjmRv1lc0ySZIkSZKkKVZVc4FjgI1VNQdYAHwGOJhml9DrkjxZVccD69t7lgJnA2+z6dE/1ow0ddpm0FFMGGlaVY40xdyovxzDKEmSJEmSNMWSPAPcArwTOBO4t/3sJOB24PSqOhr4TeA77T1XAQfZ9Ogna0aaOo40nZy5UV/ZLJMkSZIkSZpCE85xuaP9/7eAuVX1miQ3Au+h2TX0i8AHk1xfVdsBJLlruuNV96wZabiqan5VLZjw0dYjTU+mGWl67IR7lgG/TzPS9NZpDHdamRupUc8/l0+SJEmSJEkv1aDh0Y60+hngQZqzX14FnAjcA1wAzAUeS7JxcF9csOkla0Yavnak6XHARppjiQYjTf+tfT0YafonwPokn2xHml7ADB9pam6kLdxZJkmSJEmSNGSD5kXb9Ph14GrgXODgJGuBS4A9gE8BPwDeMLjXpkc/WTPS1HCk6eTMjbSFO8skSZIkSZKGqKp+ElgOXETzZP6NwJuB04DdgAuSfLmq9gJ+luZp/es6CVYjwZqRpsagCd3+xlYBzwL/CFyb5K6qei1wOvBDYE2Sr1fVdoNdmzOZuZH+N5tlkiRJkiRJQ1RVrwbeD8wHrgT2Au4GPgZ8AVgKXAZcluSJCfc5Sq+nrBlpuBxpOjlzI70wxzBKkiRJkiQNUZJ7gOtp1l2OBB4FXgucnOQzwPbAIuDlW93nImRPWTPS8DjSdHLmRprcnK4DkCRJkiRJmkmq6nDgFOA+mif1NwA/1373OLADsKptkEjWjDQkg5GmVXURzUjT89ky0vSkqprdjjR9gGak6ZeSfKereKeTuZG2zTGMkiRJkiRJQ1JVC4BLgQ8kWVtVhwAHAq9ny2i9VUku7zBMjRBrRhoeR5pOztxI2+YYRkmSJEmSpOHZSLMLaJf2/WqgaM6D+SZNQ+TywZkxEtaMNDSONJ2cuZG2zTGMkiRJkiRJQ5Lksar6CrC0qh5N8t2qWg2sBy5Mcl97nYuPAqwZaZgcaTo5cyNtm2MYJUmSJEmShqiqdgdOpjkLZg1wLPC+JKu7jEujy5qRXjpHmk7O3Egvzp1lkiRJkiRJQ5Tk3qo6m2bc1SJgeZI1HYelEWbNSEMxcaTpWpqRpgexZaTp55Lc19MzuMyN9CLcWSZJkiRJkiRJGntV9QfAAuDidqTpW4BfYsJI074yN9K2zeo6AEmSJEmSJEmShuBiYBNwXlWdBVwI/LPNIMDcSNvkzjJJkiRJkiRJ0oxQVTsCi2lGmt7iSNMtzI00OZtlkiRJkiRJkiRJ6i3HMEqSJEmSJEmSJKm3bJZJkiRJkiRJkiSpt2yWSZIkSZIkSZIkqbdslkmSJEmSJEmSJKm3bJZJkiRJkiRJkiSpt2yWSZIkSZIkSZIkqbdslkmSJEmSJEmSJKm3/htvZqAYYdei1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2160x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=[30, 10])\n",
    "\n",
    "# Plot the log loss during training\n",
    "axs[0,0].plot(evaluation_results['train']['rmse'], label='Train')\n",
    "axs[0,0].plot(evaluation_results['valid']['rmse'], label='valid')\n",
    "axs[0,0].set_ylabel('RMSE')\n",
    "axs[0,0].set_xlabel('Boosting round')\n",
    "axs[0,0].set_title('Training performance')\n",
    "axs[0,0].legend()\n",
    "\n",
    "# Plot feature importance\n",
    "importances = pd.DataFrame({'features': X_train.columns, \n",
    "                            'importance': model.feature_importance()}).sort_values('importance', ascending=False).head(10)\n",
    "axs[1,0].bar(x=np.arange(len(importances)), height=importances['importance'])\n",
    "axs[1,0].set_xticks(np.arange(len(importances)))\n",
    "axs[1,0].set_xticklabels(importances['features'])\n",
    "axs[1,0].set_ylabel('Feature importance (# times used to split)')\n",
    "axs[1,0].set_title('Feature importance')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot feature importance\n",
    "importances = pd.DataFrame({'features': X_train.columns, \n",
    "                            'importance': model.feature_importance(importance_type='gain')}).sort_values('importance', ascending=False).head(10)\n",
    "axs[1,1].bar(x=np.arange(len(importances)), height=importances['importance'])\n",
    "axs[1,1].set_xticks(np.arange(len(importances)))\n",
    "axs[1,1].set_xticklabels(importances['features'])\n",
    "axs[1,1].set_ylabel('Feature importance (# times used to split)')\n",
    "axs[1,1].set_title('Feature importance (GAIN)')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13539714-deef-4cda-b7c1-036f890b4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータを予測する\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f148cd-d122-4de2-9a9a-64c54d89e35a",
   "metadata": {},
   "source": [
    "# 後処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4aa976e-e63d-45e5-9f26-5570548e8085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20150896974036214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20150896974036214"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_pred.min())\n",
    "y_pred = np.where(y_pred < 0.0, 0.0, y_pred)\n",
    "y_pred.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a77ab-ac14-41e6-ab7a-9645ba5d7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_Pred_map(pred_df):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69426067-85f9-48e8-9789-7e015bb12c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(y_pred).plot(style='r-', kind=\"line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35791aa-f469-435d-b209-9e18039cc5bd",
   "metadata": {},
   "source": [
    "# 提出用ファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78df3a01-487d-4859-bec8-d105cafa75b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1\n",
       "0  0  100\n",
       "1  1  100\n",
       "2  2  100\n",
       "3  3  100\n",
       "4  4  100"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample_submissionの中身を確認\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7adbc480-4a18-4750-af86-ad714cf2a019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.841081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.457959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.632454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.063050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.121096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  0  0.841081\n",
       "1  1  0.457959\n",
       "2  2  0.632454\n",
       "3  3  2.063050\n",
       "4  4  2.121096"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample_submissionの右側のカラムに予測値を代入する。\n",
    "sub.iloc[:, -1] = y_pred\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9cb2b9d-367d-4b9a-85b2-7937a7bbb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 予測ファイルの生成\n",
    "# sub.to_csv('./outputs/submission10_LightGBM_oputuna.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8387dc3-7055-4d6b-b227-e2cae0ff32f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ukita_main_env': conda)",
   "language": "python",
   "name": "python395jvsc74a57bd0c19bd4dc0949b9af5049c50e337ed6e21b7d6fbec491433b163f7fe7b877fba9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
